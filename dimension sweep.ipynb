{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from data import *\n",
    "from layers import *\n",
    "from models import *\n",
    "from control_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as p\n",
    "import io "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_gpus()\n",
    "set_gpu(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mnist\"\n",
    "ds_train, ds_test, ds_info = load_dataset(dataset, 128)\n",
    "\n",
    "input_shape = ds_info.features['image'].shape\n",
    "num_classes = ds_info.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_standard(train, test):\n",
    "    head = keras.models.load_model(\"ae_conv_head.h5\")\n",
    "    \n",
    "    std = keras.Sequential([head, layers.Dense(100, activation=\"relu\"), layers.Dense(10, activation=\"softmax\")])\n",
    "    std.compile(optimizer=\"rmsprop\", loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "    \n",
    "    #return std\n",
    "    \n",
    "    losses = std.fit(train, epochs=5)\n",
    "    acc = std.evaluate(ds_test)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 4s 6ms/step - loss: 0.6092 - accuracy: 0.8660\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0984 - accuracy: 0.9742\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0738 - accuracy: 0.9821\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0635 - accuracy: 0.9844\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0524 - accuracy: 0.9869\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 0.0363 - accuracy: 0.9915\n"
     ]
    }
   ],
   "source": [
    "upper_limit = test_standard(ds_train, ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_control(train, test, n_d):\n",
    "    head = keras.models.load_model(\"ae_conv_head.h5\")\n",
    "    inv_head = keras.models.load_model(\"ae_inv_head.h5\")\n",
    "    \n",
    "    ciris = ControlIrisModel(head, inv_head, ds_info, n_d=n_d, n_hidden=100)\n",
    "    ciris.compile(optimizer=\"rmsprop\")\n",
    "    \n",
    "    data = ciris.generate_external(ds_train, True)\n",
    "    losses = ciris.train(data, epochs=5)\n",
    "    acc = ciris.accuracy(ds_test)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_iris(train, test, n_d):\n",
    "    head = keras.models.load_model(\"ae_conv_head.h5\")\n",
    "    inv_head = keras.models.load_model(\"ae_inv_head.h5\")\n",
    "    \n",
    "    iris = IrisModel(head, inv_head, ds_info, n_d=n_d, n_hidden=100)\n",
    "    iris.compile(optimizer=\"rmsprop\")\n",
    "    \n",
    "    data = iris.generate_external(ds_train, True)\n",
    "    losses = iris.train(data, epochs=5)\n",
    "    acc = iris.accuracy(ds_test)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ds = np.floor(np.logspace(1, 3, 21)).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  10,   12,   15,   19,   25,   31,   39,   50,   63,   79,  100,\n",
       "        125,  158,  199,  251,  316,  398,  501,  630,  794, 1000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_l = lambda x: test_control(ds_train, ds_test, x)\n",
    "iris_l = lambda x: test_iris(ds_train, ds_test, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 7.37298   8.372596 31.295141 47.27646 ]\n",
      "Training loss [ 0.4552517  1.1381487 14.668877  33.04172  ]\n",
      "Training loss [ 0.41142833  1.1220456  13.634823   34.652138  ]\n",
      "Training loss [ 0.3526859  1.20172   11.992836  31.577507 ]\n",
      "Training loss [ 0.32938474  1.1764102  13.090839   31.122356  ]\n",
      "Training loss [ 0.35890788  1.1005915  12.888974   33.681114  ]\n",
      "Training loss [ 0.3280273  1.1381487 12.478654  33.04172  ]\n",
      "Training loss [ 0.32695204  1.1220456  12.00243    34.652138  ]\n",
      "Training loss [ 0.30838838  1.20172    11.5144005  31.577507  ]\n",
      "Training loss [ 0.29022452  1.1764102  12.57502    31.122356  ]\n",
      "Training loss [ 0.32071945  1.1005915  12.396112   33.681114  ]\n",
      "Training loss [ 0.3092911  1.1381487 12.261037  33.04172  ]\n",
      "Training loss [ 0.3042661  1.1220456 11.857318  34.652138 ]\n",
      "Training loss [ 0.29238838  1.20172    11.500584   31.577507  ]\n",
      "Training loss [ 0.27710897  1.1764102  12.533993   31.122356  ]\n",
      "Training loss [ 0.2971593  1.1005915 12.2982235 33.681114 ]\n",
      "Training loss [ 0.29592696  1.1381487  12.211169   33.04172   ]\n",
      "Training loss [ 0.2936466  1.1220456 11.843248  34.652138 ]\n",
      "Training loss [ 0.28129855  1.20172    11.50554    31.577507  ]\n",
      "Training loss [ 0.26921874  1.1764102  12.529366   31.122356  ]\n",
      "Training loss [ 0.28762394  1.1005915  12.276295   33.681114  ]\n",
      "Training loss [ 0.29014614  1.1381487  12.203943   33.04172   ]\n",
      "Training loss [ 0.2888646  1.1220456 11.843668  34.652138 ]\n",
      "Training loss [ 0.27640793  1.20172    11.508105   31.577507  ]\n",
      "Training loss [ 0.26377484  1.1764102  12.528124   31.122356  ]\n",
      "Training loss [ 2.0564773  5.8185735 22.521385  32.872902 ]\n",
      "Training loss [ 0.36862236  0.88415647 13.2077875  23.872791  ]\n",
      "Training loss [ 0.32442793  0.87446225 13.271187   24.351505  ]\n",
      "Training loss [ 0.3294777   0.90618473 13.15414    24.351194  ]\n",
      "Training loss [ 0.2644897  0.8964462 12.56333   23.272934 ]\n",
      "Training loss [ 0.29931387  0.91057986 12.632808   23.414768  ]\n",
      "Training loss [ 0.24443765  0.88415575 12.322959   23.872765  ]\n",
      "Training loss [ 0.23915046  0.87446225 12.806168   24.351505  ]\n",
      "Training loss [ 0.24492902  0.90618473 13.039538   24.351194  ]\n",
      "Training loss [ 0.24354236  0.8964462  12.528773   23.272934  ]\n",
      "Training loss [ 0.27685314  0.91057986 12.631649   23.414677  ]\n",
      "Training loss [ 0.22528368  0.88415575 12.23794    23.872765  ]\n",
      "Training loss [ 0.22614306  0.87446225 12.759544   24.351505  ]\n",
      "Training loss [ 0.23334287  0.90618473 13.037033   24.351194  ]\n",
      "Training loss [ 0.23979662  0.8964462  12.525863   23.272934  ]\n",
      "Training loss [ 0.26391676  0.91057986 12.631987   23.414677  ]\n",
      "Training loss [ 0.21837386  0.88415575 12.230576   23.872765  ]\n",
      "Training loss [ 0.22338016  0.87446225 12.753225   24.351505  ]\n",
      "Training loss [ 0.22724953  0.90618473 13.040724   24.351194  ]\n",
      "Training loss [ 0.23767406  0.8964462  12.524702   23.272934  ]\n",
      "Training loss [ 0.2589138   0.91057986 12.631823   23.414677  ]\n",
      "Training loss [ 0.21584752  0.88415575 12.230099   23.872765  ]\n",
      "Training loss [ 0.22180045  0.87446225 12.75197    24.351505  ]\n",
      "Training loss [ 0.22309221  0.90618473 13.043387   24.351194  ]\n",
      "Training loss [ 0.23559031  0.8964462  12.524639   23.272934  ]\n",
      "Training loss [ 4.1374683  8.026075  29.554373  57.435307 ]\n",
      "Training loss [ 0.4279514  1.1365409 12.816431  32.33338  ]\n",
      "Training loss [ 0.3394171  1.1825751 12.149424  31.660158 ]\n",
      "Training loss [ 0.32118914  1.1596673  12.705618   34.109352  ]\n",
      "Training loss [ 0.31409344  1.1241841  13.299997   32.30703   ]\n",
      "Training loss [ 0.27960107  1.1563437  11.714798   31.201853  ]\n",
      "Training loss [ 0.2959525  1.136467  11.82532   32.33338  ]\n",
      "Training loss [ 0.28972578  1.1825751  11.098189   31.659458  ]\n",
      "Training loss [ 0.29714748  1.159491   11.8625965  34.109352  ]\n",
      "Training loss [ 0.28925225  1.1241841  12.390211   32.30703   ]\n",
      "Training loss [ 0.25776017  1.1563437  11.065325   31.201853  ]\n",
      "Training loss [ 0.27745515  1.136467   11.286069   32.33338   ]\n",
      "Training loss [ 0.27895647  1.1825751  10.968769   31.659458  ]\n",
      "Training loss [ 0.2837999  1.159491  11.827823  34.109352 ]\n",
      "Training loss [ 0.27468038  1.1241841  12.373292   32.30703   ]\n",
      "Training loss [ 0.24700534  1.1563437  11.04867    31.201853  ]\n",
      "Training loss [ 0.26613182  1.136467   11.2896805  32.33338   ]\n",
      "Training loss [ 0.27094126  1.1825751  10.961181   31.659458  ]\n",
      "Training loss [ 0.27749935  1.159491   11.825003   34.109352  ]\n",
      "Training loss [ 0.27232826  1.1241841  12.371277   32.30703   ]\n",
      "Training loss [ 0.24174327  1.1563437  11.04272    31.201853  ]\n",
      "Training loss [ 0.25611198  1.136467   11.292999   32.33338   ]\n",
      "Training loss [ 0.26611555  1.1825751  10.960053   31.659458  ]\n",
      "Training loss [ 0.27317977  1.159491   11.824463   34.109352  ]\n",
      "Training loss [ 0.26643345  1.1241841  12.370157   32.30703   ]\n",
      "Training loss [ 1.8063588 13.318581  31.509699  47.18098  ]\n",
      "Training loss [ 0.5094487  1.0618637  9.548423  27.780668 ]\n",
      "Training loss [ 0.33407965  1.0440416   9.686712   31.507818  ]\n",
      "Training loss [ 0.29067168  1.0342361   9.343914   32.436558  ]\n",
      "Training loss [ 0.25915635  1.0536878   9.4059     32.541466  ]\n",
      "Training loss [ 0.27678627  1.0008498   9.521666   33.866413  ]\n",
      "Training loss [ 0.2585006  1.0599824  8.353364  27.780472 ]\n",
      "Training loss [ 0.23416606  1.0439382   9.138537   31.507818  ]\n",
      "Training loss [ 0.23335193  1.0342361   9.12537    32.436558  ]\n",
      "Training loss [ 0.23810697  1.0536878   9.290876   32.541466  ]\n",
      "Training loss [ 0.24031219  1.0008498   9.478377   33.866413  ]\n",
      "Training loss [ 0.22984901  1.0599824   8.333916   27.780472  ]\n",
      "Training loss [ 0.2026272  1.0439382  9.142061  31.507818 ]\n",
      "Training loss [ 0.20004448  1.0342361   9.12595    32.436558  ]\n",
      "Training loss [ 0.20790325  1.0536878   9.287192   32.541466  ]\n",
      "Training loss [ 0.22576648  1.0008498   9.480049   33.866413  ]\n",
      "Training loss [ 0.2189907  1.0599824  8.333841  27.780472 ]\n",
      "Training loss [ 0.19947526  1.0439382   9.142802   31.507818  ]\n",
      "Training loss [ 0.19446981  1.0342361   9.126244   32.436558  ]\n",
      "Training loss [ 0.19909295  1.0536878   9.286154   32.541466  ]\n",
      "Training loss [ 0.21237428  1.0008498   9.48019    33.866413  ]\n",
      "Training loss [ 0.19562547  1.0599824   8.334563   27.780472  ]\n",
      "Training loss [ 0.1673333  1.0439382  9.141497  31.507818 ]\n",
      "Training loss [ 0.15765524  1.0342361   9.125298   32.436558  ]\n",
      "Training loss [ 0.1727432  1.0536878  9.286118  32.541466 ]\n",
      "Training loss [ 4.5712886 12.242739  23.61445   36.453938 ]\n",
      "Training loss [ 0.4898361  1.0749319  9.218258  21.636143 ]\n",
      "Training loss [ 0.34844837  1.0538429   8.994972   22.674225  ]\n",
      "Training loss [ 0.3170008  1.0467889  9.563181  24.101843 ]\n",
      "Training loss [ 0.24856591  1.0104764   9.693899   24.242195  ]\n",
      "Training loss [ 0.2922241  1.0397098  9.98056   24.714918 ]\n",
      "Training loss [ 0.22998875  1.0748686   8.626507   21.635609  ]\n",
      "Training loss [ 0.24553467  1.0538429   8.921396   22.674225  ]\n",
      "Training loss [ 0.25060573  1.0467889   9.55286    24.101843  ]\n",
      "Training loss [ 0.20316625  1.0104764   9.681443   24.242195  ]\n",
      "Training loss [ 0.23670301  1.0397098   9.981368   24.714918  ]\n",
      "Training loss [ 0.1968326  1.0748686  8.6272335 21.635609 ]\n",
      "Training loss [ 0.22298187  1.0538429   8.920375   22.674225  ]\n",
      "Training loss [ 0.22780648  1.0467889   9.553058   24.101843  ]\n",
      "Training loss [ 0.18630542  1.0104764   9.680079   24.242195  ]\n",
      "Training loss [ 0.22946808  1.0397098   9.980066   24.714918  ]\n",
      "Training loss [ 0.17743286  1.0748686   8.628034   21.635609  ]\n",
      "Training loss [ 0.21460848  1.0538429   8.919315   22.674225  ]\n",
      "Training loss [ 0.21423313  1.0467889   9.553446   24.101843  ]\n",
      "Training loss [ 0.17315985  1.0104764   9.679235   24.242195  ]\n",
      "Training loss [ 0.22046864  1.0397098   9.980527   24.714918  ]\n",
      "Training loss [ 0.16679715  1.0748686   8.6280575  21.635609  ]\n",
      "Training loss [ 0.2026062  1.0538429  8.918892  22.674225 ]\n",
      "Training loss [ 0.20195086  1.0467889   9.553158   24.101843  ]\n",
      "Training loss [ 0.16582188  1.0104764   9.678787   24.242195  ]\n",
      "Training loss [ 1.6622572 10.488811  20.565353  36.375908 ]\n",
      "Training loss [ 0.40691882  0.92998123  7.358693   21.71217   ]\n",
      "Training loss [ 0.27878892  0.9160727   6.962175   23.07889   ]\n",
      "Training loss [ 0.23308825  0.96507686  6.6441336  23.840431  ]\n",
      "Training loss [ 0.20629948  1.0018886   6.9748545  24.548811  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.19474855  0.96738493  6.425711   22.33342   ]\n",
      "Training loss [ 0.15750355  0.92998123  6.327292   21.71217   ]\n",
      "Training loss [ 0.16702567  0.9160727   6.833692   23.07889   ]\n",
      "Training loss [ 0.15655482  0.96507686  6.6362996  23.840431  ]\n",
      "Training loss [ 0.16234623  1.0018886   6.9589577  24.548811  ]\n",
      "Training loss [ 0.16098322  0.96738493  6.417216   22.33342   ]\n",
      "Training loss [ 0.13403323  0.92998123  6.327574   21.71217   ]\n",
      "Training loss [ 0.14428745  0.9160727   6.833828   23.07889   ]\n",
      "Training loss [ 0.14362127  0.96507686  6.637304   23.840431  ]\n",
      "Training loss [ 0.151614   1.0018886  6.9581413 24.548811 ]\n",
      "Training loss [ 0.15234081  0.96738493  6.416458   22.33342   ]\n",
      "Training loss [ 0.12465652  0.92998123  6.312748   21.71217   ]\n",
      "Training loss [ 0.13071981  0.9160727   6.3509502  23.07889   ]\n",
      "Training loss [ 0.1356      0.96507686  6.049603   23.840431  ]\n",
      "Training loss [ 0.14337897  1.0018886   6.4796276  24.548811  ]\n",
      "Training loss [ 0.14439951  0.96738493  5.984344   22.33342   ]\n",
      "Training loss [ 0.11825338  0.92998123  5.886862   21.71217   ]\n",
      "Training loss [ 0.12264118  0.9160727   6.3453264  23.07889   ]\n",
      "Training loss [ 0.1305492   0.96507686  6.046706   23.840431  ]\n",
      "Training loss [ 0.13750353  1.0018886   6.4828296  24.548811  ]\n",
      "Training loss [ 2.4116664  9.295271  22.195038  38.677208 ]\n",
      "Training loss [ 0.53414416  1.1562011   7.734126   23.45751   ]\n",
      "Training loss [ 0.32889047  1.181142    7.2485876  23.78137   ]\n",
      "Training loss [ 0.25224409  1.1744423   7.302417   23.824509  ]\n",
      "Training loss [ 0.23009357  1.1889477   7.0749383  22.77426   ]\n",
      "Training loss [ 0.24226776  1.1831543   7.432974   23.809782  ]\n",
      "Training loss [ 0.18324494  1.1560218   7.058865   23.45751   ]\n",
      "Training loss [ 0.17587933  1.181142    7.1464653  23.781246  ]\n",
      "Training loss [ 0.16640416  1.1744423   7.2698693  23.824509  ]\n",
      "Training loss [ 0.16436476  1.1889477   7.0742173  22.77426   ]\n",
      "Training loss [ 0.19144808  1.1831543   7.433462   23.809782  ]\n",
      "Training loss [ 0.14826813  1.1560218   7.060029   23.45751   ]\n",
      "Training loss [ 0.15545848  1.181142    7.146533   23.781246  ]\n",
      "Training loss [ 0.14571887  1.1744423   7.267113   23.824509  ]\n",
      "Training loss [ 0.14407791  1.1889477   7.0736866  22.77426   ]\n",
      "Training loss [ 0.17580284  1.1831543   7.432787   23.809782  ]\n",
      "Training loss [ 0.12983656  1.1560218   7.0600033  23.45751   ]\n",
      "Training loss [ 0.14775169  1.181142    7.1457577  23.781246  ]\n",
      "Training loss [ 0.13584717  1.1744423   7.2659297  23.824509  ]\n",
      "Training loss [ 0.13190626  1.1889477   7.0729895  22.77426   ]\n",
      "Training loss [ 0.16767159  1.1831543   7.4327073  23.809782  ]\n",
      "Training loss [ 0.12413613  1.1560218   7.0596104  23.45751   ]\n",
      "Training loss [ 0.1426292  1.181142   7.145441  23.781246 ]\n",
      "Training loss [ 0.1296969  1.1744423  7.264893  23.824509 ]\n",
      "Training loss [ 0.12532224  1.1889477   7.0723906  22.77426   ]\n",
      "Training loss [ 6.902547  9.136125 26.748766 50.076607]\n",
      "Training loss [ 0.67924285  1.030272   10.596237   30.74408   ]\n",
      "Training loss [ 0.4153863  1.0178893 10.120752  30.328873 ]\n",
      "Training loss [ 0.26862198  1.0077801   9.729206   30.176361  ]\n",
      "Training loss [ 0.22246787  0.97946805 10.200791   29.81202   ]\n",
      "Training loss [ 0.18811072  1.000508    9.799582   28.211033  ]\n",
      "Training loss [ 0.18069609  1.030272   10.047855   30.74408   ]\n",
      "Training loss [ 0.16552873  1.0178041  10.057119   30.328873  ]\n",
      "Training loss [ 0.14407763  1.0077801   9.71261    30.176361  ]\n",
      "Training loss [ 0.14904335  0.97946805 10.203234   29.81202   ]\n",
      "Training loss [ 0.14165537  1.000508    9.797928   28.211033  ]\n",
      "Training loss [ 0.13955663  1.030272   10.045289   30.74408   ]\n",
      "Training loss [ 0.13870703  1.0178041  10.057576   30.328873  ]\n",
      "Training loss [ 0.12675244  1.0077801   9.711604   30.176361  ]\n",
      "Training loss [ 0.13075969  0.97946805 10.2027025  29.81202   ]\n",
      "Training loss [ 0.12175649  1.000508    9.79635    28.211033  ]\n",
      "Training loss [ 0.12878007  1.030272   10.044556   30.74408   ]\n",
      "Training loss [ 0.12888661  1.0178041  10.056875   30.328873  ]\n",
      "Training loss [ 0.11746424  1.0077801   9.712422   30.176361  ]\n",
      "Training loss [ 0.12263709  0.97946805  9.522542   29.81202   ]\n",
      "Training loss [ 0.11454493  1.000508    9.153929   28.211033  ]\n",
      "Training loss [ 0.12240744  1.030272    9.326821   30.74408   ]\n",
      "Training loss [ 0.12303536  1.0178041   9.553722   30.328873  ]\n",
      "Training loss [ 0.10944588  1.0077801   9.101187   30.176361  ]\n",
      "Training loss [ 0.11557809  0.97946805  9.505638   29.81202   ]\n",
      "Training loss [ 3.536497  9.725939 24.654598 40.854218]\n",
      "Training loss [ 0.4890653  0.967175  12.595585  26.274944 ]\n",
      "Training loss [ 0.2501626  0.9824363 11.99062   27.1381   ]\n",
      "Training loss [ 0.17126384  0.9843178  11.093529   25.66915   ]\n",
      "Training loss [ 0.1371353  0.9571021 10.791227  26.477123 ]\n",
      "Training loss [ 0.1132044   0.96554416 10.518335   26.286385  ]\n",
      "Training loss [ 0.1047689   0.96688855 10.483617   26.27494   ]\n",
      "Training loss [ 0.11644512  0.9824363  11.140667   27.1381    ]\n",
      "Training loss [ 0.11040136  0.9843178  10.400799   25.66915   ]\n",
      "Training loss [ 0.10294592  0.9571021  10.77168    26.477123  ]\n",
      "Training loss [ 0.08984156  0.96554416 10.511814   26.286385  ]\n",
      "Training loss [ 0.0889428   0.96688855 10.479849   26.27494   ]\n",
      "Training loss [ 0.10648654  0.9824363  11.138823   27.1381    ]\n",
      "Training loss [ 0.10016751  0.9843178  10.399023   25.66915   ]\n",
      "Training loss [ 0.09825692  0.9571021  10.769531   26.477123  ]\n",
      "Training loss [ 0.08240919  0.96554416 10.509954   26.286385  ]\n",
      "Training loss [ 0.08488101  0.96688855 10.479271   26.27494   ]\n",
      "Training loss [ 0.1014462  0.9824363 11.137607  27.1381   ]\n",
      "Training loss [ 0.09478695  0.9843178  10.397755   25.66915   ]\n",
      "Training loss [ 0.09168408  0.9571021  10.767456   26.477123  ]\n",
      "Training loss [ 0.0797513   0.96554416 10.509152   26.286385  ]\n",
      "Training loss [ 0.08261269  0.96688855 10.479027   26.27494   ]\n",
      "Training loss [ 0.0970066  0.9824363 11.135815  27.1381   ]\n",
      "Training loss [ 0.09162086  0.9843178   9.276889   25.66915   ]\n",
      "Training loss [ 0.08822399  0.9571021   9.767417   26.477123  ]\n",
      "Training loss [ 3.748319 15.233885 21.211943 41.95363 ]\n",
      "Training loss [ 0.41031957  0.9434357   8.824893   23.764732  ]\n",
      "Training loss [ 0.21174566  0.9338781   8.2756605  23.77082   ]\n",
      "Training loss [ 0.10799527  0.9467407   8.049471   22.930025  ]\n",
      "Training loss [ 0.10068961  0.9507123   8.279711   23.694407  ]\n",
      "Training loss [ 0.11809349  0.9518516   8.326595   23.394587  ]\n",
      "Training loss [ 0.10094714  0.9433084   8.4953     23.764732  ]\n",
      "Training loss [ 0.08227322  0.9338781   8.207991   23.77082   ]\n",
      "Training loss [ 0.06116623  0.9467197   8.042711   22.930025  ]\n",
      "Training loss [ 0.07005681  0.9507123   8.276283   23.694407  ]\n",
      "Training loss [ 0.09085639  0.9518516   8.323625   23.394587  ]\n",
      "Training loss [ 0.08666146  0.9433084   8.462385   23.764732  ]\n",
      "Training loss [ 0.0681847  0.9338781  8.075005  23.77082  ]\n",
      "Training loss [ 0.05575063  0.9467197   7.8750443  22.930025  ]\n",
      "Training loss [ 0.06147442  0.9507123   8.129156   23.694407  ]\n",
      "Training loss [ 0.08454378  0.9518516   8.158632   23.394587  ]\n",
      "Training loss [ 0.08108879  0.9433084   8.314437   23.764732  ]\n",
      "Training loss [ 0.06408067  0.9338781   8.076394   23.77082   ]\n",
      "Training loss [ 0.05372249  0.9467197   7.8744717  22.930025  ]\n",
      "Training loss [ 0.05798757  0.9507123   8.128173   23.694407  ]\n",
      "Training loss [ 0.0817086  0.9518516  8.156485  23.394587 ]\n",
      "Training loss [ 0.07685316  0.9433084   8.314022   23.764732  ]\n",
      "Training loss [ 0.06276435  0.9338781   8.075771   23.77082   ]\n",
      "Training loss [ 0.05346159  0.9467197   7.873786   22.930025  ]\n",
      "Training loss [ 0.05576621  0.9507123   8.127642   23.694407  ]\n",
      "Training loss [ 5.2340956 16.214207  23.505127  46.764328 ]\n",
      "Training loss [ 0.5720473  1.0630876  9.205225  24.424257 ]\n",
      "Training loss [ 0.3205588  1.0573897  8.061871  23.876724 ]\n",
      "Training loss [ 0.2057623  1.0500197  7.8025    23.775911 ]\n",
      "Training loss [ 0.18281126  1.0625963   8.622145   25.994755  ]\n",
      "Training loss [ 0.16452706  1.0682037   8.182367   25.111517  ]\n",
      "Training loss [ 0.14073978  1.063057    8.25903    24.42367   ]\n",
      "Training loss [ 0.1588257  1.0573897  8.00245   23.876724 ]\n",
      "Training loss [ 0.14003062  1.0500197   7.7872434  23.775911  ]\n",
      "Training loss [ 0.13465966  1.0625963   8.622734   25.994755  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.1331103  1.0682037  8.17454   25.111517 ]\n",
      "Training loss [ 0.1232716  1.063057   8.256801  24.42367  ]\n",
      "Training loss [ 0.14358722  1.0573897   8.000246   23.876724  ]\n",
      "Training loss [ 0.12823999  1.0500197   7.785795   23.775911  ]\n",
      "Training loss [ 0.12912114  1.0625963   8.621752   25.994755  ]\n",
      "Training loss [ 0.12754193  1.0682037   8.172167   25.111517  ]\n",
      "Training loss [ 0.11785644  1.063057    8.254743   24.42367   ]\n",
      "Training loss [ 0.13441554  1.0573897   7.998376   23.876724  ]\n",
      "Training loss [ 0.11343162  1.0500197   7.7858086  23.775911  ]\n",
      "Training loss [ 0.11127564  1.0625963   8.620825   25.994755  ]\n",
      "Training loss [ 0.1059768  1.0682037  8.170325  25.111517 ]\n",
      "Training loss [ 0.09286375  1.063057    8.252928   24.42367   ]\n",
      "Training loss [ 0.11547361  1.0573897   7.9980564  23.876724  ]\n",
      "Training loss [ 0.10356536  1.0500197   7.7863684  23.775911  ]\n",
      "Training loss [ 0.10432    1.0625963  8.621164  25.994755 ]\n",
      "Training loss [ 5.568695  8.493357 23.198183 43.879734]\n",
      "Training loss [ 0.4039397  0.9519201  9.574955  24.632553 ]\n",
      "Training loss [ 0.20072006  0.9388572   9.281521   25.95381   ]\n",
      "Training loss [ 0.13937412  0.9317047   9.029814   25.836533  ]\n",
      "Training loss [ 0.11066272  0.9222954   9.208073   25.982138  ]\n",
      "Training loss [ 0.09152095  0.944951    8.694858   25.234413  ]\n",
      "Training loss [ 0.0883414   0.95191586  8.591985   24.632553  ]\n",
      "Training loss [ 0.08920909  0.938833    9.096303   25.95381   ]\n",
      "Training loss [ 0.08506276  0.9317047   9.02247    25.836533  ]\n",
      "Training loss [ 0.08250318  0.9222954   9.200502   25.982138  ]\n",
      "Training loss [ 0.07060331  0.944951    8.691516   25.234413  ]\n",
      "Training loss [ 0.07334343  0.95191586  8.590233   24.632553  ]\n",
      "Training loss [ 0.07743941  0.938833    9.1003     25.95381   ]\n",
      "Training loss [ 0.07692153  0.9317047   9.02258    25.836533  ]\n",
      "Training loss [ 0.07827749  0.9222954   9.198587   25.982138  ]\n",
      "Training loss [ 0.0634608  0.944951   8.688773  25.234413 ]\n",
      "Training loss [ 0.07012796  0.95191586  8.587814   24.632553  ]\n",
      "Training loss [ 0.07482228  0.938833    9.019549   25.95381   ]\n",
      "Training loss [ 0.07268198  0.9317047   8.958226   25.836533  ]\n",
      "Training loss [ 0.06921314  0.9222954   9.084178   25.982138  ]\n",
      "Training loss [ 0.05871249  0.944951    8.550659   25.234413  ]\n",
      "Training loss [ 0.06450733  0.95191586  8.487811   24.632553  ]\n",
      "Training loss [ 0.06656852  0.938833    8.998885   25.95381   ]\n",
      "Training loss [ 0.06217663  0.9317047   8.958746   25.836533  ]\n",
      "Training loss [ 0.05945028  0.9222954   9.083552   25.982138  ]\n",
      "Training loss [ 4.3244014 12.657349  24.075336  42.124786 ]\n",
      "Training loss [ 0.431851    0.94758356 10.352711   25.94553   ]\n",
      "Training loss [ 0.18390521  0.94606906 10.123372   26.344372  ]\n",
      "Training loss [ 0.10707199  0.9468901   9.512613   26.312948  ]\n",
      "Training loss [ 0.09301452  0.94877106  9.215734   25.69189   ]\n",
      "Training loss [ 0.07636933  0.95145154  9.203255   25.751095  ]\n",
      "Training loss [ 0.0797231  0.9475442  9.121206  25.94553  ]\n",
      "Training loss [ 0.07546499  0.94606906  9.189173   26.344372  ]\n",
      "Training loss [ 0.07083466  0.9468901   9.224686   26.312948  ]\n",
      "Training loss [ 0.07006612  0.94877106  8.969152   25.69189   ]\n",
      "Training loss [ 0.05843046  0.95145154  8.93819    25.751095  ]\n",
      "Training loss [ 0.0676733  0.9475442  9.103918  25.94553  ]\n",
      "Training loss [ 0.06457879  0.94606906  9.185507   26.344372  ]\n",
      "Training loss [ 0.06340192  0.9468901   9.2224045  26.312948  ]\n",
      "Training loss [ 0.06334344  0.94877106  8.911613   25.69189   ]\n",
      "Training loss [ 0.05018071  0.95145154  8.86478    25.751095  ]\n",
      "Training loss [ 0.06197216  0.9475442   9.012978   25.94553   ]\n",
      "Training loss [ 0.06058957  0.94606906  9.088602   26.344372  ]\n",
      "Training loss [ 0.06125243  0.9468901   9.128643   26.312948  ]\n",
      "Training loss [ 0.05998971  0.94877106  8.883478   25.69189   ]\n",
      "Training loss [ 0.04559937  0.95145154  8.861837   25.751095  ]\n",
      "Training loss [ 0.05881142  0.9475442   9.0098095  25.94553   ]\n",
      "Training loss [ 0.05808548  0.94606906  9.087872   26.344372  ]\n",
      "Training loss [ 0.0586035  0.9468901  9.12879   26.312948 ]\n",
      "Training loss [ 0.05841437  0.94877106  8.782629   25.69189   ]\n",
      "Training loss [ 6.2695146 10.80835   23.654684  50.18195  ]\n",
      "Training loss [ 0.34139097  1.0151966   9.901406   25.206095  ]\n",
      "Training loss [ 0.13312428  1.0127255   9.084251   25.301167  ]\n",
      "Training loss [ 0.08241498  1.000872    8.68351    25.278824  ]\n",
      "Training loss [ 0.08522788  1.0101522   9.1545315  26.148113  ]\n",
      "Training loss [ 0.05968035  1.0103266   8.905236   25.916012  ]\n",
      "Training loss [ 0.0635637  1.0151248  8.756997  25.206095 ]\n",
      "Training loss [ 0.0591821  1.0127255  8.816456  25.301167 ]\n",
      "Training loss [ 0.05388804  1.000872    8.545553   25.278824  ]\n",
      "Training loss [ 0.04985124  1.0101522   9.107195   26.148113  ]\n",
      "Training loss [ 0.04846743  1.0103266   8.8083935  25.916012  ]\n",
      "Training loss [ 0.05210597  1.0151248   8.7210865  25.206095  ]\n",
      "Training loss [ 0.05128825  1.0127255   8.785685   25.301167  ]\n",
      "Training loss [ 0.04791383  1.000872    8.511198   25.278824  ]\n",
      "Training loss [ 0.04941208  1.0101522   9.073099   26.148113  ]\n",
      "Training loss [ 0.04513541  1.0103266   8.81009    25.916012  ]\n",
      "Training loss [ 0.05022587  1.0151248   8.716513   25.206095  ]\n",
      "Training loss [ 0.04959672  1.0127255   8.783917   25.301167  ]\n",
      "Training loss [ 0.04458424  1.000872    8.511694   25.278824  ]\n",
      "Training loss [ 0.04144154  1.0101522   9.011458   26.148113  ]\n",
      "Training loss [ 0.0416689  1.0103266  8.7537985 25.916012 ]\n",
      "Training loss [ 0.04812472  1.0151248   8.661233   25.206095  ]\n",
      "Training loss [ 0.04931671  1.0127255   8.688085   25.301167  ]\n",
      "Training loss [ 0.04237296  1.000872    8.289927   25.278824  ]\n",
      "Training loss [ 0.03660779  1.0101522   8.846756   26.148113  ]\n",
      "Training loss [ 4.717457  8.174391 22.982628 41.934334]\n",
      "Training loss [ 0.33969083  0.9632268   9.628091   24.079803  ]\n",
      "Training loss [ 0.15767857  0.9584434   9.382607   24.686798  ]\n",
      "Training loss [ 0.08210384  0.9642336   8.74591    23.775578  ]\n",
      "Training loss [ 0.08261809  0.9541443   9.236171   24.52148   ]\n",
      "Training loss [ 0.07311091  0.95701647  9.312301   24.666986  ]\n",
      "Training loss [ 0.06227478  0.9632182   8.729921   24.079659  ]\n",
      "Training loss [ 0.05829798  0.9584434   9.019834   24.686222  ]\n",
      "Training loss [ 0.03923222  0.9642336   8.499346   23.775578  ]\n",
      "Training loss [ 0.04998664  0.9541443   9.010606   24.52148   ]\n",
      "Training loss [ 0.0525539   0.95701647  9.072774   24.666986  ]\n",
      "Training loss [ 0.04890718  0.9632182   8.554816   24.079659  ]\n",
      "Training loss [ 0.04746477  0.9584434   8.793511   24.686222  ]\n",
      "Training loss [ 0.03287384  0.9642336   8.288015   23.775578  ]\n",
      "Training loss [ 0.04406241  0.9541443   8.841097   24.52148   ]\n",
      "Training loss [ 0.04517762  0.95701647  8.96619    24.666986  ]\n",
      "Training loss [ 0.04195644  0.9632182   8.500486   24.079659  ]\n",
      "Training loss [ 0.04143846  0.9584434   8.79228    24.686222  ]\n",
      "Training loss [ 0.02962617  0.9642336   8.283118   23.775578  ]\n",
      "Training loss [ 0.04032517  0.9541443   8.839517   24.52148   ]\n",
      "Training loss [ 0.04179979  0.95701647  8.966192   24.666986  ]\n",
      "Training loss [ 0.03842722  0.9632182   8.489831   24.079659  ]\n",
      "Training loss [ 0.03954986  0.9584434   8.787123   24.686222  ]\n",
      "Training loss [ 0.02833027  0.9642336   8.279972   23.775578  ]\n",
      "Training loss [ 0.03872965  0.9541443   8.843163   24.52148   ]\n",
      "Training loss [ 5.7352858  9.606843  24.919579  42.23227  ]\n",
      "Training loss [ 0.2816423   0.96754074 10.097803   25.89087   ]\n",
      "Training loss [ 0.10757151  0.96980774  9.620377   26.308971  ]\n",
      "Training loss [ 0.07612221  0.9741857   9.649481   26.178354  ]\n",
      "Training loss [ 0.06580537  0.97093505  9.97774    27.061068  ]\n",
      "Training loss [ 0.04679669  0.97436833  9.457343   26.495216  ]\n",
      "Training loss [ 0.0515892   0.96754074  9.380248   25.890741  ]\n",
      "Training loss [ 0.0433641   0.96980774  9.362322   26.308971  ]\n",
      "Training loss [ 0.04707388  0.9741857   9.376236   26.178354  ]\n",
      "Training loss [ 0.04586934  0.97093505  9.567432   27.061068  ]\n",
      "Training loss [ 0.03713139  0.97436833  9.087171   26.495216  ]\n",
      "Training loss [ 0.04109086  0.96754074  9.089766   25.890741  ]\n",
      "Training loss [ 0.03407691  0.96980774  9.082683   26.308971  ]\n",
      "Training loss [ 0.04097632  0.9741857   9.189764   26.178354  ]\n",
      "Training loss [ 0.04154687  0.97093505  9.436626   27.061068  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.03090836  0.97436833  8.934575   26.495216  ]\n",
      "Training loss [ 0.03696107  0.96754074  8.887888   25.890741  ]\n",
      "Training loss [ 0.03027687  0.96980774  8.750071   26.308971  ]\n",
      "Training loss [ 0.04059751  0.9741857   8.870464   26.178354  ]\n",
      "Training loss [ 0.03868763  0.97093505  9.112265   27.061068  ]\n",
      "Training loss [ 0.02969491  0.97436833  8.682455   26.495216  ]\n",
      "Training loss [ 0.03460697  0.96754074  8.738215   25.890741  ]\n",
      "Training loss [ 0.02802503  0.96980774  8.744531   26.308971  ]\n",
      "Training loss [ 0.03728398  0.9741857   8.87011    26.178354  ]\n",
      "Training loss [ 0.03646314  0.97093505  9.103563   27.061068  ]\n",
      "Training loss [ 2.9053192  9.398116  23.934011  36.80714  ]\n",
      "Training loss [ 0.2756874  1.0030732 10.364876  26.963669 ]\n",
      "Training loss [ 0.10394552  1.0052555   9.807054   26.58705   ]\n",
      "Training loss [ 0.07694363  1.0110933   9.296312   26.113174  ]\n",
      "Training loss [ 0.05762063  1.0087814   9.100664   26.229202  ]\n",
      "Training loss [ 0.06724712  0.98997134  8.968377   25.43585   ]\n",
      "Training loss [ 0.04719725  1.0030732   9.313625   26.963337  ]\n",
      "Training loss [ 0.04216495  1.0052555   9.030901   26.58705   ]\n",
      "Training loss [ 0.05017838  1.0110933   9.033971   26.113174  ]\n",
      "Training loss [ 0.04489962  1.0087814   8.956226   26.229202  ]\n",
      "Training loss [ 0.04639408  0.98997134  8.752134   25.43585   ]\n",
      "Training loss [ 0.03861017  1.0030732   9.095106   26.963337  ]\n",
      "Training loss [ 0.04034251  1.0052555   8.766956   26.58705   ]\n",
      "Training loss [ 0.03767855  1.0110933   8.71528    26.113174  ]\n",
      "Training loss [ 0.03842124  1.0087814   8.657334   26.229202  ]\n",
      "Training loss [ 0.04239758  0.98997134  8.458673   25.43585   ]\n",
      "Training loss [ 0.03316617  1.0030732   8.782486   26.963337  ]\n",
      "Training loss [ 0.03388873  1.0052555   8.58009    26.58705   ]\n",
      "Training loss [ 0.03677768  1.0110933   8.548693   26.113174  ]\n",
      "Training loss [ 0.03484121  1.0087814   8.468506   26.229202  ]\n",
      "Training loss [ 0.03924553  0.98997134  8.292181   25.43585   ]\n",
      "Training loss [ 0.02972649  1.0030732   8.733177   26.963337  ]\n",
      "Training loss [ 0.0329646  1.0052555  8.561292  26.58705  ]\n",
      "Training loss [ 0.03655789  1.0110933   8.539176   26.113174  ]\n",
      "Training loss [ 0.03345786  1.0087814   8.466346   26.229202  ]\n",
      "Training loss [ 3.5599809  6.067771  25.233395  46.575424 ]\n",
      "Training loss [ 0.2506484  0.9709124  9.726319  27.049683 ]\n",
      "Training loss [ 0.09070697  0.968542    9.065456   27.480865  ]\n",
      "Training loss [ 0.06914495  0.9669412   8.648949   26.967096  ]\n",
      "Training loss [ 0.06573746  0.97114927  8.789631   27.864376  ]\n",
      "Training loss [ 0.05422116  0.9685425   8.348388   26.718697  ]\n",
      "Training loss [ 0.05477525  0.9691611   8.658384   27.047329  ]\n",
      "Training loss [ 0.04033527  0.968542    8.328635   27.480865  ]\n",
      "Training loss [ 0.04444095  0.9669412   8.394174   26.967096  ]\n",
      "Training loss [ 0.04779362  0.97114927  8.52952    27.864376  ]\n",
      "Training loss [ 0.03873736  0.9685425   8.110923   26.718697  ]\n",
      "Training loss [ 0.04279205  0.9691611   8.45005    27.047329  ]\n",
      "Training loss [ 0.02791558  0.968542    8.313664   27.480865  ]\n",
      "Training loss [ 0.03970878  0.9669412   8.185871   26.967096  ]\n",
      "Training loss [ 0.04049541  0.97114927  8.373732   27.864376  ]\n",
      "Training loss [ 0.03522396  0.9685425   8.017509   26.718697  ]\n",
      "Training loss [ 0.03738247  0.9691611   8.382      27.047329  ]\n",
      "Training loss [ 0.027515  0.968542  8.274009 27.480865]\n",
      "Training loss [ 0.03622986  0.9669412   8.167647   26.967096  ]\n",
      "Training loss [ 0.03427386  0.97114927  8.380751   27.864376  ]\n",
      "Training loss [ 0.03209763  0.9685425   7.9937057  26.718697  ]\n",
      "Training loss [ 0.03439976  0.9691611   8.321389   27.047329  ]\n",
      "Training loss [ 0.02776606  0.968542    8.191696   27.480865  ]\n",
      "Training loss [ 0.03508891  0.9669412   8.093089   26.967096  ]\n",
      "Training loss [ 0.0322435   0.97114927  8.2875595  27.864376  ]\n",
      "Training loss [ 2.3962374  8.248535  23.495537  38.50927  ]\n",
      "Training loss [ 0.27826923  0.99901974 10.383287   25.885012  ]\n",
      "Training loss [ 0.1234362   0.99595225  9.811504   26.347666  ]\n",
      "Training loss [ 0.06815022  0.9940649   9.511692   25.946596  ]\n",
      "Training loss [ 0.06719624  0.9910179   9.348127   25.83967   ]\n",
      "Training loss [ 0.05986494  0.9929822   8.951047   24.804953  ]\n",
      "Training loss [ 0.05956893  0.998279    9.173876   25.884274  ]\n",
      "Training loss [ 0.0494758   0.99595225  9.231718   26.347666  ]\n",
      "Training loss [ 0.0436349  0.9940649  9.241293  25.946596 ]\n",
      "Training loss [ 0.05551727  0.9910179   9.254922   25.83967   ]\n",
      "Training loss [ 0.04299067  0.9929822   8.874006   24.804953  ]\n",
      "Training loss [ 0.04715872  0.998279    9.034889   25.884274  ]\n",
      "Training loss [ 0.03950821  0.99595225  9.092378   26.347666  ]\n",
      "Training loss [ 0.03458267  0.9940649   8.900528   25.946596  ]\n",
      "Training loss [ 0.04293912  0.9910179   8.88352    25.83967   ]\n",
      "Training loss [ 0.03666282  0.9929822   8.503794   24.804953  ]\n",
      "Training loss [ 0.03971544  0.998279    8.632833   25.884274  ]\n",
      "Training loss [ 0.03358899  0.99595225  8.776223   26.347666  ]\n",
      "Training loss [ 0.03119984  0.9940649   8.693696   25.946596  ]\n",
      "Training loss [ 0.03760932  0.9910179   8.738789   25.83967   ]\n",
      "Training loss [ 0.0358734  0.9929822  8.31875   24.804953 ]\n",
      "Training loss [ 0.03905294  0.998279    8.379107   25.884274  ]\n",
      "Training loss [ 0.03241776  0.99595225  8.5432625  26.347666  ]\n",
      "Training loss [ 0.02979529  0.9940649   8.453965   25.946596  ]\n",
      "Training loss [ 0.03783123  0.9910179   8.377087   25.83967   ]\n",
      "Training loss [ 2.0884995  6.9809523 25.860224  38.932453 ]\n",
      "Training loss [ 0.30638433  0.9673749   9.879022   26.561548  ]\n",
      "Training loss [ 0.14055924  0.9737791   8.996628   26.472073  ]\n",
      "Training loss [ 0.11781016  0.97853374  8.980959   27.112925  ]\n",
      "Training loss [ 0.08607838  0.9723215   8.727153   26.944784  ]\n",
      "Training loss [ 0.0890626   0.97011256  8.890158   27.109554  ]\n",
      "Training loss [ 0.07863228  0.9673749   8.655871   26.561544  ]\n",
      "Training loss [ 0.06616154  0.9737791   8.282019   26.472073  ]\n",
      "Training loss [ 0.06645792  0.97853374  8.522867   27.112925  ]\n",
      "Training loss [ 0.08197276  0.9723215   8.405995   26.944784  ]\n",
      "Training loss [ 0.06998806  0.97011256  8.519966   27.109554  ]\n",
      "Training loss [ 0.05448416  0.9673749   8.30804    26.561544  ]\n",
      "Training loss [ 0.06358717  0.9737791   8.084591   26.472073  ]\n",
      "Training loss [ 0.06520078  0.97853374  8.369549   27.112925  ]\n",
      "Training loss [ 0.05715616  0.9723215   8.193821   26.944784  ]\n",
      "Training loss [ 0.05336542  0.97011256  8.352286   27.109554  ]\n",
      "Training loss [ 0.04424401  0.9673749   8.140532   26.561544  ]\n",
      "Training loss [ 0.05585436  0.9737791   7.9789896  26.472073  ]\n",
      "Training loss [ 0.05065933  0.97853374  8.220087   27.112925  ]\n",
      "Training loss [ 0.05117412  0.9723215   8.021944   26.944784  ]\n",
      "Training loss [ 0.05050334  0.97011256  8.134297   27.109554  ]\n",
      "Training loss [ 0.03742785  0.9673749   7.889325   26.561544  ]\n",
      "Training loss [ 0.04220534  0.9737791   7.83825    26.472073  ]\n",
      "Training loss [ 0.04002237  0.97853374  7.911961   27.112925  ]\n",
      "Training loss [ 0.04086633  0.9723215   7.8042665  26.944784  ]\n",
      "Training loss [ 1.7744416  4.92425   24.827465  37.59256  ]\n",
      "Training loss [ 0.34758627  0.9990905  10.7059355  26.188524  ]\n",
      "Training loss [ 0.2457848   0.99963605  9.584316   25.711048  ]\n",
      "Training loss [ 0.16596769  0.9936682   9.08307    25.147287  ]\n",
      "Training loss [ 0.1194995  0.995246   9.436495  26.566452 ]\n",
      "Training loss [ 0.11337503  1.005536    8.990273   25.914707  ]\n",
      "Training loss [ 0.08047681  0.9990905   9.23531    26.188524  ]\n",
      "Training loss [ 0.08131863  0.99963605  8.861074   25.711048  ]\n",
      "Training loss [ 0.06847015  0.9936682   8.893202   25.147287  ]\n",
      "Training loss [ 0.0673894  0.995246   9.106479  26.566452 ]\n",
      "Training loss [ 0.07818224  1.005536    8.536626   25.914707  ]\n",
      "Training loss [ 0.06371387  0.9990905   8.848788   26.188524  ]\n",
      "Training loss [ 0.05937564  0.99963605  8.4870615  25.711048  ]\n",
      "Training loss [ 0.06529377  0.9936682   8.340156   25.147287  ]\n",
      "Training loss [ 0.06043302  0.995246    8.656372   26.566452  ]\n",
      "Training loss [ 0.05968576  1.005536    8.253829   25.914707  ]\n",
      "Training loss [ 0.0573613  0.9990905  8.567668  26.188524 ]\n",
      "Training loss [ 0.05376579  0.99963605  8.198836   25.711048  ]\n",
      "Training loss [ 0.06407245  0.9936682   8.021332   25.147287  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.04822397  0.995246    8.318653   26.566452  ]\n",
      "Training loss [ 0.05267809  1.005536    7.909461   25.914707  ]\n",
      "Training loss [ 0.04467402  0.9990905   8.262219   26.188524  ]\n",
      "Training loss [ 0.04781067  0.99963605  7.9689865  25.711048  ]\n",
      "Training loss [ 0.06034228  0.9936682   7.8852863  25.147287  ]\n",
      "Training loss [ 0.05597225  0.995246    8.141154   26.566452  ]\n",
      "Training loss [ 1.8087219  5.032006  19.708902  26.667149 ]\n",
      "Training loss [ 0.3612758   0.96916807  8.451723   23.991316  ]\n",
      "Training loss [ 0.31235313  0.9795393   7.4177494  22.935577  ]\n",
      "Training loss [ 0.30113554  0.95199406  6.977274   22.723677  ]\n",
      "Training loss [ 0.282182    0.98557365  6.728339   20.420158  ]\n",
      "Training loss [ 0.3003003  0.9879255  6.6213703 21.174156 ]\n",
      "Training loss [ 0.29397196  0.9691102   7.5286903  23.991316  ]\n",
      "Training loss [ 0.26791435  0.9795393   7.0365324  22.935577  ]\n",
      "Training loss [ 0.27544975  0.95199406  6.7132707  22.723677  ]\n",
      "Training loss [ 0.26617485  0.98557365  6.4281764  20.420158  ]\n",
      "Training loss [ 0.28628957  0.9879255   6.4409895  21.174156  ]\n",
      "Training loss [ 0.28587425  0.9691102   7.314679   23.991316  ]\n",
      "Training loss [ 0.26491845  0.9795393   6.9288893  22.935577  ]\n",
      "Training loss [ 0.27311832  0.95199406  6.6906176  22.723677  ]\n",
      "Training loss [ 0.2622961   0.98557365  6.3289757  20.420158  ]\n",
      "Training loss [ 0.28466803  0.9879255   6.414459   21.174156  ]\n",
      "Training loss [ 0.28556144  0.9691102   7.2769656  23.991316  ]\n",
      "Training loss [ 0.26570025  0.9795393   6.894296   22.935577  ]\n",
      "Training loss [ 0.27094486  0.95199406  6.698592   22.723677  ]\n",
      "Training loss [ 0.25951725  0.98557365  6.290104   20.420158  ]\n",
      "Training loss [ 0.28104615  0.9879255   6.425718   21.174156  ]\n",
      "Training loss [ 0.28477758  0.9691102   7.2697105  23.991316  ]\n",
      "Training loss [ 0.2623658  0.9795393  6.886541  22.935577 ]\n",
      "Training loss [ 0.26843512  0.95199406  6.7051816  22.723677  ]\n",
      "Training loss [ 0.25777256  0.98557365  6.278676   20.420158  ]\n",
      "Training loss [ 1.0697346 15.506546  33.2501    57.973522 ]\n",
      "Training loss [ 0.49749184  1.1606314  17.559376   34.56136   ]\n",
      "Training loss [ 0.44841492  1.1442043  16.564255   34.0413    ]\n",
      "Training loss [ 0.45941797  1.200589   18.313831   35.987865  ]\n",
      "Training loss [ 0.40775356  1.1294671  16.450401   33.65308   ]\n",
      "Training loss [ 0.38228124  1.0922174  16.86394    34.63746   ]\n",
      "Training loss [ 0.3952962  1.1604227 16.508642  34.56136  ]\n",
      "Training loss [ 0.39676893  1.1442043  15.86155    34.0413    ]\n",
      "Training loss [ 0.4229235  1.200589  17.866625  35.987865 ]\n",
      "Training loss [ 0.38703263  1.1294671  16.17974    33.65308   ]\n",
      "Training loss [ 0.35975468  1.0922174  16.551083   34.63746   ]\n",
      "Training loss [ 0.37556824  1.1604227  16.343405   34.56136   ]\n",
      "Training loss [ 0.38382515  1.1442043  15.703945   34.0413    ]\n",
      "Training loss [ 0.40735388  1.200589   17.731766   35.987865  ]\n",
      "Training loss [ 0.37545934  1.1294671  16.137367   33.65308   ]\n",
      "Training loss [ 0.34663415  1.0922174  16.499094   34.63746   ]\n",
      "Training loss [ 0.25323424  1.1604227  16.337332   34.56136   ]\n",
      "Training loss [ 0.2538486  1.1442043 15.68614   34.0413   ]\n",
      "Training loss [ 0.26058406  1.200589   17.713593   35.987865  ]\n",
      "Training loss [ 0.273786   1.1294671 16.14227   33.65308  ]\n",
      "Training loss [ 0.23592584  1.0922174  16.493547   34.63746   ]\n",
      "Training loss [ 0.23835868  1.1604227  16.33897    34.56136   ]\n",
      "Training loss [ 0.24622333  1.1442043  15.685751   34.0413    ]\n",
      "Training loss [ 0.25502726  1.200589   17.709343   35.987865  ]\n",
      "Training loss [ 0.27033848  1.1294671  16.145237   33.65308   ]\n",
      "Training loss [ 1.9588482  8.5637245 20.717718  32.076523 ]\n",
      "Training loss [ 0.4597459  1.2162799  8.798368  22.995773 ]\n",
      "Training loss [ 0.3169123  1.0645745  7.244052  23.38948  ]\n",
      "Training loss [ 0.31684357  1.0669713   7.1014414  22.139591  ]\n",
      "Training loss [ 0.2943781  1.0928383  7.3487625 22.94497  ]\n",
      "Training loss [ 0.29166248  1.1366936   6.9353476  21.546467  ]\n",
      "Training loss [ 0.30341986  1.2162412   7.648838   22.995773  ]\n",
      "Training loss [ 0.24750894  1.0645745   6.7214823  23.38948   ]\n",
      "Training loss [ 0.26991677  1.0669713   7.0041394  22.139591  ]\n",
      "Training loss [ 0.2500751  1.0928383  7.2524595 22.94497  ]\n",
      "Training loss [ 0.26284283  1.1366936   6.8793325  21.546467  ]\n",
      "Training loss [ 0.27087843  1.2162412   7.6066456  22.995773  ]\n",
      "Training loss [ 0.22888371  1.0645745   6.708182   23.38948   ]\n",
      "Training loss [ 0.2565647  1.0669713  7.017995  22.139591 ]\n",
      "Training loss [ 0.23662798  1.0928383   7.2384725  22.94497   ]\n",
      "Training loss [ 0.25172895  1.1366936   6.870794   21.546467  ]\n",
      "Training loss [ 0.2574512  1.2162412  7.601673  22.995773 ]\n",
      "Training loss [ 0.22040728  1.0645745   6.711617   23.38948   ]\n",
      "Training loss [ 0.24762195  1.0669713   7.022935   22.139591  ]\n",
      "Training loss [ 0.23045026  1.0928383   7.235111   22.94497   ]\n",
      "Training loss [ 0.24392113  1.1366936   6.871999   21.546467  ]\n",
      "Training loss [ 0.25483    1.2162412  7.60154   22.995773 ]\n",
      "Training loss [ 0.2153078  1.0645745  6.7135677 23.38948  ]\n",
      "Training loss [ 0.24670309  1.0669713   7.0236773  22.139591  ]\n",
      "Training loss [ 0.22679006  1.0928383   7.2352457  22.94497   ]\n",
      "Training loss [ 2.2595084  5.227812  22.957405  43.134045 ]\n",
      "Training loss [ 0.43944046  1.0701997   7.2906733  22.842102  ]\n",
      "Training loss [ 0.37676027  1.0298499   6.7345     22.914482  ]\n",
      "Training loss [ 0.31445646  1.0124593   6.238071   23.0232    ]\n",
      "Training loss [ 0.33389604  1.090757    6.1471086  22.511229  ]\n",
      "Training loss [ 0.32694098  1.0334672   6.5085473  23.989962  ]\n",
      "Training loss [ 0.29750952  1.0701997   6.137703   22.841759  ]\n",
      "Training loss [ 0.27573252  1.0298499   6.4256754  22.914482  ]\n",
      "Training loss [ 0.25650567  1.0124593   6.1629324  23.0232    ]\n",
      "Training loss [ 0.26935795  1.090757    6.0989337  22.511229  ]\n",
      "Training loss [ 0.26816642  1.0334672   6.465337   23.989962  ]\n",
      "Training loss [ 0.26545206  1.0701997   6.1114206  22.841759  ]\n",
      "Training loss [ 0.2663946  1.0298499  6.419339  22.914482 ]\n",
      "Training loss [ 0.23750012  1.0124593   6.161357   23.0232    ]\n",
      "Training loss [ 0.25248373  1.090757    6.0961714  22.511229  ]\n",
      "Training loss [ 0.25205708  1.0334672   6.462032   23.989962  ]\n",
      "Training loss [ 0.25437814  1.0701997   6.108558   22.841759  ]\n",
      "Training loss [ 0.25820526  1.0298499   6.418912   22.914482  ]\n",
      "Training loss [ 0.22347595  1.0124593   6.1611643  23.0232    ]\n",
      "Training loss [ 0.2401934  1.090757   6.0960727 22.511229 ]\n",
      "Training loss [ 0.24291945  1.0334672   6.4613786  23.989962  ]\n",
      "Training loss [ 0.246019   1.0701997  6.107983  22.841759 ]\n",
      "Training loss [ 0.25128365  1.0298499   6.418272   22.914482  ]\n",
      "Training loss [ 0.21446067  1.0124593   6.161523   23.0232    ]\n",
      "Training loss [ 0.23363596  1.090757    6.096053   22.511229  ]\n",
      "Training loss [ 2.2394073 11.781572  24.945827  37.62298  ]\n",
      "Training loss [ 0.4088304  1.0599568 11.638771  26.922237 ]\n",
      "Training loss [ 0.29593277  1.1281922  10.555988   26.682594  ]\n",
      "Training loss [ 0.25184175  1.0351654  10.638685   26.407682  ]\n",
      "Training loss [ 0.21840559  1.088337   10.658878   26.7786    ]\n",
      "Training loss [ 0.22997272  1.1140966   9.88308    25.85507   ]\n",
      "Training loss [ 0.20752665  1.0599434  10.789293   26.922188  ]\n",
      "Training loss [ 0.19392642  1.1281922  10.426455   26.682594  ]\n",
      "Training loss [ 0.18068685  1.0351654  10.590411   26.407682  ]\n",
      "Training loss [ 0.18745764  1.088337   10.644747   26.7786    ]\n",
      "Training loss [ 0.20516115  1.1140966   9.8755     25.85507   ]\n",
      "Training loss [ 0.18491708  1.0599434  10.787628   26.922188  ]\n",
      "Training loss [ 0.16529661  1.1281922  10.426313   26.682594  ]\n",
      "Training loss [ 0.16200274  1.0351654  10.588895   26.407682  ]\n",
      "Training loss [ 0.16919416  1.088337   10.64534    26.7786    ]\n",
      "Training loss [ 0.18935882  1.1140966   9.875181   25.85507   ]\n",
      "Training loss [ 0.18161273  1.0599434  10.78642    26.922188  ]\n",
      "Training loss [ 0.15649812  1.1281922  10.426637   26.682594  ]\n",
      "Training loss [ 0.1546999  1.0351654 10.588225  26.407682 ]\n",
      "Training loss [ 0.16014886  1.088337   10.645288   26.7786    ]\n",
      "Training loss [ 0.18166727  1.1140966   9.874908   25.85507   ]\n",
      "Training loss [ 0.160296   1.0599434 10.786161  26.922188 ]\n",
      "Training loss [ 0.15124738  1.1281922  10.426401   26.682594  ]\n",
      "Training loss [ 0.15405968  1.0351654  10.5889     26.407682  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.15732896  1.088337   10.645416   26.7786    ]\n",
      "Training loss [ 3.5859308  8.926689  20.457031  44.556007 ]\n",
      "Training loss [ 0.45138076  0.9073665   8.527781   20.91703   ]\n",
      "Training loss [ 0.30455202  0.93468446  8.126099   21.612606  ]\n",
      "Training loss [ 0.2461254  0.9247562  8.582816  22.798454 ]\n",
      "Training loss [ 0.21619552  0.93303823  7.7923307  21.937618  ]\n",
      "Training loss [ 0.18280807  0.92682874  7.999882   22.051208  ]\n",
      "Training loss [ 0.18053319  0.9072386   8.002059   20.916021  ]\n",
      "Training loss [ 0.16867314  0.93468446  7.961483   21.612606  ]\n",
      "Training loss [ 0.15225767  0.9247562   8.537336   22.798454  ]\n",
      "Training loss [ 0.14046505  0.93303823  7.7693524  21.937618  ]\n",
      "Training loss [ 0.15498509  0.92682874  7.991273   22.051208  ]\n",
      "Training loss [ 0.15077674  0.9072386   8.002445   20.91601   ]\n",
      "Training loss [ 0.14908972  0.93468446  7.9592667  21.612606  ]\n",
      "Training loss [ 0.13359398  0.9247562   8.537174   22.798454  ]\n",
      "Training loss [ 0.1262371   0.93303823  7.7686834  21.937618  ]\n",
      "Training loss [ 0.13555424  0.92682874  7.988738   22.051208  ]\n",
      "Training loss [ 0.14059478  0.9072386   8.002555   20.91601   ]\n",
      "Training loss [ 0.13430978  0.93468446  7.958359   21.612606  ]\n",
      "Training loss [ 0.12501764  0.9247562   8.5371475  22.798454  ]\n",
      "Training loss [ 0.11806203  0.93303823  7.7685227  21.937618  ]\n",
      "Training loss [ 0.13016762  0.92682874  7.988187   22.051208  ]\n",
      "Training loss [ 0.13540567  0.9072386   7.866493   20.91601   ]\n",
      "Training loss [ 0.1291023   0.93468446  7.771767   21.612606  ]\n",
      "Training loss [ 0.1180338  0.9247562  8.284873  22.798454 ]\n",
      "Training loss [ 0.11265604  0.93303823  7.568884   21.937618  ]\n",
      "Training loss [ 1.8639153  6.762545  19.276619  36.399612 ]\n",
      "Training loss [ 0.5009332  1.028646   8.00078   21.076805 ]\n",
      "Training loss [ 0.3099786  1.0390587  7.6698217 22.002638 ]\n",
      "Training loss [ 0.24315587  1.0408908   7.528014   21.601597  ]\n",
      "Training loss [ 0.23297964  1.0302789   8.159706   21.666908  ]\n",
      "Training loss [ 0.225342   1.0067346  6.960785  20.80518  ]\n",
      "Training loss [ 0.18800022  1.0285717   6.8877296  21.07585   ]\n",
      "Training loss [ 0.17342857  1.0389987   7.0146217  22.002638  ]\n",
      "Training loss [ 0.1673225  1.0408908  6.9137836 21.601597 ]\n",
      "Training loss [ 0.16592972  1.0302789   7.403294   21.666908  ]\n",
      "Training loss [ 0.17931738  1.0067346   6.8625307  20.804665  ]\n",
      "Training loss [ 0.14905551  1.0285717   6.885226   21.07585   ]\n",
      "Training loss [ 0.14954032  1.0389987   7.013233   22.002638  ]\n",
      "Training loss [ 0.1456759  1.0408908  6.910959  21.601597 ]\n",
      "Training loss [ 0.14634404  1.0302789   7.402171   21.666908  ]\n",
      "Training loss [ 0.17332111  1.0067346   6.8627567  20.804665  ]\n",
      "Training loss [ 0.13443129  1.0285717   6.8831882  21.07585   ]\n",
      "Training loss [ 0.14008468  1.0389987   7.0115304  22.002638  ]\n",
      "Training loss [ 0.13573165  1.0408908   6.9107943  21.601597  ]\n",
      "Training loss [ 0.13530374  1.0302789   7.4014482  21.666908  ]\n",
      "Training loss [ 0.16911606  1.0067346   6.861411   20.804665  ]\n",
      "Training loss [ 0.12809287  1.0285717   6.882781   21.07585   ]\n",
      "Training loss [ 0.13402942  1.0389987   7.01153    22.002638  ]\n",
      "Training loss [ 0.13048841  1.0408908   6.9102716  21.601597  ]\n",
      "Training loss [ 0.12885487  1.0302789   7.4008     21.666908  ]\n",
      "Training loss [ 3.8152728 13.403841  21.326311  38.570045 ]\n",
      "Training loss [ 0.56608194  1.0711608   8.034446   21.78598   ]\n",
      "Training loss [ 0.30180588  1.0953461   7.651616   21.441225  ]\n",
      "Training loss [ 0.2279568  1.0734432  8.024635  23.105593 ]\n",
      "Training loss [ 0.15936129  1.0545473   7.5870237  21.234425  ]\n",
      "Training loss [ 0.17823109  1.054476    8.271398   22.961143  ]\n",
      "Training loss [ 0.15157717  1.0711608   7.556511   21.78598   ]\n",
      "Training loss [ 0.13402528  1.0953461   7.617152   21.441225  ]\n",
      "Training loss [ 0.14947355  1.0734432   8.018162   23.105593  ]\n",
      "Training loss [ 0.11100555  1.0545473   7.5795975  21.234425  ]\n",
      "Training loss [ 0.1254964  1.054476   8.271261  22.961143 ]\n",
      "Training loss [ 0.10915749  1.0711608   7.556329   21.78598   ]\n",
      "Training loss [ 0.11433275  1.0953461   7.6169386  21.441225  ]\n",
      "Training loss [ 0.13563187  1.0734432   8.0173645  23.105593  ]\n",
      "Training loss [ 0.10191714  1.0545473   7.5790634  21.234425  ]\n",
      "Training loss [ 0.11254528  1.054476    8.269832   22.961143  ]\n",
      "Training loss [ 0.10100235  1.0711608   7.5551853  21.78598   ]\n",
      "Training loss [ 0.10985754  1.0953461   7.617856   21.441225  ]\n",
      "Training loss [ 0.12944938  1.0734432   8.017657   23.105593  ]\n",
      "Training loss [ 0.095465   1.0545473  7.5778418 21.234425 ]\n",
      "Training loss [ 0.10544167  1.054476    8.270154   22.961143  ]\n",
      "Training loss [ 0.09464014  1.0711608   7.5537996  21.78598   ]\n",
      "Training loss [ 0.10730335  1.0953461   7.4207916  21.441225  ]\n",
      "Training loss [ 0.12585491  1.0734432   7.8453903  23.105593  ]\n",
      "Training loss [ 0.090528   1.0545473  7.3610997 21.234425 ]\n",
      "Training loss [ 4.479159  8.972233 24.586222 50.116707]\n",
      "Training loss [ 0.5894936  1.0625019  9.713104  26.271332 ]\n",
      "Training loss [ 0.31259346  1.0497344   9.527206   26.648417  ]\n",
      "Training loss [ 0.22417834  1.0471649   9.141392   26.511898  ]\n",
      "Training loss [ 0.16173123  1.0544677   8.838026   25.85764   ]\n",
      "Training loss [ 0.16020654  1.086795    9.156507   27.449919  ]\n",
      "Training loss [ 0.11925559  1.0625019   8.746444   26.271332  ]\n",
      "Training loss [ 0.11624701  1.0497344   9.226177   26.648417  ]\n",
      "Training loss [ 0.1057078  1.0471649  9.109663  26.511898 ]\n",
      "Training loss [ 0.08829725  1.0544677   8.831745   25.85764   ]\n",
      "Training loss [ 0.08668616  1.086795    9.154049   27.449919  ]\n",
      "Training loss [ 0.09107256  1.0625019   8.74608    26.271332  ]\n",
      "Training loss [ 0.10579842  1.0497344   9.225817   26.648417  ]\n",
      "Training loss [ 0.09330907  1.0471649   9.110527   26.511898  ]\n",
      "Training loss [ 0.07899277  1.0544677   8.831062   25.85764   ]\n",
      "Training loss [ 0.07394433  1.086795    9.152232   27.449919  ]\n",
      "Training loss [ 0.07921469  1.0625019   8.744651   26.271332  ]\n",
      "Training loss [ 0.09982788  1.0497344   9.226802   26.648417  ]\n",
      "Training loss [ 0.0868368  1.0471649  9.109753  26.511898 ]\n",
      "Training loss [ 0.07618113  1.0544677   8.830679   25.85764   ]\n",
      "Training loss [ 0.07003273  1.086795    9.151104   27.449919  ]\n",
      "Training loss [ 0.07361343  1.0625019   8.745502   26.271332  ]\n",
      "Training loss [ 0.09364324  1.0497344   9.2262745  26.648417  ]\n",
      "Training loss [ 0.08472017  1.0471649   9.110004   26.511898  ]\n",
      "Training loss [ 0.07416911  1.0544677   8.830744   25.85764   ]\n",
      "Training loss [ 3.425174 12.460745 26.486025 54.095085]\n",
      "Training loss [ 0.4177766  1.0163436 11.156586  27.965843 ]\n",
      "Training loss [ 0.18673535  0.99175334 10.544741   27.670738  ]\n",
      "Training loss [ 0.14134222  1.0105093  10.4980135  27.085705  ]\n",
      "Training loss [ 0.09880282  0.9887948  10.169739   26.552902  ]\n",
      "Training loss [ 0.11715558  1.0199227  11.199112   28.521763  ]\n",
      "Training loss [ 0.10584544  1.0163436  10.839706   27.965515  ]\n",
      "Training loss [ 0.08088226  0.99175334 10.478199   27.670738  ]\n",
      "Training loss [ 0.09984381  1.0105093  10.488334   27.085705  ]\n",
      "Training loss [ 0.07198226  0.9887948  10.167063   26.552902  ]\n",
      "Training loss [ 0.08626735  1.0199227  11.199598   28.521763  ]\n",
      "Training loss [ 0.0909563  1.0163436 10.838968  27.965515 ]\n",
      "Training loss [ 0.06498564  0.99175334 10.477858   27.670738  ]\n",
      "Training loss [ 0.08821358  1.0105093  10.487053   27.085705  ]\n",
      "Training loss [ 0.06360587  0.9887948  10.166486   26.552902  ]\n",
      "Training loss [ 0.07698593  1.0199227  11.200629   28.521763  ]\n",
      "Training loss [ 0.07820596  1.0163436  10.837847   27.965515  ]\n",
      "Training loss [ 0.05895751  0.99175334 10.477972   27.670738  ]\n",
      "Training loss [ 0.08285333  1.0105093  10.48483    27.085705  ]\n",
      "Training loss [ 0.05893393  0.9887948  10.166418   26.552902  ]\n",
      "Training loss [ 0.07234401  1.0199227  11.200605   28.521763  ]\n",
      "Training loss [ 0.07378    1.0163436 10.836261  27.965515 ]\n",
      "Training loss [ 0.0557136   0.99175334 10.47816    27.670738  ]\n",
      "Training loss [ 0.0783281  1.0105093 10.483764  27.085705 ]\n",
      "Training loss [ 0.05708566  0.9887948  10.165788   26.552902  ]\n",
      "Training loss [ 7.140185 18.079773 23.294098 44.604004]\n",
      "Training loss [ 0.67910975  1.0035282  10.015715   24.981739  ]\n",
      "Training loss [ 0.28504813  0.997457    8.844412   24.833458  ]\n",
      "Training loss [ 0.14295627  1.0006132   8.88221    24.898937  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.12026537  0.99213547  9.079903   24.955915  ]\n",
      "Training loss [ 0.12025584  1.0047238   9.297616   25.134441  ]\n",
      "Training loss [ 0.09408355  1.0034895   8.978411   24.981739  ]\n",
      "Training loss [ 0.09143969  0.997457    8.236439   24.833458  ]\n",
      "Training loss [ 0.0748037  1.0006132  8.168304  24.898937 ]\n",
      "Training loss [ 0.08276089  0.99213547  8.391602   24.955915  ]\n",
      "Training loss [ 0.09292433  1.0047238   8.6175165  25.134441  ]\n",
      "Training loss [ 0.07803158  1.0034895   8.313478   24.981739  ]\n",
      "Training loss [ 0.07675248  0.997457    8.105979   24.833458  ]\n",
      "Training loss [ 0.06831858  1.0006132   8.154024   24.898937  ]\n",
      "Training loss [ 0.07451482  0.99213547  8.388823   24.955915  ]\n",
      "Training loss [ 0.0859832  1.0047238  8.464395  25.134441 ]\n",
      "Training loss [ 0.07379936  1.0034895   8.173502   24.981739  ]\n",
      "Training loss [ 0.07030933  0.997457    7.982623   24.833458  ]\n",
      "Training loss [ 0.06516843  1.0006132   8.029134   24.898937  ]\n",
      "Training loss [ 0.07120109  0.99213547  8.301313   24.955915  ]\n",
      "Training loss [ 0.08127528  1.0047238   8.457306   25.134441  ]\n",
      "Training loss [ 0.07232022  1.0034895   8.171481   24.981739  ]\n",
      "Training loss [ 0.06689689  0.997457    7.9812975  24.833458  ]\n",
      "Training loss [ 0.0636684  1.0006132  7.6961727 24.898937 ]\n",
      "Training loss [ 0.0687806   0.99213547  7.911003   24.955915  ]\n",
      "Training loss [ 4.21032  13.878855 25.051998 51.24317 ]\n",
      "Training loss [ 0.44443274  1.0017433  10.518616   28.344646  ]\n",
      "Training loss [ 0.1888277  0.9952233  9.091046  27.285294 ]\n",
      "Training loss [ 0.11577039  1.01665     9.250351   27.318531  ]\n",
      "Training loss [ 0.09015361  1.0115563   8.4203205  26.373688  ]\n",
      "Training loss [ 0.08963651  0.97825605  8.626295   27.110483  ]\n",
      "Training loss [ 0.08835225  1.0017235   8.779598   28.344124  ]\n",
      "Training loss [ 0.07246795  0.9952233   8.508842   27.285294  ]\n",
      "Training loss [ 0.07449378  1.01665     8.572591   27.318531  ]\n",
      "Training loss [ 0.06640133  1.0115563   8.238071   26.373688  ]\n",
      "Training loss [ 0.06642883  0.97825605  8.466684   27.110483  ]\n",
      "Training loss [ 0.07088993  1.0017235   8.773562   28.344124  ]\n",
      "Training loss [ 0.06330379  0.9952233   8.51293    27.285294  ]\n",
      "Training loss [ 0.06902126  1.01665     8.569904   27.318531  ]\n",
      "Training loss [ 0.06062058  1.0115563   8.236157   26.373688  ]\n",
      "Training loss [ 0.05967529  0.97825605  8.467089   27.110483  ]\n",
      "Training loss [ 0.06545467  1.0017235   8.771551   28.344124  ]\n",
      "Training loss [ 0.05938847  0.9952233   8.516958   27.285294  ]\n",
      "Training loss [ 0.06510991  1.01665     8.568584   27.318531  ]\n",
      "Training loss [ 0.05460662  1.0115563   8.234381   26.373688  ]\n",
      "Training loss [ 0.05620679  0.97825605  8.46666    27.110483  ]\n",
      "Training loss [ 0.06174017  1.0017235   8.770417   28.344124  ]\n",
      "Training loss [ 0.05670732  0.9952233   8.522154   27.285294  ]\n",
      "Training loss [ 0.06262445  1.01665     8.56743    27.318531  ]\n",
      "Training loss [ 0.05240948  1.0115563   8.233424   26.373688  ]\n",
      "Training loss [ 2.7111573  9.968033  21.894043  45.819756 ]\n",
      "Training loss [ 0.3068226   0.96817076  8.742086   23.670097  ]\n",
      "Training loss [ 0.14268403  0.95026404  8.507353   24.323383  ]\n",
      "Training loss [ 0.0813496   0.93696487  8.017047   23.373476  ]\n",
      "Training loss [ 0.06844339  0.9530786   8.7000265  24.58062   ]\n",
      "Training loss [ 0.08401427  0.93926036  8.460976   23.882587  ]\n",
      "Training loss [ 0.08350109  0.96817076  8.312943   23.670097  ]\n",
      "Training loss [ 0.06173566  0.95026404  8.316645   24.323383  ]\n",
      "Training loss [ 0.04908276  0.93696487  7.86849    23.373476  ]\n",
      "Training loss [ 0.04812293  0.9530655   8.680298   24.58062   ]\n",
      "Training loss [ 0.06515045  0.93926036  8.420439   23.882587  ]\n",
      "Training loss [ 0.07309848  0.96817076  8.264734   23.670097  ]\n",
      "Training loss [ 0.05151847  0.95026404  7.835152   24.323383  ]\n",
      "Training loss [ 0.04110528  0.93696487  7.3685684  23.373476  ]\n",
      "Training loss [ 0.03916875  0.9530643   8.142304   24.58062   ]\n",
      "Training loss [ 0.05634333  0.93926036  7.8505306  23.882587  ]\n",
      "Training loss [ 0.06714673  0.96817076  7.771191   23.670097  ]\n",
      "Training loss [ 0.04650294  0.95026404  7.7939725  24.323383  ]\n",
      "Training loss [ 0.03764448  0.93696487  7.33785    23.373476  ]\n",
      "Training loss [ 0.03730087  0.9530643   8.159182   24.58062   ]\n",
      "Training loss [ 0.05201287  0.93926036  7.8524995  23.882587  ]\n",
      "Training loss [ 0.0638447   0.96817076  7.7689867  23.670097  ]\n",
      "Training loss [ 0.0453456   0.95026404  7.7939754  24.323383  ]\n",
      "Training loss [ 0.03519768  0.93696487  7.336871   23.373476  ]\n",
      "Training loss [ 0.03782889  0.9530643   8.156619   24.58062   ]\n",
      "Training loss [ 3.4622362 10.137284  23.047977  40.26504  ]\n",
      "Training loss [ 0.31674063  0.98553264  9.648611   24.244303  ]\n",
      "Training loss [ 0.1554845  0.9755301  8.921909  24.032051 ]\n",
      "Training loss [ 0.10993813  0.9819516   8.847593   24.607498  ]\n",
      "Training loss [ 0.09389736  0.9591211   8.780769   24.225147  ]\n",
      "Training loss [ 0.07448073  0.9702221   8.646982   24.43512   ]\n",
      "Training loss [ 0.07450919  0.9855105   8.58095    24.244282  ]\n",
      "Training loss [ 0.0740201   0.97557706  8.671878   24.032051  ]\n",
      "Training loss [ 0.06889002  0.9819516   8.703166   24.607498  ]\n",
      "Training loss [ 0.06367581  0.9591211   8.475519   24.225147  ]\n",
      "Training loss [ 0.05855779  0.9702221   8.347805   24.43512   ]\n",
      "Training loss [ 0.06014227  0.9855105   8.379883   24.244282  ]\n",
      "Training loss [ 0.06573378  0.9755301   8.470371   24.032051  ]\n",
      "Training loss [ 0.06115156  0.9819516   8.517851   24.607498  ]\n",
      "Training loss [ 0.05621434  0.9591211   8.454746   24.225147  ]\n",
      "Training loss [ 0.05251282  0.9702221   8.266161   24.43512   ]\n",
      "Training loss [ 0.05266068  0.9855105   8.306374   24.244282  ]\n",
      "Training loss [ 0.05446135  0.9755301   8.387825   24.032051  ]\n",
      "Training loss [ 0.05225134  0.9819516   8.42866    24.607498  ]\n",
      "Training loss [ 0.04614869  0.9591211   8.379055   24.225147  ]\n",
      "Training loss [ 0.04455692  0.9702221   8.259042   24.43512   ]\n",
      "Training loss [ 0.04823756  0.9855105   8.306173   24.244282  ]\n",
      "Training loss [ 0.05069698  0.9755301   8.385422   24.032051  ]\n",
      "Training loss [ 0.04940588  0.9819516   8.427269   24.607498  ]\n",
      "Training loss [ 0.04490911  0.9591211   8.329678   24.225147  ]\n",
      "Training loss [ 4.9037676  9.907909  22.080858  47.991306 ]\n",
      "Training loss [ 0.3244806   0.97052896  9.243978   24.250324  ]\n",
      "Training loss [ 0.1397346   0.96945673  9.289158   24.977676  ]\n",
      "Training loss [ 0.09222782  0.9908636   8.937385   24.904896  ]\n",
      "Training loss [ 0.07214697  0.9812687   9.04956    25.45908   ]\n",
      "Training loss [ 0.06965355  0.97307724  8.584347   23.993095  ]\n",
      "Training loss [ 0.067076    0.97052896  8.429962   24.250319  ]\n",
      "Training loss [ 0.05509881  0.96945673  8.954099   24.977676  ]\n",
      "Training loss [ 0.06194482  0.9908636   8.919498   24.904896  ]\n",
      "Training loss [ 0.05430296  0.9812687   9.044296   25.45908   ]\n",
      "Training loss [ 0.05147997  0.97307724  8.580617   23.993095  ]\n",
      "Training loss [ 0.05433083  0.97052896  8.4280205  24.250319  ]\n",
      "Training loss [ 0.04524231  0.96945673  8.952369   24.977676  ]\n",
      "Training loss [ 0.0540953  0.9908636  8.914417  24.904896 ]\n",
      "Training loss [ 0.05211936  0.9812687   8.791775   25.45908   ]\n",
      "Training loss [ 0.04657479  0.97307724  8.305157   23.993095  ]\n",
      "Training loss [ 0.05014353  0.97052896  8.19306    24.250319  ]\n",
      "Training loss [ 0.04245623  0.96945673  8.687021   24.977676  ]\n",
      "Training loss [ 0.0552672  0.9908636  8.45323   24.904896 ]\n",
      "Training loss [ 0.04951476  0.9812687   8.580989   25.45908   ]\n",
      "Training loss [ 0.04424411  0.97307724  8.097391   23.993095  ]\n",
      "Training loss [ 0.04711507  0.97052896  7.993293   24.250319  ]\n",
      "Training loss [ 0.03955815  0.96945673  8.512035   24.977676  ]\n",
      "Training loss [ 0.0494048  0.9908636  8.438927  24.904896 ]\n",
      "Training loss [ 0.04435314  0.9812687   8.530662   25.45908   ]\n",
      "Training loss [ 4.7766714  9.587908  25.361908  48.939026 ]\n",
      "Training loss [ 0.3236508   0.97656107  9.539108   26.62067   ]\n",
      "Training loss [ 0.12863663  0.9707717   8.716299   26.054214  ]\n",
      "Training loss [ 0.08210237  0.9818186   8.653041   26.557545  ]\n",
      "Training loss [ 0.07504347  0.9706595   8.567783   26.787106  ]\n",
      "Training loss [ 0.05634171  0.96802735  8.508934   27.200321  ]\n",
      "Training loss [ 0.05902864  0.9765525   8.463151   26.62067   ]\n",
      "Training loss [ 0.05079353  0.9707717   8.203061   26.054214  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.0356297  0.9818186  8.215592  26.557545 ]\n",
      "Training loss [ 0.0507601  0.9706595  8.177276  26.787106 ]\n",
      "Training loss [ 0.0424278   0.96802735  8.27759    27.200321  ]\n",
      "Training loss [ 0.04735438  0.9765525   8.322617   26.62067   ]\n",
      "Training loss [ 0.04448491  0.9707717   8.085312   26.054214  ]\n",
      "Training loss [ 0.02849808  0.9818186   8.144609   26.557545  ]\n",
      "Training loss [ 0.04410798  0.9706595   8.036292   26.787106  ]\n",
      "Training loss [ 0.03618647  0.96802735  7.9835496  27.200321  ]\n",
      "Training loss [ 0.03993969  0.9765525   8.018708   26.62067   ]\n",
      "Training loss [ 0.0401534  0.9707717  7.741043  26.054214 ]\n",
      "Training loss [2.6395913e-02 9.8181862e-01 7.7943792e+00 2.6557545e+01]\n",
      "Training loss [ 0.04152545  0.9706595   7.810253   26.787106  ]\n",
      "Training loss [ 0.03407871  0.96802735  7.924403   27.200321  ]\n",
      "Training loss [ 0.03544698  0.9765525   7.948951   26.62067   ]\n",
      "Training loss [ 0.04545981  0.9707717   7.7407985  26.054214  ]\n",
      "Training loss [2.5714956e-02 9.8181862e-01 7.7122488e+00 2.6557545e+01]\n",
      "Training loss [ 0.03979318  0.9706595   7.71479    26.787106  ]\n",
      "Training loss [ 3.7136471  7.917406  23.00394   42.75605  ]\n",
      "Training loss [ 0.29238594  1.029316    9.9595375  27.371542  ]\n",
      "Training loss [ 0.1033504  1.0357035  8.975585  26.078184 ]\n",
      "Training loss [ 0.07276051  1.0338662   9.02904    26.384125  ]\n",
      "Training loss [ 0.05936586  1.0391749   8.566208   25.497208  ]\n",
      "Training loss [ 0.05963382  1.0342834   8.23109    24.42212   ]\n",
      "Training loss [ 0.04587349  1.029316    9.350272   27.371538  ]\n",
      "Training loss [ 0.04785053  1.0357035   8.830386   26.078184  ]\n",
      "Training loss [ 0.04165528  1.0338662   8.934158   26.384125  ]\n",
      "Training loss [ 0.04279719  1.0391749   8.498852   25.497208  ]\n",
      "Training loss [ 0.04507111  1.0342834   8.218725   24.42212   ]\n",
      "Training loss [ 0.03802242  1.029316    9.312257   27.371538  ]\n",
      "Training loss [ 0.03919682  1.0357035   8.799526   26.078184  ]\n",
      "Training loss [ 0.03721108  1.0338662   8.871567   26.384125  ]\n",
      "Training loss [ 0.03858231  1.0391749   8.426555   25.497208  ]\n",
      "Training loss [ 0.04162415  1.0342834   8.111542   24.42212   ]\n",
      "Training loss [ 0.03374603  1.029316    9.149796   27.371538  ]\n",
      "Training loss [ 0.03883115  1.0357035   8.56805    26.078184  ]\n",
      "Training loss [ 0.03602975  1.0338662   8.646214   26.384125  ]\n",
      "Training loss [ 0.03471186  1.0391749   8.210032   25.497208  ]\n",
      "Training loss [ 0.04026096  1.0342834   7.9170628  24.42212   ]\n",
      "Training loss [ 0.02950633  1.029316    8.877038   27.371538  ]\n",
      "Training loss [ 0.03293064  1.0357035   8.374327   26.078184  ]\n",
      "Training loss [ 0.0312775  1.0338662  8.389082  26.384125 ]\n",
      "Training loss [ 0.03285183  1.0391749   8.00253    25.497208  ]\n",
      "Training loss [ 3.784569  8.386735 25.557503 42.59629 ]\n",
      "Training loss [ 0.25411117  0.997393   10.181763   26.150879  ]\n",
      "Training loss [ 0.10438275  0.9993003   8.778923   25.255749  ]\n",
      "Training loss [ 0.08094201  0.99046683  8.882565   25.546556  ]\n",
      "Training loss [ 0.06223182  0.98895013  9.253197   26.71127   ]\n",
      "Training loss [ 0.07074104  0.99145126  9.578756   26.978336  ]\n",
      "Training loss [ 0.04175964  0.9971105   9.05316    26.150879  ]\n",
      "Training loss [ 0.04149478  0.9993003   8.731152   25.255749  ]\n",
      "Training loss [ 0.03793432  0.99046683  8.768641   25.54631   ]\n",
      "Training loss [ 0.04600105  0.98895013  9.147022   26.71127   ]\n",
      "Training loss [ 0.04858894  0.99145126  9.50596    26.978336  ]\n",
      "Training loss [ 0.02942133  0.9971105   8.979475   26.150879  ]\n",
      "Training loss [ 0.03203417  0.9993003   8.538699   25.255749  ]\n",
      "Training loss [ 0.03221558  0.99046683  8.61591    25.54631   ]\n",
      "Training loss [ 0.0405561   0.98895013  8.99003    26.71127   ]\n",
      "Training loss [ 0.04308503  0.99145126  9.347187   26.978336  ]\n",
      "Training loss [ 0.02618353  0.9971105   8.822277   26.150879  ]\n",
      "Training loss [ 0.02716623  0.9993003   8.442912   25.255749  ]\n",
      "Training loss [ 0.03028816  0.99046683  8.579115   25.54631   ]\n",
      "Training loss [ 0.03686021  0.98895013  8.892748   26.71127   ]\n",
      "Training loss [ 0.04017504  0.99145126  9.195502   26.978336  ]\n",
      "Training loss [2.5442027e-02 9.9711049e-01 8.6871462e+00 2.6150879e+01]\n",
      "Training loss [2.4814837e-02 9.9930030e-01 8.3280516e+00 2.5255749e+01]\n",
      "Training loss [ 0.02910069  0.99046683  8.480853   25.54631   ]\n",
      "Training loss [ 0.03517412  0.98895013  8.858572   26.71127   ]\n",
      "Training loss [ 2.6859624  5.664189  24.981815  40.609238 ]\n",
      "Training loss [ 0.23136963  0.9837936   9.936979   26.862753  ]\n",
      "Training loss [ 0.11210941  1.0047767   8.880732   25.22414   ]\n",
      "Training loss [ 0.09132926  0.9702077   9.487281   26.588577  ]\n",
      "Training loss [ 0.05833934  0.97881746  8.98575    26.507029  ]\n",
      "Training loss [ 0.05580175  0.97792906  8.919786   26.461678  ]\n",
      "Training loss [ 0.04792802  0.9837936   8.97884    26.862753  ]\n",
      "Training loss [ 0.04720725  1.0047767   8.420248   25.22414   ]\n",
      "Training loss [ 0.04481782  0.9702077   9.044291   26.588577  ]\n",
      "Training loss [ 0.03007069  0.97881746  8.692812   26.507029  ]\n",
      "Training loss [ 0.03963863  0.97792906  8.939015   26.461678  ]\n",
      "Training loss [ 0.03427873  0.9837936   8.846003   26.862753  ]\n",
      "Training loss [ 0.04408566  1.0047767   8.191594   25.22414   ]\n",
      "Training loss [ 0.03549899  0.9702077   8.912445   26.588577  ]\n",
      "Training loss [2.5602346e-02 9.7881746e-01 8.5984383e+00 2.6507029e+01]\n",
      "Training loss [ 0.03748678  0.97792906  8.72724    26.461678  ]\n",
      "Training loss [ 0.02858628  0.9837936   8.543196   26.862753  ]\n",
      "Training loss [ 0.03524783  1.0047767   7.9353004  25.22414   ]\n",
      "Training loss [ 0.03155611  0.9702077   8.669042   26.588577  ]\n",
      "Training loss [2.3432612e-02 9.7881746e-01 8.3594360e+00 2.6507029e+01]\n",
      "Training loss [ 0.03309582  0.97792906  8.266972   26.461678  ]\n",
      "Training loss [2.6467549e-02 9.8379362e-01 8.2186823e+00 2.6862753e+01]\n",
      "Training loss [ 0.03284251  1.0047767   7.7287755  25.22414   ]\n",
      "Training loss [ 0.03042312  0.9702077   8.405167   26.588577  ]\n",
      "Training loss [2.2612050e-02 9.7881746e-01 8.0956726e+00 2.6507029e+01]\n",
      "Training loss [ 2.279776  5.885055 24.322601 36.993088]\n",
      "Training loss [ 0.2891366  1.013844   9.922676  25.229927 ]\n",
      "Training loss [ 0.19394632  1.0170562   9.5677395  25.482965  ]\n",
      "Training loss [ 0.1081467  1.017728   9.017527  24.51325  ]\n",
      "Training loss [ 0.08817977  1.0116652   9.437824   26.09525   ]\n",
      "Training loss [ 0.087615   1.0138824  9.170005  25.492191 ]\n",
      "Training loss [ 0.06289298  1.013844    9.010123   25.22961   ]\n",
      "Training loss [ 0.08559363  1.0170562   9.066744   25.482965  ]\n",
      "Training loss [ 0.08925582  1.017728    8.650121   24.51325   ]\n",
      "Training loss [ 0.05852176  1.0116652   9.076793   26.09525   ]\n",
      "Training loss [ 0.06295998  1.0138824   9.038922   25.492191  ]\n",
      "Training loss [ 0.04881977  1.013844    8.472028   25.22961   ]\n",
      "Training loss [ 0.07104705  1.0170562   8.687412   25.482965  ]\n",
      "Training loss [ 0.05905647  1.017728    8.332971   24.51325   ]\n",
      "Training loss [ 0.0502001  1.0116652  8.781754  26.09525  ]\n",
      "Training loss [ 0.04580402  1.0138824   8.763357   25.492191  ]\n",
      "Training loss [ 0.03964026  1.013844    8.263313   25.22961   ]\n",
      "Training loss [ 0.05834084  1.0170562   8.547832   25.482965  ]\n",
      "Training loss [ 0.05299287  1.017728    8.151753   24.51325   ]\n",
      "Training loss [ 0.04301425  1.0116652   8.572632   26.09525   ]\n",
      "Training loss [ 0.03984521  1.0138824   8.420627   25.492191  ]\n",
      "Training loss [ 0.03311365  1.013844    8.158098   25.22961   ]\n",
      "Training loss [ 0.05632642  1.0170562   8.436546   25.482965  ]\n",
      "Training loss [ 0.04390173  1.017728    8.012761   24.51325   ]\n",
      "Training loss [ 0.039739   1.0116652  8.46411   26.09525  ]\n",
      "Training loss [ 1.9265943  5.6305113 25.392265  35.22563  ]\n",
      "Training loss [ 0.3263007  0.9903762 10.397539  25.725094 ]\n",
      "Training loss [ 0.14865899  0.98564816  9.612785   25.953897  ]\n",
      "Training loss [ 0.11211238  0.9831963   9.351324   26.163267  ]\n",
      "Training loss [ 0.10393631  0.9791398   9.657478   26.348536  ]\n",
      "Training loss [ 0.08990296  0.98355055  9.549328   26.361507  ]\n",
      "Training loss [ 0.08288486  0.99027723  9.117995   25.725094  ]\n",
      "Training loss [ 0.08103763  0.98564816  8.876288   25.953897  ]\n",
      "Training loss [ 0.07559159  0.9831963   8.601718   26.163267  ]\n",
      "Training loss [ 0.07520049  0.9791398   8.942714   26.348536  ]\n",
      "Training loss [ 0.07690828  0.98355055  8.880085   26.361507  ]\n",
      "Training loss [ 0.06940249  0.99027723  8.370963   25.725094  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.05646394  0.98564816  8.395363   25.953897  ]\n",
      "Training loss [ 0.04495536  0.9831963   8.425614   26.163267  ]\n",
      "Training loss [ 0.05469018  0.9791398   8.4811325  26.348536  ]\n",
      "Training loss [ 0.06808673  0.98355055  8.543074   26.361507  ]\n",
      "Training loss [ 0.0545918   0.99027723  7.9887276  25.725094  ]\n",
      "Training loss [ 0.0464001   0.98564816  8.048167   25.953897  ]\n",
      "Training loss [ 0.03912783  0.9831963   7.976708   26.163267  ]\n",
      "Training loss [ 0.05014916  0.9791398   8.151577   26.348536  ]\n",
      "Training loss [ 0.05705524  0.98355055  8.262003   26.361507  ]\n",
      "Training loss [ 0.05222134  0.99027723  7.813186   25.725094  ]\n",
      "Training loss [ 0.04366169  0.98564816  7.9297924  25.953897  ]\n",
      "Training loss [ 0.03700507  0.9831963   7.890239   26.163267  ]\n",
      "Training loss [ 0.04884699  0.9791398   8.090842   26.348536  ]\n",
      "Training loss [ 1.2140708  5.234164  20.489864  29.934832 ]\n",
      "Training loss [ 0.42571312  0.94388306  8.0031185  22.12343   ]\n",
      "Training loss [ 0.41720054  1.0599542   7.52233    21.612473  ]\n",
      "Training loss [ 0.39597493  1.0732172   6.837111   21.185425  ]\n",
      "Training loss [ 0.3619503  1.0055714  7.259282  22.093534 ]\n",
      "Training loss [ 0.36320573  0.9374206   7.5048685  21.503775  ]\n",
      "Training loss [ 0.3657445   0.94388306  6.963899   22.12343   ]\n",
      "Training loss [ 0.38829443  1.0598505   7.2680635  21.612473  ]\n",
      "Training loss [ 0.3711653  1.0732172  6.686871  21.185425 ]\n",
      "Training loss [ 0.3456648  1.0055714  7.018469  22.093534 ]\n",
      "Training loss [ 0.35301805  0.9374206   7.4070444  21.503775  ]\n",
      "Training loss [ 0.3600977   0.94388306  6.8487215  22.12343   ]\n",
      "Training loss [ 0.37943655  1.0598505   7.2135124  21.612473  ]\n",
      "Training loss [ 0.36995524  1.0732172   6.6263824  21.185425  ]\n",
      "Training loss [ 0.34119487  1.0055714   6.961605   22.093534  ]\n",
      "Training loss [ 0.34876645  0.9374206   7.360207   21.503775  ]\n",
      "Training loss [ 0.3574372   0.94388306  6.814411   22.12343   ]\n",
      "Training loss [ 0.37552565  1.0598505   7.1897116  21.612473  ]\n",
      "Training loss [ 0.36528298  1.0732172   6.605676   21.185425  ]\n",
      "Training loss [ 0.3361203  1.0055714  6.9489374 22.093534 ]\n",
      "Training loss [ 0.34730527  0.9374206   7.3438745  21.503775  ]\n",
      "Training loss [ 0.35539255  0.94388306  6.807254   22.12343   ]\n",
      "Training loss [ 0.37440383  1.0598505   7.1848187  21.612473  ]\n",
      "Training loss [ 0.36249694  1.0732172   6.5993557  21.185425  ]\n",
      "Training loss [ 0.33404118  1.0055714   6.9474154  22.093534  ]\n",
      "Training loss [ 1.1764617  8.558702  24.963684  34.702675 ]\n",
      "Training loss [ 0.43888703  1.0503439  16.700151   28.633814  ]\n",
      "Training loss [ 0.40313795  1.0384963  13.922397   26.737549  ]\n",
      "Training loss [ 0.38522497  1.1527263  11.857887   25.448435  ]\n",
      "Training loss [ 0.3496679  1.1003892 11.702891  26.844414 ]\n",
      "Training loss [ 0.34929374  1.0609117   7.4911118  26.25961   ]\n",
      "Training loss [ 0.2793394  1.0503439  8.010208  28.631958 ]\n",
      "Training loss [ 0.30601108  1.0384963   7.5881395  26.737549  ]\n",
      "Training loss [ 0.29888955  1.1527263   6.95662    25.448435  ]\n",
      "Training loss [ 0.29940927  1.1003892   7.003928   26.844414  ]\n",
      "Training loss [ 0.32321116  1.0609117   7.0793724  26.25961   ]\n",
      "Training loss [ 0.2543621  1.0503439  7.72019   28.631958 ]\n",
      "Training loss [ 0.2935441  1.0384963  7.493246  26.737549 ]\n",
      "Training loss [ 0.2800582  1.1527263  6.9521966 25.448435 ]\n",
      "Training loss [ 0.28354168  1.1003892   6.961936   26.844414  ]\n",
      "Training loss [ 0.30881906  1.0609117   7.0592594  26.25961   ]\n",
      "Training loss [ 0.24601072  1.0503439   7.715892   28.631958  ]\n",
      "Training loss [ 0.28758478  1.0384963   7.4747877  26.737549  ]\n",
      "Training loss [ 0.27521637  1.1527263   6.958745   25.448435  ]\n",
      "Training loss [ 0.27681074  1.1003892   6.9522038  26.844414  ]\n",
      "Training loss [ 0.2994744  1.0609117  7.055548  26.25961  ]\n",
      "Training loss [ 0.24428585  1.0503439   7.7201223  28.631958  ]\n",
      "Training loss [ 0.2837706  1.0384963  7.469766  26.737549 ]\n",
      "Training loss [ 0.26098832  1.1527263   6.9603505  25.448435  ]\n",
      "Training loss [ 0.26645762  1.1003892   6.9512424  26.844414  ]\n",
      "Training loss [ 2.7369294  3.3832507 20.301384  37.742867 ]\n",
      "Training loss [ 0.33727336  0.79285514  6.3115544  19.848787  ]\n",
      "Training loss [ 0.28307107  0.80699515  6.161849   20.40601   ]\n",
      "Training loss [ 0.2424334   0.75838876  6.596745   21.375183  ]\n",
      "Training loss [ 0.22975911  0.78773296  6.035233   21.2412    ]\n",
      "Training loss [ 0.22038472  0.83701587  5.5887575  21.550957  ]\n",
      "Training loss [ 0.21319968  0.7928275   5.439377   19.848766  ]\n",
      "Training loss [ 0.23293507  0.80699515  5.962924   20.40601   ]\n",
      "Training loss [ 0.22460106  0.75831795  6.4956446  21.37487   ]\n",
      "Training loss [ 0.21195042  0.78773296  5.944333   21.2412    ]\n",
      "Training loss [ 0.20709327  0.83701587  5.5564165  21.550957  ]\n",
      "Training loss [ 0.19533297  0.7928275   5.4256744  19.848766  ]\n",
      "Training loss [ 0.2234391   0.80699515  5.9587383  20.40601   ]\n",
      "Training loss [ 0.21646228  0.75831795  6.500169   21.37487   ]\n",
      "Training loss [ 0.20477492  0.78773296  5.9382973  21.2412    ]\n",
      "Training loss [ 0.19937746  0.83701587  5.554066   21.550957  ]\n",
      "Training loss [ 0.18532887  0.7928275   5.424817   19.848766  ]\n",
      "Training loss [ 0.21531513  0.80699515  5.957258   20.40601   ]\n",
      "Training loss [ 0.20620899  0.75831795  6.500303   21.37487   ]\n",
      "Training loss [ 0.16767944  0.78773296  5.9388986  21.2412    ]\n",
      "Training loss [ 0.16407602  0.83701587  5.5558863  21.550957  ]\n",
      "Training loss [ 0.16214569  0.7928275   5.425144   19.848766  ]\n",
      "Training loss [ 0.1813239   0.80699515  5.9567175  20.40601   ]\n",
      "Training loss [ 0.16704604  0.75831795  6.5007057  21.37487   ]\n",
      "Training loss [ 0.15704235  0.78773296  5.939905   21.2412    ]\n",
      "Training loss [ 3.6914563 10.50006   25.158007  39.38311  ]\n",
      "Training loss [ 0.41279405  0.9824992  10.044389   26.350681  ]\n",
      "Training loss [ 0.28251705  0.9404598   8.996515   24.562832  ]\n",
      "Training loss [ 0.28630635  0.98356926  8.218439   23.991453  ]\n",
      "Training loss [ 0.24779356  0.9811624   8.89356    25.99578   ]\n",
      "Training loss [ 0.24416158  0.9515687   8.741968   26.12529   ]\n",
      "Training loss [ 0.23911375  0.9824992   8.546461   26.350681  ]\n",
      "Training loss [ 0.21369053  0.94012856  8.155742   24.562832  ]\n",
      "Training loss [ 0.22859396  0.98356926  7.9960213  23.991453  ]\n",
      "Training loss [ 0.2001482  0.9811624  8.740012  25.99578  ]\n",
      "Training loss [ 0.21608263  0.9515687   8.718062   26.12529   ]\n",
      "Training loss [ 0.20658821  0.9824992   8.515461   26.350681  ]\n",
      "Training loss [ 0.19633576  0.94012856  8.140653   24.562832  ]\n",
      "Training loss [ 0.20507461  0.98356926  7.9787583  23.991453  ]\n",
      "Training loss [ 0.18943419  0.9811624   8.728144   25.99578   ]\n",
      "Training loss [ 0.2073752  0.9515687  8.7177305 26.12529  ]\n",
      "Training loss [ 0.1880445  0.9824992  8.514618  26.350681 ]\n",
      "Training loss [ 0.18127376  0.94012856  8.1385     24.562832  ]\n",
      "Training loss [ 0.19220746  0.98356926  7.9774904  23.991453  ]\n",
      "Training loss [ 0.1704096  0.9811624  8.727221  25.99578  ]\n",
      "Training loss [ 0.1993159  0.9515687  8.7175255 26.12529  ]\n",
      "Training loss [ 0.1779685  0.9824992  8.514427  26.350681 ]\n",
      "Training loss [ 0.17476252  0.94012856  8.137636   24.562832  ]\n",
      "Training loss [ 0.18567908  0.98356926  7.977645   23.991453  ]\n",
      "Training loss [ 0.16488846  0.9811624   8.727927   25.99578   ]\n",
      "Training loss [ 1.7411151  4.6736135 19.975006  37.05133  ]\n",
      "Training loss [ 0.49520725  1.0976943   9.508028   22.555925  ]\n",
      "Training loss [ 0.35311663  1.0600735   9.728899   23.501112  ]\n",
      "Training loss [ 0.31130022  1.0561323   8.735777   21.2479    ]\n",
      "Training loss [ 0.29262123  1.0850455   9.009919   22.822634  ]\n",
      "Training loss [ 0.29720896  1.0742302   8.283483   21.835072  ]\n",
      "Training loss [ 0.25383085  1.0976943   8.073335   22.555925  ]\n",
      "Training loss [ 0.260153   1.0600735  8.6889715 23.501085 ]\n",
      "Training loss [ 0.24835716  1.0561323   8.003923   21.2479    ]\n",
      "Training loss [ 0.25701565  1.0850455   8.509114   22.822634  ]\n",
      "Training loss [ 0.27099776  1.0742302   8.17799    21.835072  ]\n",
      "Training loss [ 0.21571323  1.0976943   8.024366   22.555925  ]\n",
      "Training loss [ 0.2431956  1.0600735  8.703861  23.501085 ]\n",
      "Training loss [ 0.23125553  1.0561323   8.011031   21.2479    ]\n",
      "Training loss [ 0.23986168  1.0850455   8.504555   22.822634  ]\n",
      "Training loss [ 0.2558342  1.0742302  8.17639   21.835072 ]\n",
      "Training loss [ 0.2040776  1.0976943  8.020804  22.555925 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.23609775  1.0600735   8.706379   23.501085  ]\n",
      "Training loss [ 0.21333688  1.0561323   8.010463   21.2479    ]\n",
      "Training loss [ 0.199229   1.0850455  8.504101  22.822634 ]\n",
      "Training loss [ 0.22431254  1.0742302   8.176477   21.835072  ]\n",
      "Training loss [ 0.16516268  1.0976943   8.018515   22.555925  ]\n",
      "Training loss [ 0.20451313  1.0600735   8.706489   23.501085  ]\n",
      "Training loss [ 0.19111526  1.0561323   8.011137   21.2479    ]\n",
      "Training loss [ 0.18569797  1.0850455   8.50387    22.822634  ]\n",
      "Training loss [ 5.5971365  6.702688  21.749493  33.005074 ]\n",
      "Training loss [ 0.6055456  1.0239326  8.1464405 23.710346 ]\n",
      "Training loss [ 0.35918373  1.0107973   7.1780434  23.373638  ]\n",
      "Training loss [ 0.30520484  1.0672398   7.0545745  22.299353  ]\n",
      "Training loss [ 0.24341361  1.0100548   7.0501175  22.79647   ]\n",
      "Training loss [ 0.23860939  0.9949517   7.1568727  22.994953  ]\n",
      "Training loss [ 0.22701453  1.0237255   7.3832884  23.710346  ]\n",
      "Training loss [ 0.2196485  1.0107973  6.967614  23.373638 ]\n",
      "Training loss [ 0.23793697  1.0672398   7.013051   22.299353  ]\n",
      "Training loss [ 0.17731106  1.0100548   7.0216966  22.79647   ]\n",
      "Training loss [ 0.19700812  0.9949517   7.1602283  22.994953  ]\n",
      "Training loss [ 0.19132292  1.0237255   7.3806124  23.710346  ]\n",
      "Training loss [ 0.19492811  1.0107973   6.9688225  23.373638  ]\n",
      "Training loss [ 0.21259843  1.0672398   7.0132847  22.299353  ]\n",
      "Training loss [ 0.15627134  1.0100548   7.0205727  22.79647   ]\n",
      "Training loss [ 0.17689206  0.9949517   7.160213   22.994953  ]\n",
      "Training loss [ 0.17501873  1.0237255   7.379878   23.710346  ]\n",
      "Training loss [ 0.18218687  1.0107973   6.9688005  23.373638  ]\n",
      "Training loss [ 0.20579639  1.0672398   7.013301   22.299353  ]\n",
      "Training loss [ 0.1498881  1.0100548  7.02015   22.79647  ]\n",
      "Training loss [ 0.16667509  0.9949517   7.1607304  22.994953  ]\n",
      "Training loss [ 0.16649932  1.0237255   7.379073   23.710346  ]\n",
      "Training loss [ 0.17522812  1.0107973   6.967926   23.373638  ]\n",
      "Training loss [ 0.19935429  1.0672398   7.0130887  22.299353  ]\n",
      "Training loss [ 0.14335752  1.0100548   7.019744   22.79647   ]\n",
      "Training loss [ 3.227554  8.146251 22.110237 45.200275]\n",
      "Training loss [ 0.5152087  1.033941   8.419023  24.924133 ]\n",
      "Training loss [ 0.3010354  1.0439836  6.9650507 24.040382 ]\n",
      "Training loss [ 0.22546431  1.0409596   6.240125   24.660326  ]\n",
      "Training loss [ 0.19995475  1.0111635   6.3792486  24.703896  ]\n",
      "Training loss [ 0.1837772  1.0007548  6.4680223 24.005873 ]\n",
      "Training loss [ 0.15148197  1.0339035   6.4264617  24.924122  ]\n",
      "Training loss [ 0.14101286  1.0439836   6.442936   24.040382  ]\n",
      "Training loss [ 0.14022818  1.0409596   6.1917696  24.660326  ]\n",
      "Training loss [ 0.1283772  1.0111635  6.3542614 24.703896 ]\n",
      "Training loss [ 0.13702783  1.0007548   6.466321   24.005873  ]\n",
      "Training loss [ 0.12303014  1.0339035   6.419114   24.924122  ]\n",
      "Training loss [ 0.11959393  1.0439836   6.4417114  24.040382  ]\n",
      "Training loss [ 0.12350982  1.0409596   6.193483   24.660326  ]\n",
      "Training loss [ 0.11574365  1.0111635   6.3534784  24.703896  ]\n",
      "Training loss [ 0.12718967  1.0007548   6.466737   24.005873  ]\n",
      "Training loss [ 0.11579848  1.0339035   6.417246   24.924122  ]\n",
      "Training loss [ 0.11167167  1.0439836   6.440195   24.040382  ]\n",
      "Training loss [ 0.11706126  1.0409596   6.193977   24.660326  ]\n",
      "Training loss [ 0.10692985  1.0111635   6.3529015  24.703896  ]\n",
      "Training loss [ 0.12096022  1.0007548   6.467663   24.005873  ]\n",
      "Training loss [ 0.108133   1.0339035  6.417733  24.924122 ]\n",
      "Training loss [ 0.10450911  1.0439836   6.439296   24.040382  ]\n",
      "Training loss [ 0.11283978  1.0409596   6.193075   24.660326  ]\n",
      "Training loss [ 0.10249224  1.0111635   6.351985   24.703896  ]\n",
      "Training loss [ 6.723471 10.616305 24.624714 38.336235]\n",
      "Training loss [ 0.7125598  1.0119665 10.097308  24.515116 ]\n",
      "Training loss [ 0.40267867  1.0080009   9.801247   25.388542  ]\n",
      "Training loss [ 0.2924323  0.9706152  7.4648395 25.459843 ]\n",
      "Training loss [ 0.2564314  0.9829082  7.3097343 24.94197  ]\n",
      "Training loss [ 0.23600835  0.99609137  7.3767962  26.049644  ]\n",
      "Training loss [ 0.21823005  1.0119616   7.3535953  24.514313  ]\n",
      "Training loss [ 0.18685871  1.0080009   7.3543587  25.388542  ]\n",
      "Training loss [ 0.18240964  0.9706152   7.3289504  25.459843  ]\n",
      "Training loss [ 0.17175576  0.9829082   7.2866144  24.94197   ]\n",
      "Training loss [ 0.17979556  0.99609137  7.364921   26.049644  ]\n",
      "Training loss [ 0.17771554  1.0119616   7.351299   24.514313  ]\n",
      "Training loss [ 0.15747818  1.0080009   7.3573704  25.388542  ]\n",
      "Training loss [ 0.14715154  0.9706152   7.3281245  25.459843  ]\n",
      "Training loss [ 0.14527424  0.9829082   7.285431   24.94197   ]\n",
      "Training loss [ 0.15751204  0.99609137  7.3633513  26.049644  ]\n",
      "Training loss [ 0.15453616  1.0119616   7.351946   24.514313  ]\n",
      "Training loss [ 0.1378866  1.0080009  7.356716  25.388542 ]\n",
      "Training loss [ 0.11420096  0.9706152   7.326494   25.459843  ]\n",
      "Training loss [ 0.1155761  0.9829082  7.2843    24.94197  ]\n",
      "Training loss [ 0.1272736   0.99609137  7.3640337  26.049644  ]\n",
      "Training loss [ 0.12519695  1.0119616   7.3530035  24.514313  ]\n",
      "Training loss [ 0.1093284  1.0080009  7.356885  25.388542 ]\n",
      "Training loss [ 0.10278462  0.9706152   7.3267937  25.459843  ]\n",
      "Training loss [ 0.10954145  0.9829082   7.283724   24.94197   ]\n",
      "Training loss [ 4.538213 15.687983 25.243536 52.443623]\n",
      "Training loss [ 0.509083   0.9348181 11.107933  26.712578 ]\n",
      "Training loss [ 0.26452494  0.9365405  10.112406   26.440578  ]\n",
      "Training loss [ 0.17191714  0.953853    9.884256   26.748524  ]\n",
      "Training loss [ 0.13526034  0.9260863  10.065243   26.105024  ]\n",
      "Training loss [ 0.12394419  0.93915683 10.132565   26.682018  ]\n",
      "Training loss [ 0.11407855  0.9343048   8.992932   26.712376  ]\n",
      "Training loss [ 0.10621005  0.9365405   8.194072   26.440578  ]\n",
      "Training loss [ 0.10722162  0.953853    8.188058   26.748524  ]\n",
      "Training loss [ 0.09431343  0.9260863   8.359328   26.105024  ]\n",
      "Training loss [ 0.09738822  0.93915683  8.417496   26.682018  ]\n",
      "Training loss [ 0.09411766  0.9343048   8.5529785  26.712376  ]\n",
      "Training loss [ 0.09286632  0.9365405   8.18283    26.440578  ]\n",
      "Training loss [ 0.09940673  0.953853    8.182082   26.748524  ]\n",
      "Training loss [ 0.08153486  0.9260863   8.357291   26.105024  ]\n",
      "Training loss [ 0.08474042  0.93915683  8.413179   26.682018  ]\n",
      "Training loss [ 0.07350474  0.9343048   8.551243   26.712376  ]\n",
      "Training loss [ 0.06556264  0.9365405   8.181271   26.440578  ]\n",
      "Training loss [ 0.0705279  0.953853   8.179083  26.748524 ]\n",
      "Training loss [ 0.05437688  0.9260863   8.356398   26.105024  ]\n",
      "Training loss [ 0.06588882  0.93915683  8.411172   26.682018  ]\n",
      "Training loss [ 0.07011991  0.9343048   8.550459   26.712376  ]\n",
      "Training loss [ 0.06119254  0.9365405   8.18131    26.440578  ]\n",
      "Training loss [ 0.06643252  0.953853    8.17938    26.748524  ]\n",
      "Training loss [ 0.05243791  0.9260863   8.355125   26.105024  ]\n",
      "Training loss [ 5.7966666 15.903663  29.429989  62.216244 ]\n",
      "Training loss [ 0.49682683  0.95293397 11.908261   28.771503  ]\n",
      "Training loss [ 0.21879755  0.9706373  11.532276   29.443525  ]\n",
      "Training loss [ 0.15656559  0.9498046   9.218708   29.150986  ]\n",
      "Training loss [ 0.11386378  0.97681284  8.897982   28.906355  ]\n",
      "Training loss [ 0.09569836  0.9437127   9.443972   31.882412  ]\n",
      "Training loss [ 0.08850426  0.95289624  8.977563   28.771057  ]\n",
      "Training loss [ 0.08222011  0.9706373   8.975622   29.443525  ]\n",
      "Training loss [ 0.09825063  0.9498046   8.840458   29.150986  ]\n",
      "Training loss [ 0.08149976  0.97681284  8.703243   28.906355  ]\n",
      "Training loss [ 0.07126401  0.9437127   9.225398   31.882412  ]\n",
      "Training loss [ 0.06817456  0.95289624  8.960211   28.771057  ]\n",
      "Training loss [ 0.0660889  0.9706373  8.973904  29.443525 ]\n",
      "Training loss [ 0.08745413  0.9498046   8.838601   29.150986  ]\n",
      "Training loss [ 0.0753269   0.97681284  8.65913    28.906355  ]\n",
      "Training loss [ 0.0658769  0.9437127  8.500687  31.882412 ]\n",
      "Training loss [ 0.06322983  0.95289624  8.206064   28.771057  ]\n",
      "Training loss [ 0.05867598  0.9706373   8.256417   29.443525  ]\n",
      "Training loss [ 0.07942025  0.9498046   8.157987   29.150986  ]\n",
      "Training loss [ 0.07271965  0.97681284  8.085487   28.906355  ]\n",
      "Training loss [ 0.06318345  0.9437127   8.488413   31.882412  ]\n",
      "Training loss [ 0.0600426   0.95289624  8.202228   28.771057  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.05527406  0.9706373   8.252715   29.443525  ]\n",
      "Training loss [ 0.07626718  0.9498046   8.157341   29.150986  ]\n",
      "Training loss [ 0.06923074  0.97681284  7.906516   28.906355  ]\n",
      "Training loss [ 3.794005 12.171531 23.25336  42.678932]\n",
      "Training loss [ 0.43848082  1.0209417  10.915907   26.395994  ]\n",
      "Training loss [ 0.21642019  1.03385    10.273975   25.267176  ]\n",
      "Training loss [ 0.1241692  1.0487205  9.984449  25.501915 ]\n",
      "Training loss [ 0.0968966  1.0450748 10.517955  26.948849 ]\n",
      "Training loss [ 0.11831729  1.0385635   9.89137    25.086857  ]\n",
      "Training loss [ 0.09063186  1.0209389  10.394768   26.395689  ]\n",
      "Training loss [ 0.092955  1.03385  10.249171 25.267176]\n",
      "Training loss [ 0.06956859  1.0487205   9.596354   25.501915  ]\n",
      "Training loss [ 0.06211573  1.0450748   9.231445   26.948849  ]\n",
      "Training loss [ 0.09812373  1.0385635   8.821431   25.086857  ]\n",
      "Training loss [ 0.07380789  1.0209389   9.240156   26.395689  ]\n",
      "Training loss [ 0.08216091  1.03385     9.177353   25.267176  ]\n",
      "Training loss [ 0.06297132  1.0487205   8.697757   25.501915  ]\n",
      "Training loss [ 0.05395073  1.0450748   9.114167   26.948849  ]\n",
      "Training loss [ 0.09034882  1.0385635   8.710758   25.086857  ]\n",
      "Training loss [ 0.06722116  1.0209389   9.129612   26.395689  ]\n",
      "Training loss [ 0.07906594  1.03385     9.064737   25.267176  ]\n",
      "Training loss [ 0.05875536  1.0487205   8.691438   25.501915  ]\n",
      "Training loss [ 0.04847861  1.0450748   9.114054   26.948849  ]\n",
      "Training loss [ 0.0844401  1.0385635  8.70861   25.086857 ]\n",
      "Training loss [ 0.0643214  1.0209389  9.127827  26.395689 ]\n",
      "Training loss [ 0.07523207  1.03385     9.06463    25.267176  ]\n",
      "Training loss [ 0.05629747  1.0487205   8.689273   25.501915  ]\n",
      "Training loss [ 0.04665948  1.0450748   9.026427   26.948849  ]\n",
      "Training loss [ 7.6493177 12.507357  23.877115  42.939438 ]\n",
      "Training loss [ 0.5429103  1.0521587  9.338052  25.168407 ]\n",
      "Training loss [ 0.22275366  1.0231824   9.261399   26.468658  ]\n",
      "Training loss [ 0.13200754  1.039969    9.043632   26.000345  ]\n",
      "Training loss [ 0.10765688  1.0321097   8.814365   25.473955  ]\n",
      "Training loss [ 0.08834866  1.0399486   8.620913   25.671856  ]\n",
      "Training loss [ 0.06965433  1.0521472   8.409807   25.168388  ]\n",
      "Training loss [ 0.0732003  1.0231824  8.427212  26.468658 ]\n",
      "Training loss [ 0.07501297  1.039969    8.758326   26.000345  ]\n",
      "Training loss [ 0.07227162  1.0321097   8.560949   25.473955  ]\n",
      "Training loss [ 0.06795038  1.0399486   8.373964   25.671856  ]\n",
      "Training loss [ 0.05574661  1.0521472   8.163033   25.168388  ]\n",
      "Training loss [ 0.06039169  1.0231824   8.378896   26.468658  ]\n",
      "Training loss [ 0.0684763  1.039969   8.752116  26.000345 ]\n",
      "Training loss [ 0.06288558  1.0321097   8.557774   25.473955  ]\n",
      "Training loss [ 0.06055732  1.0399486   8.369109   25.671856  ]\n",
      "Training loss [ 0.04950076  1.0521472   8.162697   25.168388  ]\n",
      "Training loss [ 0.05599045  1.0231824   8.375837   26.468658  ]\n",
      "Training loss [ 0.0599652  1.039969   8.749849  26.000345 ]\n",
      "Training loss [ 0.05507132  1.0321097   8.555817   25.473955  ]\n",
      "Training loss [ 0.05567687  1.0399486   8.36467    25.671856  ]\n",
      "Training loss [ 0.04690986  1.0521472   8.16167    25.168388  ]\n",
      "Training loss [ 0.05274634  1.0231824   8.375222   26.468658  ]\n",
      "Training loss [ 0.05685051  1.039969    8.617341   26.000345  ]\n",
      "Training loss [ 0.05044101  1.0321097   8.427207   25.473955  ]\n",
      "Training loss [ 6.110548 13.13777  22.279284 42.826363]\n",
      "Training loss [ 0.40306723  0.99370956  9.058437   24.987293  ]\n",
      "Training loss [ 0.19495833  1.0153633   7.759381   24.851032  ]\n",
      "Training loss [ 0.10581786  1.0204213   7.822119   25.026634  ]\n",
      "Training loss [ 0.09703822  1.0132581   7.629931   24.414799  ]\n",
      "Training loss [ 0.07820261  1.0261276   7.603821   24.48074   ]\n",
      "Training loss [ 0.0781951   0.99369407  7.543745   24.986967  ]\n",
      "Training loss [ 0.0913717  1.0153633  7.551999  24.851032 ]\n",
      "Training loss [ 0.06122982  1.0204213   7.5521092  25.026634  ]\n",
      "Training loss [ 0.06588824  1.0132581   7.473979   24.414799  ]\n",
      "Training loss [ 0.06281565  1.0261276   7.4663496  24.48074   ]\n",
      "Training loss [ 0.06786257  0.99369407  7.360772   24.986967  ]\n",
      "Training loss [ 0.075155   1.0153633  7.354868  24.851032 ]\n",
      "Training loss [ 0.05447432  1.0204213   7.425391   25.026634  ]\n",
      "Training loss [ 0.05812388  1.0132581   7.3503876  24.414799  ]\n",
      "Training loss [ 0.05764776  1.0261276   7.3178453  24.48074   ]\n",
      "Training loss [ 0.06476967  0.99369407  7.359593   24.986967  ]\n",
      "Training loss [ 0.06942798  1.0153633   7.352208   24.851032  ]\n",
      "Training loss [ 0.04989752  1.0204213   7.4216175  25.026634  ]\n",
      "Training loss [ 0.05472796  1.0132581   7.07784    24.414799  ]\n",
      "Training loss [ 0.05521354  1.0261276   7.001626   24.48074   ]\n",
      "Training loss [ 0.0630946   0.99369407  7.093705   24.986967  ]\n",
      "Training loss [ 0.0661214  1.0153633  7.1156673 24.851032 ]\n",
      "Training loss [ 0.04779191  1.0204213   7.171142   25.026634  ]\n",
      "Training loss [ 0.05191519  1.0132581   7.0616736  24.414799  ]\n",
      "Training loss [ 3.4402437 10.0959215 23.471119  43.48197  ]\n",
      "Training loss [ 0.32997432  0.99401873  9.800485   25.017908  ]\n",
      "Training loss [ 0.17211062  0.99046755  9.482295   25.140432  ]\n",
      "Training loss [ 0.08756658  0.98932296  9.049569   25.245583  ]\n",
      "Training loss [ 0.07205829  0.98857796  9.26507    25.532225  ]\n",
      "Training loss [ 0.08924472  0.98088443  9.087674   25.413292  ]\n",
      "Training loss [ 0.06342877  0.99401873  8.920127   25.017908  ]\n",
      "Training loss [ 0.07482699  0.99046755  8.81946    25.140427  ]\n",
      "Training loss [ 0.05271189  0.98932296  8.593883   25.245583  ]\n",
      "Training loss [ 0.05438323  0.98857796  8.835268   25.532225  ]\n",
      "Training loss [ 0.06967295  0.98088443  8.7713585  25.413292  ]\n",
      "Training loss [ 0.05419473  0.99401873  8.544722   25.017908  ]\n",
      "Training loss [ 0.06279721  0.99046755  8.4736595  25.140427  ]\n",
      "Training loss [ 0.04618453  0.98932296  8.404221   25.245583  ]\n",
      "Training loss [ 0.05265187  0.98857796  8.6453     25.532225  ]\n",
      "Training loss [ 0.06319576  0.98088443  8.739971   25.413292  ]\n",
      "Training loss [ 0.04998915  0.99401873  8.542936   25.017908  ]\n",
      "Training loss [ 0.05768648  0.99046755  8.471054   25.140427  ]\n",
      "Training loss [ 0.04486683  0.98932296  8.403059   25.245583  ]\n",
      "Training loss [ 0.04810582  0.98857796  8.643842   25.532225  ]\n",
      "Training loss [ 0.06104048  0.98088443  8.738474   25.413292  ]\n",
      "Training loss [ 0.04853808  0.99401873  8.54339    25.017908  ]\n",
      "Training loss [ 0.0540067   0.99046755  8.469077   25.140427  ]\n",
      "Training loss [ 0.04303819  0.98932296  8.400912   25.245583  ]\n",
      "Training loss [ 0.04539418  0.98857796  8.640999   25.532225  ]\n",
      "Training loss [ 3.4225888 10.41239   22.4907    39.913475 ]\n",
      "Training loss [ 0.2968614  0.9730824  8.251968  24.371332 ]\n",
      "Training loss [ 0.12670048  0.97144073  8.206256   25.34384   ]\n",
      "Training loss [ 0.07668369  0.9582488   8.081888   24.807549  ]\n",
      "Training loss [ 0.06796616  0.9732641   8.23391    25.282928  ]\n",
      "Training loss [ 0.08946908  0.98678136  7.7361135  24.562263  ]\n",
      "Training loss [ 0.05682341  0.97302186  7.6822963  24.371332  ]\n",
      "Training loss [ 0.05615786  0.97144073  7.9645815  25.34384   ]\n",
      "Training loss [ 0.04443851  0.9582488   7.8452396  24.807549  ]\n",
      "Training loss [ 0.05626701  0.9732641   8.005132   25.282928  ]\n",
      "Training loss [ 0.07589909  0.98678136  7.684498   24.562263  ]\n",
      "Training loss [ 0.04319024  0.97302186  7.557628   24.371332  ]\n",
      "Training loss [ 0.04542798  0.97144073  7.9017086  25.34384   ]\n",
      "Training loss [ 0.03805092  0.9582488   7.423434   24.807549  ]\n",
      "Training loss [ 0.05503464  0.9732641   7.5981655  25.282928  ]\n",
      "Training loss [ 0.07004524  0.98678136  7.2850876  24.562263  ]\n",
      "Training loss [ 0.04001458  0.97302186  7.2408886  24.371332  ]\n",
      "Training loss [ 0.03989749  0.97144073  7.584758   25.34384   ]\n",
      "Training loss [ 0.03434088  0.9582488   7.400096   24.807549  ]\n",
      "Training loss [ 0.05189241  0.9732641   7.6219997  25.282928  ]\n",
      "Training loss [ 0.06649779  0.98678136  7.291629   24.562263  ]\n",
      "Training loss [ 0.03844451  0.97302186  7.2222404  24.371332  ]\n",
      "Training loss [ 0.03728133  0.97144073  7.542284   25.34384   ]\n",
      "Training loss [ 0.03263409  0.9582488   7.373473   24.807549  ]\n",
      "Training loss [ 0.04719666  0.9732641   7.5831423  25.282928  ]\n",
      "Training loss [ 4.1987543  8.336488  24.27629   39.696964 ]\n",
      "Training loss [ 0.28807485  1.0687605   9.639708   24.162716  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.13231692  1.0700382   9.794067   25.186262  ]\n",
      "Training loss [ 0.07710015  1.0573864   9.361965   25.093199  ]\n",
      "Training loss [ 0.05453249  1.0516195   9.209931   24.342505  ]\n",
      "Training loss [ 0.06181415  1.0632801   9.802337   26.028889  ]\n",
      "Training loss [ 0.04773689  1.0687578   8.998979   24.161964  ]\n",
      "Training loss [ 0.05531863  1.0700407   9.492456   25.185354  ]\n",
      "Training loss [ 0.04206118  1.0573864   9.20324    25.093199  ]\n",
      "Training loss [ 0.03630457  1.0516195   8.980242   24.342505  ]\n",
      "Training loss [ 0.04621213  1.0632801   9.571829   26.028889  ]\n",
      "Training loss [ 0.03734561  1.0687578   8.855545   24.161964  ]\n",
      "Training loss [ 0.04535417  1.0700382   9.297845   25.185354  ]\n",
      "Training loss [ 0.0344223  1.0573864  8.7743435 25.093199 ]\n",
      "Training loss [ 0.03203388  1.0516195   8.655262   24.342505  ]\n",
      "Training loss [ 0.04000842  1.0632801   9.208225   26.028889  ]\n",
      "Training loss [ 0.03391556  1.0687578   8.508783   24.161964  ]\n",
      "Training loss [ 0.03741323  1.0700382   9.065481   25.185354  ]\n",
      "Training loss [ 0.03151175  1.0573864   8.674225   25.093199  ]\n",
      "Training loss [ 0.0294512  1.0516195  8.111333  24.342505 ]\n",
      "Training loss [ 0.03678722  1.0632801   8.647169   26.028889  ]\n",
      "Training loss [ 0.03160123  1.0687578   7.911834   24.161964  ]\n",
      "Training loss [ 0.03583238  1.0700382   8.455883   25.185354  ]\n",
      "Training loss [ 0.02915422  1.0573864   8.1449     25.093199  ]\n",
      "Training loss [ 0.03036929  1.0516195   7.9955716  24.342505  ]\n",
      "Training loss [ 3.470523   6.3904305 24.25196   39.639687 ]\n",
      "Training loss [ 0.27078235  0.9916669   9.965403   25.653101  ]\n",
      "Training loss [ 0.10408031  0.98187375  8.56358    25.043726  ]\n",
      "Training loss [ 0.08664526  0.9861569   8.782601   25.998188  ]\n",
      "Training loss [ 0.06786751  0.9946312   8.757924   26.09821   ]\n",
      "Training loss [ 0.06216206  0.99979484  8.64629    25.904934  ]\n",
      "Training loss [ 0.06499844  0.9916669   8.456541   25.653101  ]\n",
      "Training loss [ 0.0460965   0.98187375  8.156969   25.043726  ]\n",
      "Training loss [ 0.05725677  0.9861569   8.49361    25.998188  ]\n",
      "Training loss [ 0.05055503  0.9946312   8.503903   26.09821   ]\n",
      "Training loss [ 0.04246729  0.99979484  8.443181   25.904934  ]\n",
      "Training loss [ 0.05126835  0.9916669   8.34623    25.653101  ]\n",
      "Training loss [ 0.03586965  0.98187375  8.023078   25.043726  ]\n",
      "Training loss [ 0.04988503  0.9861569   8.255894   25.998188  ]\n",
      "Training loss [ 0.04082147  0.9946312   8.251642   26.09821   ]\n",
      "Training loss [ 0.03577176  0.99979484  8.284929   25.904934  ]\n",
      "Training loss [ 0.05248132  0.9916669   8.097242   25.653101  ]\n",
      "Training loss [ 0.03193387  0.98187375  7.8718677  25.043726  ]\n",
      "Training loss [ 0.04699628  0.9861569   8.197301   25.998188  ]\n",
      "Training loss [ 0.03584664  0.9946312   8.185396   26.09821   ]\n",
      "Training loss [ 0.03126564  0.99979484  8.2126665  25.904934  ]\n",
      "Training loss [ 0.04644767  0.9916669   8.062157   25.653101  ]\n",
      "Training loss [ 0.02954444  0.98187375  7.679893   25.043726  ]\n",
      "Training loss [ 0.04465425  0.9861569   7.93659    25.998188  ]\n",
      "Training loss [ 0.0343595  0.9946312  7.9462194 26.09821  ]\n",
      "Training loss [ 4.7349434  6.228844  24.198109  41.94703  ]\n",
      "Training loss [ 0.33646885  1.0393919  10.141391   26.08177   ]\n",
      "Training loss [ 0.12895566  1.0381851   9.276775   26.204933  ]\n",
      "Training loss [ 0.09935295  1.0473822   9.290658   26.801971  ]\n",
      "Training loss [ 0.05976554  1.0428553   8.93593    26.370125  ]\n",
      "Training loss [ 0.08716761  1.0418842   8.946048   25.916704  ]\n",
      "Training loss [ 0.06119394  1.0393198   9.134446   26.08177   ]\n",
      "Training loss [ 0.066875   1.0381851  9.157789  26.204933 ]\n",
      "Training loss [ 0.06201653  1.0473822   9.040203   26.801971  ]\n",
      "Training loss [ 0.04020088  1.0428553   8.723947   26.370125  ]\n",
      "Training loss [ 0.0581098  1.0418842  8.773411  25.916704 ]\n",
      "Training loss [ 0.05421648  1.0393198   8.960045   26.08177   ]\n",
      "Training loss [ 0.04048751  1.0381851   8.765739   26.204933  ]\n",
      "Training loss [ 0.04849689  1.0473822   8.867687   26.801971  ]\n",
      "Training loss [ 0.03200395  1.0428553   8.5818815  26.370125  ]\n",
      "Training loss [ 0.04951603  1.0418842   8.595471   25.916704  ]\n",
      "Training loss [ 0.04648884  1.0393198   8.776163   26.08177   ]\n",
      "Training loss [ 0.03590855  1.0381851   8.525844   26.204933  ]\n",
      "Training loss [ 0.04328424  1.0473822   8.789743   26.801971  ]\n",
      "Training loss [ 0.02940843  1.0428553   8.139012   26.370125  ]\n",
      "Training loss [ 0.05275355  1.0418842   8.220779   25.916704  ]\n",
      "Training loss [ 0.03996768  1.0393198   8.411528   26.08177   ]\n",
      "Training loss [ 0.03343299  1.0381851   8.144743   26.204933  ]\n",
      "Training loss [ 0.04097946  1.0473822   8.371979   26.801971  ]\n",
      "Training loss [ 0.02781948  1.0428553   8.022714   26.370125  ]\n",
      "Training loss [ 2.5474763  6.734639  24.517115  41.98017  ]\n",
      "Training loss [ 0.23625046  0.98501694 10.192651   25.761913  ]\n",
      "Training loss [ 0.12392201  0.980288    9.961386   26.42505   ]\n",
      "Training loss [ 0.07708073  0.98368216  9.094609   25.166002  ]\n",
      "Training loss [ 0.05814384  0.99699724  9.077511   25.430538  ]\n",
      "Training loss [ 0.03963136  0.98810816  9.021056   25.858618  ]\n",
      "Training loss [ 0.04620891  0.98501694  8.747982   25.761066  ]\n",
      "Training loss [ 0.05012649  0.980288    9.104073   26.42505   ]\n",
      "Training loss [ 0.03679717  0.98368216  8.355403   25.166002  ]\n",
      "Training loss [ 0.03435859  0.99699724  8.549435   25.430538  ]\n",
      "Training loss [ 0.02836571  0.98810816  8.55695    25.858618  ]\n",
      "Training loss [ 0.04224521  0.98501694  8.404498   25.761066  ]\n",
      "Training loss [ 0.03776953  0.980288    8.899013   26.42505   ]\n",
      "Training loss [ 0.03358475  0.98368216  8.220992   25.166002  ]\n",
      "Training loss [ 0.0302863   0.99699724  8.408227   25.430538  ]\n",
      "Training loss [2.1227755e-02 9.8810816e-01 8.3777018e+00 2.5858618e+01]\n",
      "Training loss [ 0.03603729  0.98501694  8.333153   25.761066  ]\n",
      "Training loss [ 0.03511016  0.980288    8.7512     26.42505   ]\n",
      "Training loss [ 0.02750633  0.98368216  8.079317   25.166002  ]\n",
      "Training loss [ 0.02885445  0.99699724  8.285124   25.430538  ]\n",
      "Training loss [2.2100627e-02 9.8810816e-01 8.2593117e+00 2.5858618e+01]\n",
      "Training loss [ 0.03358756  0.98501694  8.225994   25.761066  ]\n",
      "Training loss [ 0.03299273  0.980288    8.635666   26.42505   ]\n",
      "Training loss [ 0.02830892  0.98368216  7.963418   25.166002  ]\n",
      "Training loss [ 0.0262403   0.99699724  8.236389   25.430538  ]\n",
      "Training loss [ 3.1504903  6.7447    25.547888  38.596657 ]\n",
      "Training loss [ 0.26866016  1.0123473  10.710972   26.535116  ]\n",
      "Training loss [ 0.15478157  1.0155228   9.910164   26.667385  ]\n",
      "Training loss [ 0.09808458  1.023949    9.30385    26.263628  ]\n",
      "Training loss [ 0.06860479  1.0146683   9.455866   26.74095   ]\n",
      "Training loss [ 0.08069447  1.0205449   9.63382    26.941391  ]\n",
      "Training loss [ 0.05781504  1.0123473   9.249285   26.535116  ]\n",
      "Training loss [ 0.06073389  1.0155228   9.186145   26.667385  ]\n",
      "Training loss [ 0.05218735  1.023949    8.820948   26.263628  ]\n",
      "Training loss [ 0.04344473  1.0146683   9.092154   26.74095   ]\n",
      "Training loss [ 0.05638266  1.0205449   9.137139   26.941391  ]\n",
      "Training loss [ 0.04247677  1.0123473   8.910141   26.535116  ]\n",
      "Training loss [ 0.04222183  1.0155228   8.93596    26.667385  ]\n",
      "Training loss [ 0.04477002  1.023949    8.621887   26.263628  ]\n",
      "Training loss [ 0.03888774  1.0146683   8.908671   26.74095   ]\n",
      "Training loss [ 0.0450446  1.0205449  8.971832  26.941391 ]\n",
      "Training loss [ 0.04187723  1.0123473   8.651607   26.535116  ]\n",
      "Training loss [ 0.04626129  1.0155228   8.831334   26.667385  ]\n",
      "Training loss [ 0.04313466  1.023949    8.575113   26.263628  ]\n",
      "Training loss [ 0.03356166  1.0146683   8.770632   26.74095   ]\n",
      "Training loss [ 0.04004011  1.0205449   8.795065   26.941391  ]\n",
      "Training loss [ 0.03983804  1.0123473   8.564318   26.535116  ]\n",
      "Training loss [ 0.03724049  1.0155228   8.7000265  26.667385  ]\n",
      "Training loss [ 0.03374365  1.023949    8.392206   26.263628  ]\n",
      "Training loss [ 0.03625598  1.0146683   8.561741   26.74095   ]\n",
      "Training loss [ 1.7365645  5.2946243 25.083908  35.821552 ]\n",
      "Training loss [ 0.3506585   0.99343026 10.55831    25.911192  ]\n",
      "Training loss [ 0.23086375  0.9949454   9.673826   25.51228   ]\n",
      "Training loss [ 0.17912728  0.99825406  9.556687   25.37909   ]\n",
      "Training loss [ 0.14858    0.9968026  9.11608   26.140656 ]\n",
      "Training loss [ 0.14071421  1.0027772   9.049912   26.269276  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.1376847   0.99343026  8.882114   25.911192  ]\n",
      "Training loss [ 0.12908193  0.9949454   8.835915   25.51228   ]\n",
      "Training loss [ 0.10393403  0.99825406  8.67281    25.37909   ]\n",
      "Training loss [ 0.09829003  0.9968026   8.441777   26.140656  ]\n",
      "Training loss [ 0.10348559  1.0027772   8.453558   26.269276  ]\n",
      "Training loss [ 0.10747038  0.99343026  8.384943   25.911192  ]\n",
      "Training loss [ 0.09105039  0.9949454   8.382771   25.51228   ]\n",
      "Training loss [ 0.08795939  0.99825406  8.338254   25.37909   ]\n",
      "Training loss [ 0.09812595  0.9968026   8.148592   26.140656  ]\n",
      "Training loss [ 0.1076109  1.0027772  8.207417  26.269276 ]\n",
      "Training loss [ 0.08797238  0.99343026  8.181356   25.911192  ]\n",
      "Training loss [ 0.08658683  0.9949454   8.1559305  25.51228   ]\n",
      "Training loss [ 0.07898577  0.99825406  8.151133   25.37909   ]\n",
      "Training loss [ 0.08337615  0.9968026   8.008963   26.140656  ]\n",
      "Training loss [ 0.08853808  1.0027772   8.068359   26.269276  ]\n",
      "Training loss [ 0.0828729   0.99343026  8.044933   25.911192  ]\n",
      "Training loss [ 0.07956214  0.9949454   8.071189   25.51228   ]\n",
      "Training loss [ 0.08928135  0.99825406  8.082909   25.37909   ]\n",
      "Training loss [ 0.07967883  0.9968026   7.9455304  26.140656  ]\n",
      "Training loss [ 0.8995131  8.198547  33.35887   55.470444 ]\n",
      "Training loss [ 0.27980387  0.84539896 10.1664295  32.254883  ]\n",
      "Training loss [ 0.28442636  0.8225348  10.110255   32.75155   ]\n",
      "Training loss [ 0.23515518  0.82763624  9.386427   33.349876  ]\n",
      "Training loss [ 0.22311391  0.8675071   9.455431   33.307884  ]\n",
      "Training loss [ 0.24681295  0.85740143  9.277546   34.332924  ]\n",
      "Training loss [ 0.21796465  0.84539896  8.358535   32.254883  ]\n",
      "Training loss [ 0.24561685  0.8225348   9.478828   32.75155   ]\n",
      "Training loss [ 0.21274866  0.82763624  8.853134   33.349876  ]\n",
      "Training loss [ 0.20377919  0.8675071   9.165916   33.307884  ]\n",
      "Training loss [ 0.23277074  0.85740143  8.989749   34.332924  ]\n",
      "Training loss [ 0.20524636  0.84539896  8.265844   32.254883  ]\n",
      "Training loss [ 0.23415259  0.8225348   9.3519535  32.75155   ]\n",
      "Training loss [ 0.20675632  0.82763624  8.799347   33.349876  ]\n",
      "Training loss [ 0.19993807  0.8675071   9.077611   33.307884  ]\n",
      "Training loss [ 0.21558577  0.85740143  8.940371   34.332924  ]\n",
      "Training loss [ 0.19998775  0.84539896  8.263252   32.254883  ]\n",
      "Training loss [ 0.2334881  0.8225348  9.312157  32.75155  ]\n",
      "Training loss [ 0.20119661  0.82763624  8.79467    33.349876  ]\n",
      "Training loss [ 0.19471504  0.8675071   9.0515585  33.307884  ]\n",
      "Training loss [ 0.20971557  0.85740143  8.93242    34.332924  ]\n",
      "Training loss [ 0.19619448  0.84539896  8.265853   32.254883  ]\n",
      "Training loss [ 0.22617951  0.8225348   9.301184   32.75155   ]\n",
      "Training loss [ 0.20129275  0.82763624  8.79461    33.349876  ]\n",
      "Training loss [ 0.19379944  0.8675071   9.045194   33.307884  ]\n",
      "Training loss [ 1.8566327  2.9571471 22.017727  30.17126  ]\n",
      "Training loss [ 0.40560853  1.0781938   9.250811   24.169785  ]\n",
      "Training loss [ 0.3797084  1.128397   8.355017  23.677153 ]\n",
      "Training loss [ 0.36728576  1.2025888   7.474413   22.635004  ]\n",
      "Training loss [ 0.30960235  1.1141288   7.7484107  23.181732  ]\n",
      "Training loss [ 0.33690435  1.143388    7.500312   22.573055  ]\n",
      "Training loss [ 0.33212104  1.077902    8.496567   24.165922  ]\n",
      "Training loss [ 0.32129532  1.128397    8.050679   23.677153  ]\n",
      "Training loss [ 0.317967   1.2025888  7.3757033 22.635004 ]\n",
      "Training loss [ 0.27726704  1.1141288   7.6833677  23.181732  ]\n",
      "Training loss [ 0.3100053  1.143388   7.461093  22.573055 ]\n",
      "Training loss [ 0.31599075  1.077902    8.469143   24.165922  ]\n",
      "Training loss [ 0.3012758  1.128397   8.04713   23.677153 ]\n",
      "Training loss [ 0.2929645  1.2025888  7.36518   22.635004 ]\n",
      "Training loss [ 0.26283756  1.1141288   7.6782646  23.181732  ]\n",
      "Training loss [ 0.29740283  1.143388    7.4578066  22.573055  ]\n",
      "Training loss [ 0.31010738  1.077902    8.467208   24.165922  ]\n",
      "Training loss [ 0.29359138  1.128397    8.04769    23.677153  ]\n",
      "Training loss [ 0.28123552  1.2025888   7.3637443  22.635004  ]\n",
      "Training loss [ 0.25719625  1.1141288   7.6777477  23.181732  ]\n",
      "Training loss [ 0.29270637  1.143388    7.4573283  22.573055  ]\n",
      "Training loss [ 0.30703413  1.077902    8.467168   24.165922  ]\n",
      "Training loss [ 0.28883988  1.128397    8.047291   23.677153  ]\n",
      "Training loss [ 0.27412403  1.2025888   7.3628244  22.635004  ]\n",
      "Training loss [ 0.25429642  1.1141288   7.6780105  23.181732  ]\n",
      "Training loss [ 2.8924785  5.9707465 28.167315  33.686653 ]\n",
      "Training loss [ 0.32010296  0.8815568  10.449891   26.567944  ]\n",
      "Training loss [ 0.28697532  0.9566047  10.338787   29.290337  ]\n",
      "Training loss [ 0.22844408  0.8694285   9.354201   25.602844  ]\n",
      "Training loss [ 0.23462433  0.92415506  9.913925   28.495739  ]\n",
      "Training loss [ 0.2408182  0.9211086 10.126598  28.893093 ]\n",
      "Training loss [ 0.20743504  0.8815568   9.260036   26.567944  ]\n",
      "Training loss [ 0.21847983  0.9566047   9.83252    29.290337  ]\n",
      "Training loss [ 0.19434382  0.8694285   9.04044    25.602844  ]\n",
      "Training loss [ 0.20675188  0.92415506  9.686064   28.495739  ]\n",
      "Training loss [ 0.22122815  0.9211086   9.995449   28.893093  ]\n",
      "Training loss [ 0.18997738  0.8815568   9.139363   26.567944  ]\n",
      "Training loss [ 0.2019919  0.9566047  9.792726  29.290337 ]\n",
      "Training loss [ 0.18666488  0.8694285   9.017067   25.602844  ]\n",
      "Training loss [ 0.19828297  0.92415506  9.648672   28.495739  ]\n",
      "Training loss [ 0.206825   0.9211086  9.97506   28.893093 ]\n",
      "Training loss [ 0.17596312  0.8815568   9.12144    26.567944  ]\n",
      "Training loss [ 0.19969398  0.9566047   9.7859955  29.290337  ]\n",
      "Training loss [ 0.18193129  0.8694285   9.017034   25.602844  ]\n",
      "Training loss [ 0.19165167  0.92415506  9.643953   28.495739  ]\n",
      "Training loss [ 0.19531497  0.9211086   9.971215   28.893093  ]\n",
      "Training loss [ 0.16932707  0.8815568   9.118912   26.567944  ]\n",
      "Training loss [ 0.1938823  0.9566047  9.783035  29.290337 ]\n",
      "Training loss [ 0.17677252  0.8694285   9.01738    25.602844  ]\n",
      "Training loss [ 0.1933567   0.92415506  9.643202   28.495739  ]\n",
      "Training loss [ 2.84558   5.837939 16.728373 27.077505]\n",
      "Training loss [ 0.59264565  1.1467792   8.695012   19.21177   ]\n",
      "Training loss [ 0.51098835  1.1679671   8.723368   20.136032  ]\n",
      "Training loss [ 0.4641498  1.1096456  8.745171  20.06985  ]\n",
      "Training loss [ 0.44894254  1.1219814   8.316663   18.498611  ]\n",
      "Training loss [ 0.45643297  1.1757672   8.436474   18.608604  ]\n",
      "Training loss [ 0.43720704  1.1467792   8.292564   19.211159  ]\n",
      "Training loss [ 0.39712772  1.1679671   8.674793   20.136032  ]\n",
      "Training loss [ 0.38480264  1.1096456   8.676241   20.06985   ]\n",
      "Training loss [ 0.3427119  1.1219814  8.286506  18.498611 ]\n",
      "Training loss [ 0.35747817  1.1757672   8.444992   18.608604  ]\n",
      "Training loss [ 0.35342932  1.1467792   8.294244   19.211159  ]\n",
      "Training loss [ 0.37096626  1.1679671   8.677607   20.136032  ]\n",
      "Training loss [ 0.3663234  1.1096456  8.671074  20.06985  ]\n",
      "Training loss [ 0.32069418  1.1219814   8.281407   18.498611  ]\n",
      "Training loss [ 0.34100303  1.1757672   8.448219   18.608604  ]\n",
      "Training loss [ 0.3377055  1.1467792  8.294169  19.211159 ]\n",
      "Training loss [ 0.3580196  1.1679671  8.679245  20.136032 ]\n",
      "Training loss [ 0.35570294  1.1096456   8.670441   20.06985   ]\n",
      "Training loss [ 0.31344524  1.1219814   8.280919   18.498611  ]\n",
      "Training loss [ 0.33190244  1.1757672   8.44853    18.608604  ]\n",
      "Training loss [ 0.33381468  1.1467792   8.294349   19.211159  ]\n",
      "Training loss [ 0.35532123  1.1679671   8.6792345  20.136032  ]\n",
      "Training loss [ 0.34688476  1.1096456   8.6699295  20.06985   ]\n",
      "Training loss [ 0.30693412  1.1219814   8.281437   18.498611  ]\n",
      "Training loss [ 4.5184736 12.191505  25.678728  37.75614  ]\n",
      "Training loss [ 0.49511415  1.0279624   9.8131075  28.133945  ]\n",
      "Training loss [ 0.32695156  1.0058775   9.139721   28.023554  ]\n",
      "Training loss [ 0.29154274  1.0056779   8.889021   27.558075  ]\n",
      "Training loss [ 0.26918224  0.9865304  10.360357   30.500414  ]\n",
      "Training loss [ 0.2260381  1.0193028  8.46159   27.583454 ]\n",
      "Training loss [ 0.22243202  1.0279624   8.678726   28.133945  ]\n",
      "Training loss [ 0.22234537  1.0058775   8.849612   28.023554  ]\n",
      "Training loss [ 0.2140436  1.0056779  8.811974  27.558075 ]\n",
      "Training loss [ 0.21289763  0.9865304  10.315931   30.500414  ]\n",
      "Training loss [ 0.17479777  1.0193028   8.422751   27.583454  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.18218835  1.0279624   8.64234    28.133945  ]\n",
      "Training loss [ 0.18327205  1.0058775   8.841476   28.023554  ]\n",
      "Training loss [ 0.18238275  1.0056779   8.814015   27.558075  ]\n",
      "Training loss [ 0.19373393  0.9865304  10.315077   30.500414  ]\n",
      "Training loss [ 0.15747926  1.0193028   8.419406   27.583454  ]\n",
      "Training loss [ 0.1627053  1.0279624  8.637291  28.133945 ]\n",
      "Training loss [ 0.16704306  1.0058775   8.8403225  28.023554  ]\n",
      "Training loss [ 0.17074347  1.0056779   8.814128   27.558075  ]\n",
      "Training loss [ 0.17804599  0.9865304  10.314348   30.500414  ]\n",
      "Training loss [ 0.15380311  1.0193028   8.419261   27.583454  ]\n",
      "Training loss [ 0.15951282  1.0279624   8.637498   28.133945  ]\n",
      "Training loss [ 0.16351156  1.0058775   8.839137   28.023554  ]\n",
      "Training loss [ 0.16142677  1.0056779   8.814091   27.558075  ]\n",
      "Training loss [ 0.17247358  0.9865304  10.313263   30.500414  ]\n",
      "Training loss [ 5.6878357  8.55217   21.044249  50.33642  ]\n",
      "Training loss [ 0.60477555  1.1183937   9.451297   24.082697  ]\n",
      "Training loss [ 0.41857994  1.1153219   7.9601974  21.877537  ]\n",
      "Training loss [ 0.31321296  1.1462468   8.103615   23.61673   ]\n",
      "Training loss [ 0.28676367  1.1077222   8.149616   23.392845  ]\n",
      "Training loss [ 0.2836376  1.1221354  7.9874935 22.991354 ]\n",
      "Training loss [ 0.23776735  1.1182735   8.411843   24.082697  ]\n",
      "Training loss [ 0.2421109  1.1153219  7.8055954 21.877537 ]\n",
      "Training loss [ 0.19788513  1.1462468   7.770528   23.61673   ]\n",
      "Training loss [ 0.19398287  1.1077222   7.777978   23.392845  ]\n",
      "Training loss [ 0.20953715  1.1221354   7.6739383  22.991354  ]\n",
      "Training loss [ 0.18162233  1.1182735   7.99203    24.082697  ]\n",
      "Training loss [ 0.18716285  1.1153219   7.5489883  21.877537  ]\n",
      "Training loss [ 0.17484877  1.1462468   7.7671595  23.61673   ]\n",
      "Training loss [ 0.17240667  1.1077222   7.7775373  23.392845  ]\n",
      "Training loss [ 0.18589276  1.1221354   7.6730485  22.991354  ]\n",
      "Training loss [ 0.16800347  1.1182735   7.9911413  24.082697  ]\n",
      "Training loss [ 0.16967386  1.1153219   7.5478697  21.877537  ]\n",
      "Training loss [ 0.17203885  1.1462468   7.766817   23.61673   ]\n",
      "Training loss [ 0.16968349  1.1077222   7.777133   23.392845  ]\n",
      "Training loss [ 0.18277162  1.1221354   7.67266    22.991354  ]\n",
      "Training loss [ 0.16172263  1.1182735   7.9912896  24.082697  ]\n",
      "Training loss [ 0.15992434  1.1153219   7.5464106  21.877537  ]\n",
      "Training loss [ 0.16157898  1.1462468   7.7653136  23.61673   ]\n",
      "Training loss [ 0.16315362  1.1077222   7.7758913  23.392845  ]\n",
      "Training loss [ 4.3834424 10.621098  24.386772  53.00801  ]\n",
      "Training loss [ 0.5824146  1.0588608  7.8265963 25.326233 ]\n",
      "Training loss [ 0.34501386  1.0393566   7.50749    26.484142  ]\n",
      "Training loss [ 0.24071887  1.0520109   6.758746   25.490185  ]\n",
      "Training loss [ 0.21253453  1.0281234   7.58987    26.462944  ]\n",
      "Training loss [ 0.1986123  1.0195624  7.593622  26.870945 ]\n",
      "Training loss [ 0.17788745  1.0588608   7.2343707  25.3217    ]\n",
      "Training loss [ 0.17203002  1.0393566   7.479723   26.484142  ]\n",
      "Training loss [ 0.1464223  1.0520109  6.7578754 25.490185 ]\n",
      "Training loss [ 0.15953371  1.0281234   7.596722   26.462944  ]\n",
      "Training loss [ 0.15676442  1.0195624   7.5917683  26.870945  ]\n",
      "Training loss [ 0.13813165  1.0588608   7.2347713  25.3217    ]\n",
      "Training loss [ 0.1499454  1.0393566  7.48055   26.484142 ]\n",
      "Training loss [ 0.12653692  1.0520109   6.758131   25.490185  ]\n",
      "Training loss [ 0.14459392  1.0281234   7.5965233  26.462944  ]\n",
      "Training loss [ 0.13369158  1.0195624   7.591154   26.870945  ]\n",
      "Training loss [ 0.1261478  1.0588608  7.232091  25.3217   ]\n",
      "Training loss [ 0.1316219  1.0393566  7.4805512 26.484142 ]\n",
      "Training loss [ 0.11547995  1.0520109   6.758004   25.490185  ]\n",
      "Training loss [ 0.12493899  1.0281234   7.59589    26.462944  ]\n",
      "Training loss [ 0.12659207  1.0195624   7.5905147  26.870945  ]\n",
      "Training loss [ 0.12252754  1.0588608   7.2313857  25.3217    ]\n",
      "Training loss [ 0.12424058  1.0393566   7.4807897  26.484142  ]\n",
      "Training loss [ 0.1088855  1.0520109  6.75817   25.490185 ]\n",
      "Training loss [ 0.12300708  1.0281234   7.5963182  26.462944  ]\n",
      "Training loss [ 5.8334427 10.797741  25.126692  42.569054 ]\n",
      "Training loss [ 0.52679044  0.8894373  10.697555   25.731894  ]\n",
      "Training loss [ 0.28934997  0.96504617 10.111189   26.252874  ]\n",
      "Training loss [ 0.1769037  0.8883734  8.3356285 25.895184 ]\n",
      "Training loss [ 0.15342978  0.9363479   8.421648   25.802338  ]\n",
      "Training loss [ 0.1398966   0.91388595  8.248774   26.60638   ]\n",
      "Training loss [ 0.10927538  0.88941133  7.8857913  25.731894  ]\n",
      "Training loss [ 0.10947323  0.9645513   8.421928   26.252874  ]\n",
      "Training loss [ 0.11127965  0.8883734   8.209232   25.895184  ]\n",
      "Training loss [ 0.10669248  0.9363479   8.406029   25.802338  ]\n",
      "Training loss [ 0.10957134  0.91381276  8.242385   26.60638   ]\n",
      "Training loss [ 0.09547117  0.88941133  7.8815393  25.731894  ]\n",
      "Training loss [ 0.09502883  0.9645513   8.420914   26.252874  ]\n",
      "Training loss [ 0.10330572  0.8883734   8.208349   25.895184  ]\n",
      "Training loss [ 0.09467555  0.9363479   8.404905   25.802338  ]\n",
      "Training loss [ 0.10305371  0.91381276  8.241051   26.60638   ]\n",
      "Training loss [ 0.09055485  0.88941133  7.369713   25.731894  ]\n",
      "Training loss [ 0.09076609  0.9645513   7.765798   26.252874  ]\n",
      "Training loss [ 0.10089124  0.8883734   7.671271   25.895184  ]\n",
      "Training loss [ 0.08851775  0.9363479   7.8583713  25.802338  ]\n",
      "Training loss [ 0.09954719  0.91381276  7.712097   26.60638   ]\n",
      "Training loss [ 0.08699802  0.88941133  7.3086295  25.731894  ]\n",
      "Training loss [ 0.08637882  0.9645513   7.7622375  26.252874  ]\n",
      "Training loss [ 0.09899601  0.8883734   7.6704874  25.895184  ]\n",
      "Training loss [ 0.08447085  0.9363479   7.8572817  25.802338  ]\n",
      "Training loss [ 2.8083885 13.406322  23.164818  61.763912 ]\n",
      "Training loss [ 0.4729976  1.057399   8.541342  23.980145 ]\n",
      "Training loss [ 0.24914724  1.0462868   8.239956   24.478546  ]\n",
      "Training loss [ 0.187363   1.0620648  8.270443  24.100016 ]\n",
      "Training loss [ 0.12461946  1.0734425   8.013648   23.745045  ]\n",
      "Training loss [ 0.13143282  1.0814718   8.290741   25.076967  ]\n",
      "Training loss [ 0.12378779  1.057399    8.199879   23.980145  ]\n",
      "Training loss [ 0.1108478  1.0462868  8.145054  24.478546 ]\n",
      "Training loss [ 0.12122336  1.0620648   8.267918   24.100016  ]\n",
      "Training loss [ 0.09064548  1.0734425   7.9999228  23.745045  ]\n",
      "Training loss [ 0.100886   1.0814718  8.288322  25.076967 ]\n",
      "Training loss [ 0.10604259  1.057399    8.198565   23.980145  ]\n",
      "Training loss [ 0.0938093  1.0462868  8.1434555 24.478546 ]\n",
      "Training loss [ 0.10637042  1.0620648   8.269348   24.100016  ]\n",
      "Training loss [ 0.07978898  1.0734425   8.000102   23.745045  ]\n",
      "Training loss [ 0.0891933  1.0814718  8.286375  25.076967 ]\n",
      "Training loss [ 0.10168719  1.057399    8.197836   23.980145  ]\n",
      "Training loss [ 0.08488813  1.0462868   8.143603   24.478546  ]\n",
      "Training loss [ 0.09791906  1.0620648   8.2690525  24.100016  ]\n",
      "Training loss [ 0.07247713  1.0734425   7.1412396  23.745045  ]\n",
      "Training loss [ 0.08149876  1.0814718   7.335033   25.076967  ]\n",
      "Training loss [ 0.09671396  1.057399    7.253685   23.980145  ]\n",
      "Training loss [ 0.08079125  1.0462868   7.154212   24.478546  ]\n",
      "Training loss [ 0.09170131  1.0620648   7.425676   24.100016  ]\n",
      "Training loss [ 0.06785177  1.0734425   7.0837107  23.745045  ]\n",
      "Training loss [ 4.454565 11.859645 25.17551  54.00962 ]\n",
      "Training loss [ 0.4367368  0.9489342 10.406367  25.999788 ]\n",
      "Training loss [ 0.22828388  1.0175226   9.930981   26.777477  ]\n",
      "Training loss [ 0.16155097  0.989972    9.453903   27.58663   ]\n",
      "Training loss [ 0.11433561  1.0045313   9.018852   26.076347  ]\n",
      "Training loss [ 0.10456138  0.97253877  9.53148    27.59636   ]\n",
      "Training loss [ 0.07344536  0.9489342   9.176508   25.999788  ]\n",
      "Training loss [ 0.08757753  1.0175226   9.341214   26.777477  ]\n",
      "Training loss [ 0.09593296  0.989972    9.437728   27.58663   ]\n",
      "Training loss [ 0.08670032  1.0045313   9.014042   26.076347  ]\n",
      "Training loss [ 0.0825444   0.97253877  9.530983   27.59636   ]\n",
      "Training loss [ 0.05925049  0.9489342   9.176222   25.999788  ]\n",
      "Training loss [ 0.07687914  1.0175226   9.341539   26.777477  ]\n",
      "Training loss [ 0.08317053  0.989972    9.234031   27.58663   ]\n",
      "Training loss [ 0.08042245  1.0045313   8.836221   26.076347  ]\n",
      "Training loss [ 0.07708868  0.97253877  9.303821   27.59636   ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.05595464  0.9489342   8.95804    25.999788  ]\n",
      "Training loss [ 0.07510813  1.0175226   9.122167   26.777477  ]\n",
      "Training loss [ 0.07857007  0.989972    8.997325   27.58663   ]\n",
      "Training loss [ 0.07471144  1.0045313   8.600295   26.076347  ]\n",
      "Training loss [ 0.073528    0.97253877  8.835212   27.59636   ]\n",
      "Training loss [ 0.05488761  0.9489342   8.517259   25.999788  ]\n",
      "Training loss [ 0.07428255  1.0175226   8.621597   26.777477  ]\n",
      "Training loss [ 0.0769752  0.989972   8.748747  27.58663  ]\n",
      "Training loss [ 0.07182877  1.0045313   8.390217   26.076347  ]\n",
      "Training loss [ 3.2050204 11.842779  24.480553  54.63995  ]\n",
      "Training loss [ 0.39068264  0.93645865 10.16416    27.133793  ]\n",
      "Training loss [ 0.21280761  0.9680723   9.705726   26.893585  ]\n",
      "Training loss [ 0.14525072  0.9606244   9.483652   26.905872  ]\n",
      "Training loss [ 0.10375895  0.96057326  7.95497    27.160181  ]\n",
      "Training loss [ 0.08631606  0.9367605   7.822148   26.346317  ]\n",
      "Training loss [ 0.0874229  0.9362284  7.9142933 27.133682 ]\n",
      "Training loss [ 0.09085128  0.9680723   8.078206   26.893837  ]\n",
      "Training loss [ 0.08674909  0.96056646  8.184564   26.905872  ]\n",
      "Training loss [ 0.07193804  0.96057326  7.895284   27.160181  ]\n",
      "Training loss [ 0.06508077  0.9367605   7.746091   26.346317  ]\n",
      "Training loss [ 0.07143752  0.9362284   7.701724   27.133682  ]\n",
      "Training loss [ 0.0802073  0.9680723  7.821265  26.893585 ]\n",
      "Training loss [ 0.07734691  0.96056646  7.883295   26.905872  ]\n",
      "Training loss [ 0.0651893   0.96057326  7.6287985  27.160181  ]\n",
      "Training loss [ 0.06188342  0.9367605   7.578706   26.346317  ]\n",
      "Training loss [ 0.06452679  0.9362284   7.6834497  27.133682  ]\n",
      "Training loss [ 0.07574411  0.9680723   7.825384   26.893585  ]\n",
      "Training loss [ 0.07285333  0.96056646  7.8797655  26.905872  ]\n",
      "Training loss [ 0.06123517  0.96057326  7.6296997  27.160181  ]\n",
      "Training loss [ 0.05972001  0.9367605   7.57699    26.346317  ]\n",
      "Training loss [ 0.06044516  0.9362284   7.6828375  27.133682  ]\n",
      "Training loss [ 0.07357773  0.9680723   7.826295   26.893585  ]\n",
      "Training loss [ 0.07056845  0.96056646  7.8775926  26.905872  ]\n",
      "Training loss [ 0.05873981  0.96057326  7.6313543  27.160181  ]\n",
      "Training loss [ 4.324699 13.600208 24.423704 52.838726]\n",
      "Training loss [ 0.42402166  1.0613787   8.227971   25.133194  ]\n",
      "Training loss [ 0.19004947  1.0555935   8.152657   26.40651   ]\n",
      "Training loss [ 0.12627321  1.0696687   7.655345   25.128729  ]\n",
      "Training loss [ 0.12181491  1.0521896   8.090443   26.298168  ]\n",
      "Training loss [ 0.10289771  1.0755782   7.933197   26.077442  ]\n",
      "Training loss [ 0.08244943  1.061224    7.790357   25.133186  ]\n",
      "Training loss [ 0.07370526  1.0555935   8.035856   26.406477  ]\n",
      "Training loss [ 0.06140111  1.0696687   7.1011977  25.128729  ]\n",
      "Training loss [ 0.08964362  1.0521896   7.561119   26.298168  ]\n",
      "Training loss [ 0.08116832  1.0755782   7.3714385  26.077442  ]\n",
      "Training loss [ 0.06717509  1.061224    7.260641   25.133186  ]\n",
      "Training loss [ 0.06220458  1.0555935   7.5745325  26.406477  ]\n",
      "Training loss [ 0.04956085  1.0696687   7.0737047  25.128729  ]\n",
      "Training loss [ 0.07586294  1.0521896   7.5595665  26.298168  ]\n",
      "Training loss [ 0.06766539  1.0755782   7.3686886  26.077442  ]\n",
      "Training loss [ 0.05916581  1.061224    7.259643   25.133186  ]\n",
      "Training loss [ 0.05795636  1.0555935   7.572726   26.406477  ]\n",
      "Training loss [ 0.04474944  1.0696687   7.074896   25.128729  ]\n",
      "Training loss [ 0.07152481  1.0521896   7.5610743  26.298168  ]\n",
      "Training loss [ 0.06171662  1.0755782   7.3670583  26.077442  ]\n",
      "Training loss [ 0.05723391  1.061224    7.2604723  25.133186  ]\n",
      "Training loss [ 0.0559658  1.0555935  7.5721498 26.406477 ]\n",
      "Training loss [ 0.04189966  1.0696687   7.075157   25.128729  ]\n",
      "Training loss [ 0.06757333  1.0521896   7.561824   26.298168  ]\n",
      "Training loss [ 3.980507 11.750741 22.180658 40.39643 ]\n",
      "Training loss [ 0.38070908  0.9895154   9.052287   25.969917  ]\n",
      "Training loss [ 0.15681413  0.9858264   7.9183655  24.462635  ]\n",
      "Training loss [ 0.10509467  0.96672124  8.107029   24.940964  ]\n",
      "Training loss [ 0.09629086  0.9892598   8.160421   24.99897   ]\n",
      "Training loss [ 0.07858115  0.9754448   7.549224   24.114893  ]\n",
      "Training loss [ 0.07803574  0.98951495  8.398641   25.969023  ]\n",
      "Training loss [ 0.06822056  0.9858264   7.6034303  24.462635  ]\n",
      "Training loss [ 0.07045293  0.96672124  7.8152537  24.940964  ]\n",
      "Training loss [ 0.07124424  0.989186    7.9609056  24.99897   ]\n",
      "Training loss [ 0.05955433  0.9754448   7.3810577  24.114893  ]\n",
      "Training loss [ 0.06452069  0.98951495  8.222151   25.969023  ]\n",
      "Training loss [ 0.05757752  0.9858264   7.5429688  24.462635  ]\n",
      "Training loss [ 0.06197797  0.96672124  7.767838   24.940964  ]\n",
      "Training loss [ 0.06380695  0.989186    7.9556007  24.99897   ]\n",
      "Training loss [ 0.05184314  0.9754448   7.3788476  24.114893  ]\n",
      "Training loss [ 0.05909235  0.98951495  8.146714   25.969023  ]\n",
      "Training loss [ 0.05432862  0.9858264   7.41886    24.462635  ]\n",
      "Training loss [ 0.05648467  0.96672124  7.6581407  24.940964  ]\n",
      "Training loss [ 0.05928111  0.989186    7.8262334  24.99897   ]\n",
      "Training loss [ 0.04841091  0.9754448   7.1702642  24.114893  ]\n",
      "Training loss [ 0.05573098  0.98951495  8.023845   25.969023  ]\n",
      "Training loss [ 0.05060545  0.9858264   7.3604546  24.462635  ]\n",
      "Training loss [ 0.05327131  0.96672124  7.621661   24.940964  ]\n",
      "Training loss [ 0.05642696  0.989186    7.767784   24.99897   ]\n",
      "Training loss [ 5.1459584 10.901641  25.507702  45.642082 ]\n",
      "Training loss [ 0.35587317  0.9918637  11.418058   27.60292   ]\n",
      "Training loss [ 0.16555896  0.9645937  10.185713   26.743942  ]\n",
      "Training loss [ 0.10955129  0.9628109   9.977726   27.17973   ]\n",
      "Training loss [ 0.09497538  0.99574345  9.931891   27.006773  ]\n",
      "Training loss [ 0.08106072  0.9707141   9.734425   27.232388  ]\n",
      "Training loss [ 0.08240018  0.9918637   9.9084     27.602684  ]\n",
      "Training loss [ 0.06688553  0.9645937   9.58676    26.743942  ]\n",
      "Training loss [ 0.06996773  0.9628109   9.603115   27.17973   ]\n",
      "Training loss [ 0.06405026  0.99574345  8.792955   27.006773  ]\n",
      "Training loss [ 0.06182925  0.9707141   8.571121   27.232388  ]\n",
      "Training loss [ 0.06540805  0.9918637   8.752419   27.602684  ]\n",
      "Training loss [ 0.06080661  0.9645937   8.313721   26.743942  ]\n",
      "Training loss [ 0.05758319  0.9628109   8.485446   27.17973   ]\n",
      "Training loss [ 0.05706263  0.99574345  8.545993   27.006773  ]\n",
      "Training loss [ 0.05446887  0.9707141   8.404535   27.232388  ]\n",
      "Training loss [ 0.06361751  0.9918637   8.736827   27.602684  ]\n",
      "Training loss [ 0.05551127  0.9645937   8.3131075  26.743942  ]\n",
      "Training loss [ 0.05267952  0.9628109   8.479471   27.17973   ]\n",
      "Training loss [ 0.05388627  0.99574345  8.546314   27.006773  ]\n",
      "Training loss [ 0.05020146  0.9707141   8.394781   27.232388  ]\n",
      "Training loss [ 0.0600304  0.9918637  8.734986  27.602684 ]\n",
      "Training loss [ 0.05142175  0.9645937   8.129494   26.743942  ]\n",
      "Training loss [ 0.05454983  0.9628109   8.2950535  27.17973   ]\n",
      "Training loss [ 0.05013115  0.99574345  8.378786   27.006773  ]\n",
      "Training loss [ 2.1560373  9.587364  23.167494  41.699654 ]\n",
      "Training loss [ 0.2960564  0.9755472  9.559996  26.254868 ]\n",
      "Training loss [ 0.14858368  0.98082626  8.741582   25.180317  ]\n",
      "Training loss [ 0.08356448  0.9785229   8.561403   25.79727   ]\n",
      "Training loss [ 0.07585821  0.96926314  8.244741   24.938524  ]\n",
      "Training loss [ 0.07222116  0.9644847   8.465384   25.211643  ]\n",
      "Training loss [ 0.05753079  0.9755472   8.373066   26.254868  ]\n",
      "Training loss [ 0.06913693  0.98082626  7.9344726  25.180317  ]\n",
      "Training loss [ 0.04838023  0.9785229   7.8363132  25.79727   ]\n",
      "Training loss [ 0.0592902   0.96926314  7.609461   24.938524  ]\n",
      "Training loss [ 0.05552492  0.9644847   7.791258   25.211643  ]\n",
      "Training loss [ 0.04593608  0.9755472   7.738165   26.254868  ]\n",
      "Training loss [ 0.05704982  0.98082626  7.812362   25.180317  ]\n",
      "Training loss [ 0.04172097  0.9785229   7.8256383  25.79727   ]\n",
      "Training loss [ 0.05199705  0.96926314  7.607101   24.938524  ]\n",
      "Training loss [ 0.04751693  0.9644847   7.775388   25.211643  ]\n",
      "Training loss [ 0.03822599  0.9755472   7.736795   26.254868  ]\n",
      "Training loss [ 0.05031844  0.98082626  7.7518797  25.180317  ]\n",
      "Training loss [ 0.03763998  0.9785229   7.768523   25.79727   ]\n",
      "Training loss [ 0.04783436  0.96926314  7.5559373  24.938524  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.04523284  0.9644847   7.6951013  25.211643  ]\n",
      "Training loss [ 0.03483946  0.9755472   7.662633   26.254868  ]\n",
      "Training loss [ 0.04782228  0.98082626  7.748231   25.180317  ]\n",
      "Training loss [ 0.03626976  0.9785229   7.7197223  25.79727   ]\n",
      "Training loss [ 0.04456636  0.96926314  7.520953   24.938524  ]\n",
      "Training loss [ 3.9078796  8.793839  25.422714  43.489532 ]\n",
      "Training loss [ 0.26684394  1.0342982   9.787412   26.36366   ]\n",
      "Training loss [ 0.10428047  1.0271523   8.806278   26.13759   ]\n",
      "Training loss [ 0.07845922  1.0237913   9.117895   26.86586   ]\n",
      "Training loss [ 0.06425658  1.0104046   9.445297   27.722715  ]\n",
      "Training loss [ 0.07137962  1.0300411   9.316475   27.391579  ]\n",
      "Training loss [ 0.05197857  1.0342662   8.713725   26.36366   ]\n",
      "Training loss [ 0.0455583  1.0271523  8.127067  26.13759  ]\n",
      "Training loss [ 0.05213545  1.0237913   8.486692   26.86586   ]\n",
      "Training loss [ 0.05111018  1.0104046   8.825043   27.722715  ]\n",
      "Training loss [ 0.04945964  1.0300411   8.914715   27.391579  ]\n",
      "Training loss [ 0.03943747  1.0342662   8.639123   26.36366   ]\n",
      "Training loss [ 0.03760621  1.0271523   8.080448   26.13759   ]\n",
      "Training loss [ 0.04801989  1.0237913   8.441251   26.86586   ]\n",
      "Training loss [ 0.03935286  1.0104046   8.735753   27.722715  ]\n",
      "Training loss [ 0.04363926  1.0300411   8.78788    27.391579  ]\n",
      "Training loss [ 0.03009477  1.0342662   8.360362   26.36366   ]\n",
      "Training loss [ 0.02935203  1.0271523   7.7351837  26.13759   ]\n",
      "Training loss [ 0.03919386  1.0237913   8.163385   26.86586   ]\n",
      "Training loss [ 0.0347589  1.0104046  8.447507  27.722715 ]\n",
      "Training loss [ 0.03417844  1.0300411   8.460678   27.391579  ]\n",
      "Training loss [ 0.02692344  1.0342662   8.25545    26.36366   ]\n",
      "Training loss [ 0.02838851  1.0271523   7.7082644  26.13759   ]\n",
      "Training loss [ 0.03597905  1.0237913   8.135372   26.86586   ]\n",
      "Training loss [ 0.02961971  1.0104046   8.419601   27.722715  ]\n",
      "Training loss [ 3.4533095 11.304066  26.257753  49.175766 ]\n",
      "Training loss [ 0.26768   0.997244 10.67469  28.34683 ]\n",
      "Training loss [ 0.10246262  0.98620236  9.355337   28.332216  ]\n",
      "Training loss [ 0.09296259  0.9957729   9.304373   27.705837  ]\n",
      "Training loss [ 0.06531474  0.980998    9.114939   28.136501  ]\n",
      "Training loss [ 0.06700166  0.9773815   9.336032   27.970716  ]\n",
      "Training loss [ 0.0458626  0.997244   9.35413   28.346798 ]\n",
      "Training loss [ 0.04352794  0.98620236  9.06776    28.332216  ]\n",
      "Training loss [ 0.06055858  0.9957729   9.061048   27.705837  ]\n",
      "Training loss [ 0.04422794  0.980998    8.822693   28.136501  ]\n",
      "Training loss [ 0.04816898  0.9773815   9.12316    27.970716  ]\n",
      "Training loss [ 0.03163364  0.997244    9.144205   28.346798  ]\n",
      "Training loss [ 0.03723712  0.98620236  8.926282   28.332216  ]\n",
      "Training loss [ 0.05449194  0.9957729   8.992966   27.705837  ]\n",
      "Training loss [ 0.04346891  0.980998    8.810799   28.136501  ]\n",
      "Training loss [ 0.04025123  0.9773815   8.9990425  27.970716  ]\n",
      "Training loss [ 0.02850601  0.997244    8.936304   28.346798  ]\n",
      "Training loss [ 0.03253627  0.98620236  8.993743   28.332216  ]\n",
      "Training loss [ 0.05244165  0.9957729   8.945681   27.705837  ]\n",
      "Training loss [ 0.03436018  0.980998    8.636587   28.136501  ]\n",
      "Training loss [ 0.03831167  0.9773815   8.82653    27.970716  ]\n",
      "Training loss [2.5982779e-02 9.9724400e-01 8.7574100e+00 2.8346798e+01]\n",
      "Training loss [ 0.0317052   0.98620236  8.849003   28.332216  ]\n",
      "Training loss [ 0.04836684  0.9957729   8.820888   27.705837  ]\n",
      "Training loss [ 0.03333484  0.980998    8.583057   28.136501  ]\n",
      "Training loss [ 2.6384344  5.921255  23.699566  42.136467 ]\n",
      "Training loss [ 0.2381393   0.96396536  9.646502   25.126637  ]\n",
      "Training loss [ 0.08705562  0.9664001   9.450693   26.13087   ]\n",
      "Training loss [ 0.0709596   0.97217107  9.147017   26.070335  ]\n",
      "Training loss [ 0.06535405  0.9743204   9.22015    27.075974  ]\n",
      "Training loss [ 0.04334019  0.97006196  8.622411   25.203423  ]\n",
      "Training loss [ 0.04726712  0.9634556   8.994238   25.126637  ]\n",
      "Training loss [ 0.03901248  0.9664001   8.957478   26.13087   ]\n",
      "Training loss [ 0.03893312  0.97217107  8.829531   26.070335  ]\n",
      "Training loss [ 0.03822695  0.9743204   9.07066    27.075974  ]\n",
      "Training loss [ 0.03232891  0.97006196  8.379091   25.203423  ]\n",
      "Training loss [ 0.03831323  0.9634556   8.541412   25.126637  ]\n",
      "Training loss [ 0.02909939  0.9664001   8.681097   26.13087   ]\n",
      "Training loss [ 0.02758432  0.97217107  8.811602   26.070335  ]\n",
      "Training loss [ 0.03209426  0.9743204   8.854274   27.075974  ]\n",
      "Training loss [ 0.02798982  0.97006196  8.222412   25.203423  ]\n",
      "Training loss [ 0.03220898  0.9634556   8.287029   25.126637  ]\n",
      "Training loss [2.5678266e-02 9.6640009e-01 8.4087315e+00 2.6130871e+01]\n",
      "Training loss [2.4900533e-02 9.7217107e-01 8.6240149e+00 2.6070335e+01]\n",
      "Training loss [ 0.03006052  0.9743204   8.669907   27.075974  ]\n",
      "Training loss [2.5070354e-02 9.7006196e-01 8.0407448e+00 2.5203423e+01]\n",
      "Training loss [ 0.02890895  0.9634556   8.196744   25.126637  ]\n",
      "Training loss [2.3197759e-02 9.6640009e-01 8.2825136e+00 2.6130871e+01]\n",
      "Training loss [2.3569636e-02 9.7217107e-01 8.3358974e+00 2.6070335e+01]\n",
      "Training loss [ 0.02826096  0.9743204   8.529522   27.075974  ]\n",
      "Training loss [ 2.7772918  6.743432  24.530584  36.70073  ]\n",
      "Training loss [ 0.25662485  0.99673176 10.445911   26.146141  ]\n",
      "Training loss [ 0.10295427  0.99360794  9.292384   26.655521  ]\n",
      "Training loss [ 0.08907163  0.9948534   8.914951   26.279016  ]\n",
      "Training loss [ 0.0666349   0.99144775  8.690317   26.855532  ]\n",
      "Training loss [ 0.05905368  0.99647415  8.558864   26.007828  ]\n",
      "Training loss [ 0.065425    0.99639964  8.841251   26.146141  ]\n",
      "Training loss [ 0.04642621  0.99360794  8.4231825  26.655521  ]\n",
      "Training loss [ 0.04933026  0.9948534   8.376469   26.279016  ]\n",
      "Training loss [ 0.04257385  0.99144775  8.367961   26.855532  ]\n",
      "Training loss [ 0.05099772  0.99647415  7.989182   26.007828  ]\n",
      "Training loss [ 0.05202749  0.99639964  8.4286175  26.146141  ]\n",
      "Training loss [ 0.0415026   0.99360794  8.157882   26.655521  ]\n",
      "Training loss [ 0.03870458  0.9948534   8.080419   26.279016  ]\n",
      "Training loss [ 0.03271109  0.99144775  8.184484   26.855532  ]\n",
      "Training loss [ 0.04010703  0.99647415  7.7939816  26.007828  ]\n",
      "Training loss [ 0.03994788  0.99639964  8.215476   26.146141  ]\n",
      "Training loss [ 0.03679947  0.99360794  7.980007   26.655521  ]\n",
      "Training loss [ 0.03799687  0.9948534   7.9888196  26.279016  ]\n",
      "Training loss [ 0.03023681  0.99144775  8.089468   26.855532  ]\n",
      "Training loss [ 0.03558003  0.99647415  7.6752715  26.007828  ]\n",
      "Training loss [ 0.03569949  0.99639964  8.036758   26.146141  ]\n",
      "Training loss [ 0.0350105   0.99360794  7.8015623  26.655521  ]\n",
      "Training loss [ 0.03641038  0.9948534   7.795093   26.279016  ]\n",
      "Training loss [ 0.02855815  0.99144775  7.8878064  26.855532  ]\n",
      "Training loss [ 2.3731997  5.750148  24.167305  37.434254 ]\n",
      "Training loss [ 0.25857058  0.97883654  9.459614   25.08097   ]\n",
      "Training loss [ 0.14102918  0.98812246  9.248654   26.286728  ]\n",
      "Training loss [ 0.09855808  0.9872452   9.033733   26.350468  ]\n",
      "Training loss [ 0.06087127  0.9965359   8.660713   26.159294  ]\n",
      "Training loss [ 0.07754183  0.9908321   8.790186   25.648682  ]\n",
      "Training loss [ 0.05053359  0.97883654  8.211049   25.08097   ]\n",
      "Training loss [ 0.03220602  0.98812246  8.553062   26.286728  ]\n",
      "Training loss [ 0.05132159  0.9872452   8.532425   26.350468  ]\n",
      "Training loss [ 0.03703147  0.9965359   8.195364   26.159294  ]\n",
      "Training loss [ 0.0465847  0.9908321  8.319408  25.648682 ]\n",
      "Training loss [ 0.03354454  0.97883654  7.8167458  25.08097   ]\n",
      "Training loss [2.3490064e-02 9.8812246e-01 8.2928982e+00 2.6286728e+01]\n",
      "Training loss [ 0.04423343  0.9872452   8.301432   26.350468  ]\n",
      "Training loss [ 0.02780689  0.9965359   8.059185   26.159294  ]\n",
      "Training loss [ 0.03533743  0.9908321   8.194127   25.648682  ]\n",
      "Training loss [ 0.02901922  0.97883654  7.7326264  25.08097   ]\n",
      "Training loss [1.9429557e-02 9.8812246e-01 8.1804638e+00 2.6286728e+01]\n",
      "Training loss [ 0.03689825  0.9872452   8.181872   26.350468  ]\n",
      "Training loss [2.3304511e-02 9.9653590e-01 7.9210191e+00 2.6159294e+01]\n",
      "Training loss [ 0.03154743  0.9908321   8.037359   25.648682  ]\n",
      "Training loss [ 0.0260545   0.97883654  7.59731    25.08097   ]\n",
      "Training loss [1.7060021e-02 9.8812246e-01 8.0814571e+00 2.6286728e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.03343673  0.9872452   8.115046   26.350468  ]\n",
      "Training loss [2.0930436e-02 9.9653590e-01 7.9030056e+00 2.6159294e+01]\n",
      "Training loss [ 2.1256926  4.0854874 23.616863  34.8001   ]\n",
      "Training loss [ 0.3380887  1.0030936 10.914703  26.369673 ]\n",
      "Training loss [ 0.18731637  0.99859273  9.536079   24.709862  ]\n",
      "Training loss [ 0.10063824  0.99960434  8.923463   24.6406    ]\n",
      "Training loss [ 0.09546563  1.012641    9.337833   25.330551  ]\n",
      "Training loss [ 0.08718033  1.0035384   9.026216   25.0797    ]\n",
      "Training loss [ 0.08303705  1.0030936   9.296087   26.369673  ]\n",
      "Training loss [ 0.11034582  0.99859273  8.967865   24.709862  ]\n",
      "Training loss [ 0.07161206  0.99960434  8.429374   24.6406    ]\n",
      "Training loss [ 0.05889124  1.012641    8.792687   25.330551  ]\n",
      "Training loss [ 0.05854641  1.0035384   8.550575   25.0797    ]\n",
      "Training loss [ 0.06867594  1.0030936   8.989664   26.369673  ]\n",
      "Training loss [ 0.07134922  0.99859273  8.45466    24.709862  ]\n",
      "Training loss [ 0.05274701  0.99960434  8.094583   24.6406    ]\n",
      "Training loss [ 0.0533454  1.012641   8.50188   25.330551 ]\n",
      "Training loss [ 0.04869385  1.0035384   8.334652   25.0797    ]\n",
      "Training loss [ 0.05393322  1.0030936   8.555405   26.369673  ]\n",
      "Training loss [ 0.05920128  0.99859273  8.205603   24.709862  ]\n",
      "Training loss [ 0.04608529  0.99960434  7.9284644  24.6406    ]\n",
      "Training loss [ 0.04599845  1.012641    8.409954   25.330551  ]\n",
      "Training loss [ 0.04867284  1.0035384   8.267345   25.0797    ]\n",
      "Training loss [ 0.04745755  1.0030936   8.51096    26.369673  ]\n",
      "Training loss [ 0.04926508  0.99859273  8.149139   24.709862  ]\n",
      "Training loss [ 0.04370213  0.99960434  7.848871   24.6406    ]\n",
      "Training loss [ 0.04456405  1.012641    8.272352   25.330551  ]\n",
      "Training loss [ 1.9404111  3.381604  25.12537   42.875847 ]\n",
      "Training loss [ 0.28658855  0.8925359   8.324718   26.12305   ]\n",
      "Training loss [ 0.26228526  0.92488074  7.270418   25.774351  ]\n",
      "Training loss [ 0.25476602  0.85442084  7.145178   26.020678  ]\n",
      "Training loss [ 0.19551122  0.9013337   6.918151   26.239975  ]\n",
      "Training loss [ 0.20579161  0.8537554   6.682716   26.27159   ]\n",
      "Training loss [ 0.21205246  0.8925359   6.806326   26.12305   ]\n",
      "Training loss [ 0.23089805  0.92488074  6.6148567  25.774351  ]\n",
      "Training loss [ 0.23322967  0.85442084  6.714985   26.020678  ]\n",
      "Training loss [ 0.18167895  0.9013337   6.5336328  26.239975  ]\n",
      "Training loss [ 0.19722502  0.8537554   6.363184   26.27159   ]\n",
      "Training loss [ 0.20488456  0.8925359   6.565222   26.12305   ]\n",
      "Training loss [ 0.2197673   0.92488074  6.4263334  25.774351  ]\n",
      "Training loss [ 0.22368968  0.85442084  6.5438294  26.020678  ]\n",
      "Training loss [ 0.1759622  0.9013337  6.4226885 26.239975 ]\n",
      "Training loss [ 0.19058856  0.8537554   6.25127    26.27159   ]\n",
      "Training loss [ 0.20367797  0.8925359   6.503913   26.12305   ]\n",
      "Training loss [ 0.21458611  0.92488074  6.3796697  25.774351  ]\n",
      "Training loss [ 0.21370007  0.85442084  6.5028944  26.020678  ]\n",
      "Training loss [ 0.17365268  0.9013337   6.3847857  26.239975  ]\n",
      "Training loss [ 0.18635672  0.8537554   6.2162795  26.27159   ]\n",
      "Training loss [ 0.20208994  0.8925359   6.4915066  26.12305   ]\n",
      "Training loss [ 0.20882414  0.92488074  6.370696   25.774351  ]\n",
      "Training loss [ 0.21154734  0.85442084  6.492962   26.020678  ]\n",
      "Training loss [ 0.17177458  0.9013337   6.3737946  26.239975  ]\n",
      "Training loss [ 1.2927976  4.444029  25.38164   40.894825 ]\n",
      "Training loss [ 0.29791474  0.9450507  10.934384   26.007343  ]\n",
      "Training loss [ 0.27072945  0.9486761  10.804425   26.623833  ]\n",
      "Training loss [ 0.23460644  1.014337   10.516123   27.18167   ]\n",
      "Training loss [ 0.24632981  0.9194584   9.964399   24.98581   ]\n",
      "Training loss [ 0.2568093  1.0095785 10.635009  26.494047 ]\n",
      "Training loss [ 0.21701165  0.9450507   9.803927   26.007343  ]\n",
      "Training loss [ 0.23780975  0.9486761  10.3402195  26.623833  ]\n",
      "Training loss [ 0.21459584  1.014337   10.328606   27.18167   ]\n",
      "Training loss [ 0.22598493  0.9194584   9.818393   24.98581   ]\n",
      "Training loss [ 0.23176505  1.0095785  10.600292   26.494047  ]\n",
      "Training loss [ 0.21013564  0.9450507   9.749288   26.007343  ]\n",
      "Training loss [ 0.2211813  0.9486761 10.270578  26.623833 ]\n",
      "Training loss [ 0.20320144  1.014337   10.299896   27.18167   ]\n",
      "Training loss [ 0.2178269  0.9194584  9.784467  24.98581  ]\n",
      "Training loss [ 0.22022566  1.0095785  10.585713   26.494047  ]\n",
      "Training loss [ 0.20266344  0.9450507   9.737316   26.007343  ]\n",
      "Training loss [ 0.21870962  0.9486761  10.243458   26.623833  ]\n",
      "Training loss [ 0.18377723  1.014337   10.281342   27.18167   ]\n",
      "Training loss [ 0.20942551  0.9194584   9.771162   24.98581   ]\n",
      "Training loss [ 0.2208356  1.0095785 10.579755  26.494047 ]\n",
      "Training loss [ 0.19883096  0.9450507   9.733753   26.007343  ]\n",
      "Training loss [ 0.21531394  0.9486761  10.2343235  26.623833  ]\n",
      "Training loss [ 0.18140976  1.014337   10.270597   27.18167   ]\n",
      "Training loss [ 0.2066649  0.9194584  9.763187  24.98581  ]\n",
      "Training loss [ 2.3517907  6.1248426 28.397577  52.37726  ]\n",
      "Training loss [ 0.4654734   0.96761143 15.453475   27.674732  ]\n",
      "Training loss [ 0.31617406  0.97465515 13.502911   27.652359  ]\n",
      "Training loss [ 0.33920148  1.0554167  15.096665   32.558483  ]\n",
      "Training loss [ 0.29180863  0.9393163  13.602106   28.447142  ]\n",
      "Training loss [ 0.28834212  1.0019593  14.0378275  29.337805  ]\n",
      "Training loss [ 0.26473036  0.96761143 13.095066   27.674408  ]\n",
      "Training loss [ 0.22426823  0.97465515 13.038914   27.652359  ]\n",
      "Training loss [ 0.27595204  1.0554167  14.893935   32.558483  ]\n",
      "Training loss [ 0.23737076  0.9393163  13.487564   28.447142  ]\n",
      "Training loss [ 0.2576089  1.0019593 13.930645  29.337805 ]\n",
      "Training loss [ 0.23625597  0.96761143 13.100275   27.674408  ]\n",
      "Training loss [ 0.20379767  0.97465515 13.029635   27.652359  ]\n",
      "Training loss [ 0.25584927  1.0554167  14.874632   32.558483  ]\n",
      "Training loss [ 0.22438905  0.9393163  13.487972   28.447142  ]\n",
      "Training loss [ 0.24090946  1.0019593  13.926277   29.337805  ]\n",
      "Training loss [ 0.22736877  0.96761143 13.109489   27.674408  ]\n",
      "Training loss [ 0.19248477  0.97465515 13.028532   27.652359  ]\n",
      "Training loss [ 0.2452588  1.0554167 14.87155   32.558483 ]\n",
      "Training loss [ 0.21583849  0.9393163  13.488425   28.447142  ]\n",
      "Training loss [ 0.22790009  1.0019593  13.926093   29.337805  ]\n",
      "Training loss [ 0.2191897   0.96761143 13.112122   27.674408  ]\n",
      "Training loss [ 0.18802786  0.97465515 13.02842    27.652359  ]\n",
      "Training loss [ 0.23717204  1.0554167  14.870462   32.558483  ]\n",
      "Training loss [ 0.20897445  0.9393163  13.4891205  28.447142  ]\n",
      "Training loss [ 2.8364427  8.068238  23.506298  35.237072 ]\n",
      "Training loss [ 0.4097308   0.97084475 10.3919115  25.745066  ]\n",
      "Training loss [ 0.32416242  1.021483    8.843782   24.789145  ]\n",
      "Training loss [ 0.27612695  1.0115724   8.299481   23.874947  ]\n",
      "Training loss [ 0.2587353  0.9503644  8.551371  23.99782  ]\n",
      "Training loss [ 0.27735177  1.0799886   8.965858   24.76666   ]\n",
      "Training loss [ 0.24205579  0.9706323   9.483412   25.745066  ]\n",
      "Training loss [ 0.25525126  1.021483    8.499038   24.789145  ]\n",
      "Training loss [ 0.23338014  1.0115724   8.18291    23.874947  ]\n",
      "Training loss [ 0.22962907  0.9503644   8.48651    23.99782   ]\n",
      "Training loss [ 0.261899   1.0799886  8.959757  24.76666  ]\n",
      "Training loss [ 0.22640646  0.9706323   9.469799   25.745066  ]\n",
      "Training loss [ 0.24347755  1.021483    8.4900255  24.789145  ]\n",
      "Training loss [ 0.21703868  1.0115724   8.1742935  23.874947  ]\n",
      "Training loss [ 0.2174935  0.9503644  8.481072  23.99782  ]\n",
      "Training loss [ 0.25194    1.0799886  8.9581585 24.76666  ]\n",
      "Training loss [ 0.2182052  0.9706323  9.466564  25.745066 ]\n",
      "Training loss [ 0.23361707  1.021483    8.489536   24.789145  ]\n",
      "Training loss [ 0.20774186  1.0115724   8.173798   23.874947  ]\n",
      "Training loss [ 0.21027836  0.9503644   8.481316   23.99782   ]\n",
      "Training loss [ 0.24686709  1.0799886   8.955978   24.76666   ]\n",
      "Training loss [ 0.21201016  0.9706323   9.464839   25.745066  ]\n",
      "Training loss [ 0.22861405  1.021483    8.488853   24.789145  ]\n",
      "Training loss [ 0.20469403  1.0115724   8.172181   23.874947  ]\n",
      "Training loss [ 0.20448701  0.9503644   8.483307   23.99782   ]\n",
      "Training loss [ 5.915475 19.158022 28.151264 56.80243 ]\n",
      "Training loss [ 0.5618721  1.0285432 10.124966  30.733742 ]\n",
      "Training loss [ 0.36450842  1.0526718   8.698714   30.245413  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.30654734  1.0270736   6.8812456  29.61969   ]\n",
      "Training loss [ 0.29230583  1.0547264   7.185392   29.81212   ]\n",
      "Training loss [ 0.28855786  1.0529917   7.2409186  30.357906  ]\n",
      "Training loss [ 0.25636363  1.0283302   7.387003   30.733742  ]\n",
      "Training loss [ 0.21447144  1.0526648   7.037855   30.245413  ]\n",
      "Training loss [ 0.22663811  1.0270565   6.6783323  29.61969   ]\n",
      "Training loss [ 0.24998313  1.0547264   7.130016   29.81212   ]\n",
      "Training loss [ 0.25184602  1.0529917   7.220239   30.357906  ]\n",
      "Training loss [ 0.21898314  1.0283302   7.385633   30.733742  ]\n",
      "Training loss [ 0.19856912  1.0526648   7.037373   30.245413  ]\n",
      "Training loss [ 0.1873133  1.0270565  6.675355  29.61969  ]\n",
      "Training loss [ 0.213745   1.0547264  7.1300173 29.81212  ]\n",
      "Training loss [ 0.22983724  1.0529917   7.2200556  30.357906  ]\n",
      "Training loss [ 0.20509154  1.0283302   7.386427   30.733742  ]\n",
      "Training loss [ 0.17840204  1.0526648   7.0367417  30.245413  ]\n",
      "Training loss [ 0.17181104  1.0270565   6.6740947  29.61969   ]\n",
      "Training loss [ 0.20405565  1.0547264   7.1294107  29.81212   ]\n",
      "Training loss [ 0.22478305  1.0529917   7.2199273  30.357906  ]\n",
      "Training loss [ 0.1864146  1.0283302  7.3859043 30.733742 ]\n",
      "Training loss [ 0.1714345  1.0526648  7.036212  30.245413 ]\n",
      "Training loss [ 0.16544892  1.0270565   6.6750383  29.61969   ]\n",
      "Training loss [ 0.19117656  1.0547264   7.12941    29.81212   ]\n",
      "Training loss [ 4.6236916  9.385326  20.993633  39.045536 ]\n",
      "Training loss [ 0.5119732  1.0137159  8.254337  23.19781  ]\n",
      "Training loss [ 0.33643585  1.0206909   7.2369256  22.31692   ]\n",
      "Training loss [ 0.2619273  1.0557549  7.1677217 21.94635  ]\n",
      "Training loss [ 0.25813088  1.0126625   8.343893   24.278336  ]\n",
      "Training loss [ 0.25452924  1.0673742   7.409655   22.699131  ]\n",
      "Training loss [ 0.2391143  1.0137159  7.589188  23.197836 ]\n",
      "Training loss [ 0.2157071  1.0206909  7.110718  22.31692  ]\n",
      "Training loss [ 0.2028048  1.0557549  7.1132693 21.94635  ]\n",
      "Training loss [ 0.21373834  1.0126625   8.332256   24.278336  ]\n",
      "Training loss [ 0.2267158  1.0673742  7.406572  22.699131 ]\n",
      "Training loss [ 0.21993686  1.0137159   7.590766   23.19781   ]\n",
      "Training loss [ 0.1979497  1.0206909  7.1097403 22.31692  ]\n",
      "Training loss [ 0.19260538  1.0557549   7.111479   21.94635   ]\n",
      "Training loss [ 0.20088083  1.0126625   8.332099   24.278336  ]\n",
      "Training loss [ 0.21370926  1.0673742   7.407817   22.699131  ]\n",
      "Training loss [ 0.2094091  1.0137159  7.590354  23.19781  ]\n",
      "Training loss [ 0.18891653  1.0206909   7.109071   22.31692   ]\n",
      "Training loss [ 0.1854949  1.0557549  7.1098914 21.94635  ]\n",
      "Training loss [ 0.18906614  1.0126625   8.3308525  24.278336  ]\n",
      "Training loss [ 0.20530498  1.0673742   7.4069557  22.699131  ]\n",
      "Training loss [ 0.20173582  1.0137159   7.5900927  23.19781   ]\n",
      "Training loss [ 0.18042469  1.0206909   7.1095233  22.31692   ]\n",
      "Training loss [ 0.17769252  1.0557549   7.110168   21.94635   ]\n",
      "Training loss [ 0.18499282  1.0126625   8.330859   24.278336  ]\n",
      "Training loss [ 4.474864 11.314382 23.961098 40.649235]\n",
      "Training loss [ 0.52723867  1.0757574   8.042299   24.978409  ]\n",
      "Training loss [ 0.29925507  1.1346883   6.9292097  22.770813  ]\n",
      "Training loss [ 0.22224627  1.0857831   6.9892774  24.743004  ]\n",
      "Training loss [ 0.2152048  1.087292   6.9314756 23.867628 ]\n",
      "Training loss [ 0.20823096  1.073149    7.4538383  25.721697  ]\n",
      "Training loss [ 0.2122716  1.0756817  7.229515  24.972303 ]\n",
      "Training loss [ 0.15334518  1.1346883   6.7211637  22.770813  ]\n",
      "Training loss [ 0.13630526  1.0857831   6.8984447  24.743004  ]\n",
      "Training loss [ 0.16890459  1.087292    6.925574   23.867628  ]\n",
      "Training loss [ 0.17034751  1.073149    7.440896   25.721697  ]\n",
      "Training loss [ 0.18125895  1.0756817   7.2269173  24.972303  ]\n",
      "Training loss [ 0.14175026  1.1346883   6.71908    22.770813  ]\n",
      "Training loss [ 0.11548433  1.0857831   6.8980293  24.743004  ]\n",
      "Training loss [ 0.13947952  1.087292    6.925581   23.867628  ]\n",
      "Training loss [ 0.14139196  1.073149    7.440625   25.721697  ]\n",
      "Training loss [ 0.15315543  1.0756817   7.226501   24.972303  ]\n",
      "Training loss [ 0.11103925  1.1346883   6.7191358  22.770813  ]\n",
      "Training loss [ 0.09704797  1.0857831   6.8982296  24.743004  ]\n",
      "Training loss [ 0.13350007  1.087292    6.925816   23.867628  ]\n",
      "Training loss [ 0.13586095  1.073149    7.440462   25.721697  ]\n",
      "Training loss [ 0.14589134  1.0756817   7.22448    24.972303  ]\n",
      "Training loss [ 0.10677975  1.1346883   6.718261   22.770813  ]\n",
      "Training loss [ 0.09327407  1.0857831   6.8997393  24.743004  ]\n",
      "Training loss [ 0.13013387  1.087292    6.9249935  23.867628  ]\n",
      "Training loss [ 3.4204981  9.867006  21.261236  39.813686 ]\n",
      "Training loss [ 0.5129505   0.89892346  7.923756   24.397173  ]\n",
      "Training loss [ 0.2707473   0.89501476  6.8118124  23.914032  ]\n",
      "Training loss [ 0.19294362  0.8805814   7.290037   23.356052  ]\n",
      "Training loss [ 0.1568082  0.9094399  7.204606  23.94411  ]\n",
      "Training loss [ 0.1186447   0.86942184  7.0270863  23.221085  ]\n",
      "Training loss [ 0.11230496  0.89877516  7.255472   24.397104  ]\n",
      "Training loss [ 0.10891497  0.89501476  6.747445   23.914032  ]\n",
      "Training loss [ 0.11691678  0.8805814   7.289408   23.356052  ]\n",
      "Training loss [ 0.11830147  0.9094399   7.2045116  23.94411   ]\n",
      "Training loss [ 0.08759007  0.86942184  7.0265903  23.221085  ]\n",
      "Training loss [ 0.08837146  0.89877516  7.255821   24.397104  ]\n",
      "Training loss [ 0.09204343  0.89501476  6.745858   23.914032  ]\n",
      "Training loss [ 0.1071323  0.8805814  7.2886515 23.356052 ]\n",
      "Training loss [ 0.10045623  0.9094399   7.2038965  23.94411   ]\n",
      "Training loss [ 0.08059825  0.86942184  7.026376   23.221085  ]\n",
      "Training loss [ 0.07993883  0.89877516  7.255726   24.397104  ]\n",
      "Training loss [ 0.08291803  0.89501476  6.7449417  23.914032  ]\n",
      "Training loss [ 0.10089736  0.8805814   7.287941   23.356052  ]\n",
      "Training loss [ 0.09256687  0.9094399   7.2032537  23.94411   ]\n",
      "Training loss [ 0.0785363   0.86942184  7.025839   23.221085  ]\n",
      "Training loss [ 0.07879739  0.89877516  6.8721657  24.397104  ]\n",
      "Training loss [ 0.07946554  0.89501476  6.4138436  23.914032  ]\n",
      "Training loss [ 0.09630699  0.8805814   6.814095   23.356052  ]\n",
      "Training loss [ 0.08932918  0.9094399   6.9081364  23.94411   ]\n",
      "Training loss [ 4.1617985 12.648637  23.750994  38.144417 ]\n",
      "Training loss [ 0.52520704  0.9621906   9.575405   25.58244   ]\n",
      "Training loss [ 0.2749861  0.9736357  8.297601  24.445675 ]\n",
      "Training loss [ 0.19004656  0.97324085  8.828903   26.10027   ]\n",
      "Training loss [ 0.15715782  0.9677549   8.58753    25.11017   ]\n",
      "Training loss [ 0.14046258  0.9538508   8.567852   25.390299  ]\n",
      "Training loss [ 0.1281639   0.96204567  9.156128   25.58244   ]\n",
      "Training loss [ 0.12364887  0.9736357   7.036149   24.445435  ]\n",
      "Training loss [ 0.12602061  0.97324085  7.7061195  26.10027   ]\n",
      "Training loss [ 0.11673086  0.9677549   7.419939   25.11017   ]\n",
      "Training loss [ 0.11212081  0.9538508   7.277852   25.390299  ]\n",
      "Training loss [ 0.1095193   0.96204567  7.817006   25.58244   ]\n",
      "Training loss [ 0.11114038  0.9736357   7.0186133  24.445435  ]\n",
      "Training loss [ 0.11367943  0.97324085  7.70212    26.10027   ]\n",
      "Training loss [ 0.10714942  0.9677549   7.416665   25.11017   ]\n",
      "Training loss [ 0.10250305  0.9538508   7.278007   25.390299  ]\n",
      "Training loss [ 0.10324425  0.96204567  7.816142   25.58244   ]\n",
      "Training loss [ 0.10447434  0.9736357   7.0174556  24.445435  ]\n",
      "Training loss [ 0.10808121  0.97324085  7.7031302  26.10027   ]\n",
      "Training loss [ 0.10274807  0.9677549   7.4145904  25.11017   ]\n",
      "Training loss [ 0.09767802  0.9538508   7.2775774  25.390299  ]\n",
      "Training loss [ 0.08835352  0.96204567  7.81673    25.58244   ]\n",
      "Training loss [ 0.07334311  0.9736357   7.0163574  24.445435  ]\n",
      "Training loss [ 0.08657607  0.97324085  7.7024007  26.10027   ]\n",
      "Training loss [ 0.07734881  0.9677549   7.4125233  25.11017   ]\n",
      "Training loss [ 3.9982305 12.758912  22.985115  45.40087  ]\n",
      "Training loss [ 0.4962994  0.9838687 10.016909  24.872505 ]\n",
      "Training loss [ 0.2266961  0.9781718  8.029596  24.600758 ]\n",
      "Training loss [ 0.1488469  1.005909   8.361373  25.491554 ]\n",
      "Training loss [ 0.12576225  0.9991426   8.690718   26.572407  ]\n",
      "Training loss [ 0.10785608  0.9783894   8.182681   24.761826  ]\n",
      "Training loss [ 0.08640425  0.9838687   8.103987   24.872505  ]\n",
      "Training loss [ 0.09213312  0.9781718   7.87716    24.600758  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.09090378  1.005909    8.355177   25.491554  ]\n",
      "Training loss [ 0.09400212  0.9991426   8.684866   26.572407  ]\n",
      "Training loss [ 0.08881222  0.9783894   8.177354   24.761826  ]\n",
      "Training loss [ 0.07324412  0.9838687   8.100361   24.872505  ]\n",
      "Training loss [ 0.08671343  0.9781718   7.87255    24.600758  ]\n",
      "Training loss [ 0.07899131  1.005909    8.354503   25.491554  ]\n",
      "Training loss [ 0.08436336  0.9991426   8.47138    26.572407  ]\n",
      "Training loss [ 0.08172197  0.9783894   7.9015026  24.761826  ]\n",
      "Training loss [ 0.0699231  0.9838687  7.8340917 24.872505 ]\n",
      "Training loss [ 0.08297487  0.9781718   7.606355   24.600758  ]\n",
      "Training loss [ 0.07243603  1.005909    8.119217   25.491554  ]\n",
      "Training loss [ 0.08038975  0.9991426   8.464083   26.572407  ]\n",
      "Training loss [ 0.07874675  0.9783894   7.891654   24.761826  ]\n",
      "Training loss [ 0.06795293  0.9838687   7.832549   24.872505  ]\n",
      "Training loss [ 0.07871675  0.9781718   7.6046624  24.600758  ]\n",
      "Training loss [ 0.06876282  1.005909    8.117102   25.491554  ]\n",
      "Training loss [ 0.07775369  0.9991426   8.462597   26.572407  ]\n",
      "Training loss [ 4.9011707  6.742388  24.527147  45.20423  ]\n",
      "Training loss [ 0.45624113  0.97841376 10.023111   26.758833  ]\n",
      "Training loss [ 0.22089864  0.9685329   9.4414425  25.748428  ]\n",
      "Training loss [ 0.15293865  0.97367793 10.205748   27.251106  ]\n",
      "Training loss [ 0.10923876  0.9835073   9.558952   26.611584  ]\n",
      "Training loss [ 0.10584247  0.97929955  9.518517   26.3455    ]\n",
      "Training loss [ 0.09803164  0.97840804  9.669928   26.758833  ]\n",
      "Training loss [ 0.09632011  0.9685329   9.405689   25.748428  ]\n",
      "Training loss [ 0.0939803   0.97367793 10.203206   27.251106  ]\n",
      "Training loss [ 0.07660574  0.9835073   9.473934   26.611584  ]\n",
      "Training loss [ 0.08330339  0.97929955  9.286341   26.3455    ]\n",
      "Training loss [ 0.08049185  0.97840804  8.571527   26.758833  ]\n",
      "Training loss [ 0.08331654  0.9685329   8.355266   25.748428  ]\n",
      "Training loss [ 0.07775481  0.97367793  8.818463   27.251106  ]\n",
      "Training loss [ 0.05970198  0.9835073   8.304115   26.611584  ]\n",
      "Training loss [ 0.06949919  0.97929955  8.251352   26.3455    ]\n",
      "Training loss [ 0.06748103  0.97840804  8.470017   26.758833  ]\n",
      "Training loss [ 0.06906036  0.9685329   8.293464   25.748428  ]\n",
      "Training loss [ 0.07012884  0.97367793  8.816467   27.251106  ]\n",
      "Training loss [ 0.05624317  0.9835073   8.301515   26.611584  ]\n",
      "Training loss [ 0.06564812  0.97929955  8.250615   26.3455    ]\n",
      "Training loss [ 0.06263372  0.97840804  8.470104   26.758833  ]\n",
      "Training loss [ 0.06488852  0.9685329   8.171924   25.748428  ]\n",
      "Training loss [ 0.06704134  0.97367793  8.667293   27.251106  ]\n",
      "Training loss [ 0.0551031  0.9835073  8.17032   26.611584 ]\n",
      "Training loss [ 4.752797 14.792022 22.422186 36.424057]\n",
      "Training loss [ 0.38561052  0.99394345  9.803028   24.915413  ]\n",
      "Training loss [ 0.18165213  0.964507    9.277787   24.659903  ]\n",
      "Training loss [ 0.13216044  0.9960331   8.833227   23.600166  ]\n",
      "Training loss [ 0.09054452  0.9865955   9.306921   24.529854  ]\n",
      "Training loss [ 0.09241512  0.9720577   9.396288   24.630838  ]\n",
      "Training loss [ 0.0789664   0.99394345  9.327548   24.915413  ]\n",
      "Training loss [ 0.07064241  0.96450675  9.166662   24.659903  ]\n",
      "Training loss [ 0.0868651  0.9960331  8.5326    23.600166 ]\n",
      "Training loss [ 0.06892816  0.9865955   8.359037   24.529854  ]\n",
      "Training loss [ 0.07278958  0.9720577   8.429079   24.630838  ]\n",
      "Training loss [ 0.06611262  0.99394345  8.409473   24.915413  ]\n",
      "Training loss [ 0.0615126   0.96450675  8.220548   24.659903  ]\n",
      "Training loss [ 0.07754821  0.9960331   7.916075   23.600166  ]\n",
      "Training loss [ 0.0596484  0.9865955  8.195836  24.529854 ]\n",
      "Training loss [ 0.06888561  0.9720577   8.26531    24.630838  ]\n",
      "Training loss [ 0.06208891  0.99394345  8.238817   24.915413  ]\n",
      "Training loss [ 0.05732606  0.96450675  8.131489   24.659903  ]\n",
      "Training loss [ 0.07165152  0.9960331   7.8545985  23.600166  ]\n",
      "Training loss [ 0.05676192  0.9865955   8.191441   24.529854  ]\n",
      "Training loss [ 0.06488405  0.9720577   8.263115   24.630838  ]\n",
      "Training loss [ 0.05911196  0.99394345  8.236872   24.915413  ]\n",
      "Training loss [ 0.05636414  0.96450675  8.129564   24.659903  ]\n",
      "Training loss [ 0.06778602  0.9960331   7.8525324  23.600166  ]\n",
      "Training loss [ 0.05287267  0.9865955   8.190781   24.529854  ]\n",
      "Training loss [ 4.0943484 11.145694  24.436775  49.056244 ]\n",
      "Training loss [ 0.3574624  1.0232043 10.944172  25.919432 ]\n",
      "Training loss [ 0.1723511  1.0269953 10.742506  25.882204 ]\n",
      "Training loss [ 0.11363786  1.0599642  10.829628   26.195702  ]\n",
      "Training loss [ 0.087437   1.0116757 10.542341  25.327198 ]\n",
      "Training loss [ 0.08914341  1.0391216  10.858377   26.05181   ]\n",
      "Training loss [ 0.07303353  1.0232043  10.628198   25.919243  ]\n",
      "Training loss [ 0.07049485  1.0269953  10.704401   25.882204  ]\n",
      "Training loss [ 0.06358923  1.0599642  10.682985   26.195702  ]\n",
      "Training loss [ 0.05405745  1.0116757   9.885056   25.327198  ]\n",
      "Training loss [ 0.07128929  1.0391216  10.131578   26.05181   ]\n",
      "Training loss [ 0.05264907  1.0232043   9.959446   25.919243  ]\n",
      "Training loss [ 0.06058109  1.0269953   9.972368   25.882204  ]\n",
      "Training loss [ 0.05128571  1.0599642  10.060355   26.195702  ]\n",
      "Training loss [ 0.05132014  1.0116757   9.863496   25.327198  ]\n",
      "Training loss [ 0.06385051  1.0391216  10.125039   26.05181   ]\n",
      "Training loss [ 0.04566546  1.0232043   9.95532    25.919243  ]\n",
      "Training loss [ 0.05665511  1.0269953   9.970058   25.882204  ]\n",
      "Training loss [ 0.04712587  1.0599642  10.057959   26.195702  ]\n",
      "Training loss [ 0.04570332  1.0116757   9.862686   25.327198  ]\n",
      "Training loss [ 0.06074926  1.0391216  10.120814   26.05181   ]\n",
      "Training loss [ 0.04347058  1.0232043   9.954294   25.919243  ]\n",
      "Training loss [ 0.0554938  1.0269953  9.969022  25.882204 ]\n",
      "Training loss [ 0.044187   1.0599642  9.83788   26.195702 ]\n",
      "Training loss [ 0.04438438  1.0116757   9.618217   25.327198  ]\n",
      "Training loss [ 5.1748466 10.493036  24.04908   49.636024 ]\n",
      "Training loss [ 0.381568  1.040213  9.072867 25.065153]\n",
      "Training loss [ 0.20728844  1.0481286   8.740301   25.75928   ]\n",
      "Training loss [ 0.11897858  1.054852    8.019124   24.86435   ]\n",
      "Training loss [ 0.09353853  1.043192    8.230694   25.128632  ]\n",
      "Training loss [ 0.09800108  1.0575026   8.007372   26.032896  ]\n",
      "Training loss [ 0.07818601  1.040213    7.7083025  25.065027  ]\n",
      "Training loss [ 0.06402702  1.0481286   7.865913   25.75912   ]\n",
      "Training loss [ 0.06058387  1.054852    7.51913    24.86435   ]\n",
      "Training loss [ 0.05747066  1.043192    7.73414    25.128632  ]\n",
      "Training loss [ 0.06160578  1.0575026   7.970909   26.032896  ]\n",
      "Training loss [ 0.05699223  1.040213    7.6891356  25.065027  ]\n",
      "Training loss [ 0.04571007  1.0481286   7.856864   25.75912   ]\n",
      "Training loss [ 0.0503903  1.054852   7.5159607 24.86435  ]\n",
      "Training loss [ 0.04700942  1.043192    7.467971   25.128632  ]\n",
      "Training loss [ 0.05886713  1.0575026   7.7151203  26.032896  ]\n",
      "Training loss [ 0.05063547  1.040213    7.4197993  25.065027  ]\n",
      "Training loss [ 0.03814163  1.0481286   7.6015353  25.75912   ]\n",
      "Training loss [ 0.04588814  1.054852    7.2729855  24.86435   ]\n",
      "Training loss [ 0.04216809  1.043192    7.41797    25.128632  ]\n",
      "Training loss [ 0.05083069  1.0575026   7.6812134  26.032896  ]\n",
      "Training loss [ 0.04694888  1.040213    7.3965693  25.065027  ]\n",
      "Training loss [ 0.03532355  1.0481286   7.5733294  25.75912   ]\n",
      "Training loss [ 0.04360323  1.054852    7.158516   24.86435   ]\n",
      "Training loss [ 0.04097742  1.043192    7.30517    25.128632  ]\n",
      "Training loss [ 2.2694516 12.509729  22.084045  42.644608 ]\n",
      "Training loss [ 0.2678821   0.98445606  8.896552   25.928986  ]\n",
      "Training loss [ 0.11086284  0.98958963  8.761132   26.174112  ]\n",
      "Training loss [ 0.08274154  1.0012546   8.737385   26.078255  ]\n",
      "Training loss [ 0.07714655  0.9814695   8.4191265  25.614178  ]\n",
      "Training loss [ 0.07075041  0.990433    8.0101185  24.326603  ]\n",
      "Training loss [ 0.05575718  0.98445606  8.088745   25.928986  ]\n",
      "Training loss [ 0.04951908  0.98958963  8.214716   26.174112  ]\n",
      "Training loss [ 0.04565303  1.0012546   8.228881   26.078255  ]\n",
      "Training loss [ 0.05680789  0.9814695   8.175355   25.614178  ]\n",
      "Training loss [ 0.05126785  0.990433    7.5096273  24.326603  ]\n",
      "Training loss [ 0.04252668  0.98445606  7.420189   25.928986  ]\n",
      "Training loss [ 0.04127374  0.98958963  7.504561   26.174112  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.03591147  1.0012546   7.4858656  26.078255  ]\n",
      "Training loss [ 0.04834878  0.9814695   7.4221997  25.614178  ]\n",
      "Training loss [ 0.04477074  0.990433    7.0860105  24.326603  ]\n",
      "Training loss [ 0.03826374  0.98445606  7.1858053  25.928986  ]\n",
      "Training loss [ 0.0373719   0.98958963  7.2745404  26.174112  ]\n",
      "Training loss [ 0.03032978  1.0012546   7.455637   26.078255  ]\n",
      "Training loss [ 0.04330949  0.9814695   7.4152813  25.614178  ]\n",
      "Training loss [ 0.03926839  0.990433    7.0198755  24.326603  ]\n",
      "Training loss [ 0.03647136  0.98445606  7.183324   25.928986  ]\n",
      "Training loss [ 0.03451443  0.98958963  7.2707872  26.174112  ]\n",
      "Training loss [ 0.02766127  1.0012546   7.4192963  26.078255  ]\n",
      "Training loss [ 0.04038879  0.9814695   7.4119925  25.614178  ]\n",
      "Training loss [ 2.5616422  8.020569  22.273323  37.772713 ]\n",
      "Training loss [ 0.28935173  1.0094935   9.835878   24.31245   ]\n",
      "Training loss [ 0.16758762  1.0012815   8.852685   23.86753   ]\n",
      "Training loss [ 0.09712122  1.005192    8.910892   24.168245  ]\n",
      "Training loss [ 0.08793028  0.99760205  8.769035   24.412764  ]\n",
      "Training loss [ 0.07807814  1.0033073   8.587318   24.057041  ]\n",
      "Training loss [ 0.07038626  1.0094935   8.67689    24.31245   ]\n",
      "Training loss [ 0.06320286  1.0012815   8.507168   23.86753   ]\n",
      "Training loss [ 0.05709255  1.005192    8.716791   24.168245  ]\n",
      "Training loss [ 0.06143764  0.99760205  8.682497   24.412764  ]\n",
      "Training loss [ 0.05762048  1.0033073   8.514823   24.057041  ]\n",
      "Training loss [ 0.05001953  1.0094935   8.670603   24.31245   ]\n",
      "Training loss [ 0.05005716  1.0012815   8.283863   23.86753   ]\n",
      "Training loss [ 0.0499717  1.005192   8.485935  24.168245 ]\n",
      "Training loss [ 0.05332939  0.99760205  8.433815   24.412764  ]\n",
      "Training loss [ 0.0518246  1.0033073  8.223854  24.057041 ]\n",
      "Training loss [ 0.04668839  1.0094935   8.41479    24.31245   ]\n",
      "Training loss [ 0.05086944  1.0012815   8.190452   23.86753   ]\n",
      "Training loss [ 0.04747695  1.005192    8.381495   24.168245  ]\n",
      "Training loss [ 0.04969094  0.99760205  8.334576   24.412764  ]\n",
      "Training loss [ 0.05136322  1.0033073   8.1370945  24.057041  ]\n",
      "Training loss [ 0.04267567  1.0094935   8.302928   24.31245   ]\n",
      "Training loss [ 0.05082062  1.0012815   8.17812    23.86753   ]\n",
      "Training loss [ 0.04225534  1.005192    8.327814   24.168245  ]\n",
      "Training loss [ 0.04335681  0.99760205  8.298064   24.412764  ]\n",
      "Training loss [ 3.9321723  7.4085197 23.765533  41.176315 ]\n",
      "Training loss [ 0.29550445  1.0103867  10.268906   25.497215  ]\n",
      "Training loss [ 0.13219924  1.0100493   9.750446   25.54801   ]\n",
      "Training loss [ 0.07791622  1.0000168   8.790371   23.908504  ]\n",
      "Training loss [ 0.06501303  1.0111461   9.255566   25.634357  ]\n",
      "Training loss [ 0.06026519  1.012738    8.984289   25.227324  ]\n",
      "Training loss [ 0.05175892  1.0103867   9.028166   25.497215  ]\n",
      "Training loss [ 0.04349001  1.0100493   9.112217   25.54801   ]\n",
      "Training loss [ 0.04422363  1.0000168   8.298204   23.908504  ]\n",
      "Training loss [ 0.04620777  1.0111461   8.815086   25.634357  ]\n",
      "Training loss [ 0.04638129  1.012738    8.547251   25.227324  ]\n",
      "Training loss [ 0.03873402  1.0103867   8.580906   25.497215  ]\n",
      "Training loss [ 0.04423933  1.0100493   8.78681    25.54801   ]\n",
      "Training loss [ 0.03528586  1.0000168   8.0841255  23.908504  ]\n",
      "Training loss [ 0.03869426  1.0111461   8.616121   25.634357  ]\n",
      "Training loss [ 0.04095142  1.012738    8.302634   25.227324  ]\n",
      "Training loss [ 0.03736114  1.0103867   8.459708   25.497215  ]\n",
      "Training loss [ 0.03202678  1.0100493   8.621174   25.54801   ]\n",
      "Training loss [ 0.0310631  1.0000168  7.9399385 23.908504 ]\n",
      "Training loss [ 0.03684535  1.0111461   8.517323   25.634357  ]\n",
      "Training loss [ 0.03827541  1.012738    8.2630005  25.227324  ]\n",
      "Training loss [ 0.03574243  1.0103867   8.431358   25.497215  ]\n",
      "Training loss [ 0.02868827  1.0100493   8.475681   25.54801   ]\n",
      "Training loss [ 0.02954426  1.0000168   7.7963657  23.908504  ]\n",
      "Training loss [ 0.03750189  1.0111461   8.356537   25.634357  ]\n",
      "Training loss [ 2.5216222  6.982611  22.83236   36.942253 ]\n",
      "Training loss [ 0.28284112  1.0134139  10.376221   25.277403  ]\n",
      "Training loss [ 0.10753231  1.0068442   9.451377   25.79824   ]\n",
      "Training loss [ 0.07370098  1.0124241   9.194654   26.19834   ]\n",
      "Training loss [ 0.07315774  1.017988    9.074939   25.97287   ]\n",
      "Training loss [ 0.07094238  1.0157      8.394623   24.633713  ]\n",
      "Training loss [ 0.06040248  1.0134139   8.5238495  25.277403  ]\n",
      "Training loss [ 0.03918616  1.0067565   8.692584   25.79824   ]\n",
      "Training loss [ 0.05497304  1.0124241   8.786329   26.19834   ]\n",
      "Training loss [ 0.05446766  1.017988    8.753527   25.97287   ]\n",
      "Training loss [ 0.05240411  1.0157      8.19491    24.633713  ]\n",
      "Training loss [ 0.0453168  1.0134139  8.356277  25.277403 ]\n",
      "Training loss [ 0.0310949  1.0067565  8.596762  25.79824  ]\n",
      "Training loss [ 0.03992277  1.0124241   8.618483   26.19834   ]\n",
      "Training loss [ 0.04516529  1.017988    8.61212    25.97287   ]\n",
      "Training loss [ 0.04353735  1.0157      8.085403   24.633713  ]\n",
      "Training loss [ 0.03847949  1.0134139   8.207026   25.277403  ]\n",
      "Training loss [ 0.02772474  1.0067565   8.518049   25.79824   ]\n",
      "Training loss [ 0.03958393  1.0124241   8.534573   26.19834   ]\n",
      "Training loss [ 0.035609  1.017988  8.419864 25.97287 ]\n",
      "Training loss [ 0.04082158  1.0157      7.908347   24.633713  ]\n",
      "Training loss [ 0.03487125  1.0134139   8.064365   25.277403  ]\n",
      "Training loss [ 0.02687517  1.0067565   8.274514   25.79824   ]\n",
      "Training loss [ 0.03505332  1.0124241   8.431519   26.19834   ]\n",
      "Training loss [ 0.03416449  1.017988    8.40358    25.97287   ]\n",
      "Training loss [ 2.0375998  7.5533667 24.930172  37.914944 ]\n",
      "Training loss [ 0.3055938  1.0187051 11.113518  25.981337 ]\n",
      "Training loss [ 0.16235366  1.0187786  10.318754   26.253216  ]\n",
      "Training loss [ 0.09727395  1.0234404  10.211428   26.586828  ]\n",
      "Training loss [ 0.06937543  1.021018    9.35747    25.063179  ]\n",
      "Training loss [ 0.05869752  1.0112512   9.485224   26.097092  ]\n",
      "Training loss [ 0.06646989  1.0186689   9.334967   25.981337  ]\n",
      "Training loss [ 0.07410447  1.0187786   9.4522505  26.253216  ]\n",
      "Training loss [ 0.05676351  1.0226499   9.6336975  26.586828  ]\n",
      "Training loss [ 0.05667338  1.021018    8.857304   25.063179  ]\n",
      "Training loss [ 0.04725397  1.0112512   9.048921   26.097092  ]\n",
      "Training loss [ 0.05261539  1.0186689   9.163208   25.981337  ]\n",
      "Training loss [ 0.05859712  1.0187786   9.190558   26.253216  ]\n",
      "Training loss [ 0.04150277  1.0226499   9.18119    26.586828  ]\n",
      "Training loss [ 0.04729995  1.021018    8.558584   25.063179  ]\n",
      "Training loss [ 0.04050699  1.0112512   8.60696    26.097092  ]\n",
      "Training loss [ 0.04669766  1.0186689   8.649902   25.981337  ]\n",
      "Training loss [ 0.0523125  1.0187786  8.683729  26.253216 ]\n",
      "Training loss [ 0.03654475  1.0226499   8.95311    26.586828  ]\n",
      "Training loss [ 0.03608542  1.021018    8.412322   25.063179  ]\n",
      "Training loss [ 0.03732028  1.0112512   8.400742   26.097092  ]\n",
      "Training loss [ 0.03762859  1.0186689   8.487336   25.981337  ]\n",
      "Training loss [ 0.04326296  1.0187786   8.50451    26.253216  ]\n",
      "Training loss [ 0.03226124  1.0226499   8.784951   26.586828  ]\n",
      "Training loss [ 0.03088877  1.021018    8.161879   25.063179  ]\n",
      "Training loss [ 2.1508775  7.0089545 24.142937  34.60321  ]\n",
      "Training loss [ 0.31108764  1.0110195  10.672599   25.868454  ]\n",
      "Training loss [ 0.17382154  1.009187    9.623981   25.959433  ]\n",
      "Training loss [ 0.10254572  1.0078597  10.3009815  26.144644  ]\n",
      "Training loss [ 0.08795495  1.0137262   9.284195   24.85011   ]\n",
      "Training loss [ 0.06943335  1.0114232   9.164583   25.37561   ]\n",
      "Training loss [ 0.07519957  1.0110195   9.336623   25.868454  ]\n",
      "Training loss [ 0.06862412  1.009187    8.921242   25.959433  ]\n",
      "Training loss [ 0.05771317  1.0078597   9.173368   26.144644  ]\n",
      "Training loss [ 0.05650345  1.0137262   8.747589   24.85011   ]\n",
      "Training loss [ 0.05213834  1.0114232   8.602798   25.37561   ]\n",
      "Training loss [ 0.05082693  1.0110195   9.068698   25.868454  ]\n",
      "Training loss [ 0.05571899  1.009187    8.662078   25.959433  ]\n",
      "Training loss [ 0.05105264  1.0078597   9.024407   26.144644  ]\n",
      "Training loss [ 0.05623422  1.0137262   8.5138     24.85011   ]\n",
      "Training loss [ 0.04053081  1.0114232   8.335748   25.37561   ]\n",
      "Training loss [ 0.04231284  1.0110195   8.758198   25.868454  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [ 0.04674738  1.009187    8.380521   25.959433  ]\n",
      "Training loss [ 0.04816177  1.0078597   8.685228   26.144644  ]\n",
      "Training loss [ 0.04267586  1.0137262   8.334868   24.85011   ]\n",
      "Training loss [ 0.03704662  1.0114232   8.259187   25.37561   ]\n",
      "Training loss [ 0.0359589  1.0110195  8.636614  25.868454 ]\n",
      "Training loss [ 0.04320012  1.009187    8.264136   25.959433  ]\n",
      "Training loss [ 0.04384694  1.0078597   8.357531   26.144644  ]\n",
      "Training loss [ 0.04053049  1.0137262   8.043539   24.85011   ]\n",
      "Training loss [ 2.3433623  5.141082  24.340721  35.482353 ]\n",
      "Training loss [ 0.30004382  1.0049131  11.085517   25.700306  ]\n",
      "Training loss [ 0.16175348  1.012827   10.832758   27.166086  ]\n",
      "Training loss [ 0.12214571  1.010024   10.166418   25.6206    ]\n",
      "Training loss [ 0.09081975  1.0106605   9.874683   26.030334  ]\n",
      "Training loss [ 0.0947282  1.0021025  9.300225  25.483341 ]\n",
      "Training loss [ 0.08050631  1.0049131   9.374685   25.700306  ]\n",
      "Training loss [ 0.07426997  1.012827    9.727388   27.166086  ]\n",
      "Training loss [ 0.07464819  1.010024    9.276812   25.6206    ]\n",
      "Training loss [ 0.06080437  1.0106605   9.161309   26.030334  ]\n",
      "Training loss [ 0.06334956  1.0021025   8.664396   25.483341  ]\n",
      "Training loss [ 0.05240988  1.0049131   8.7265215  25.700306  ]\n",
      "Training loss [ 0.05078406  1.012827    9.203363   27.166086  ]\n",
      "Training loss [ 0.06511427  1.010024    8.749086   25.6206    ]\n",
      "Training loss [ 0.05487259  1.0106605   8.6264515  26.030334  ]\n",
      "Training loss [ 0.05119648  1.0021025   8.188396   25.483341  ]\n",
      "Training loss [ 0.04520209  1.0049131   8.2823925  25.700306  ]\n",
      "Training loss [ 0.05881326  1.012827    8.676434   27.166086  ]\n",
      "Training loss [ 0.05317152  1.010024    8.4318495  25.6206    ]\n",
      "Training loss [ 0.0476051  1.0106605  8.356722  26.030334 ]\n",
      "Training loss [ 0.05811027  1.0021025   7.9976516  25.483341  ]\n",
      "Training loss [ 0.0406879  1.0049131  8.10481   25.700306 ]\n",
      "Training loss [ 0.04864043  1.012827    8.560265   27.166086  ]\n",
      "Training loss [ 0.05471879  1.010024    8.277758   25.6206    ]\n",
      "Training loss [ 0.04566858  1.0106605   8.2704735  26.030334  ]\n"
     ]
    }
   ],
   "source": [
    "ctrl_trials = [list(map(ctrl_l, n_ds)) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.39624792 0.39299905 0.41354263 0.45667845]\n",
      "Training loss [0.29225314 0.15257472 0.19177729 0.15185836]\n",
      "Training loss [0.3035823  0.1569348  0.20061174 0.15587634]\n",
      "Training loss [0.27971268 0.13298959 0.18961912 0.13248768]\n",
      "Training loss [0.31410784 0.15992683 0.22525463 0.1591801 ]\n",
      "Training loss [0.29055455 0.15115252 0.19195393 0.15039024]\n",
      "Training loss [0.28773803 0.14093198 0.18781978 0.14038062]\n",
      "Training loss [0.29993    0.1486406  0.20058587 0.14728692]\n",
      "Training loss [0.27513874 0.12440062 0.18938743 0.1241093 ]\n",
      "Training loss [0.30876225 0.15291455 0.22532159 0.15175161]\n",
      "Training loss [0.2859624  0.14343394 0.19196376 0.14254025]\n",
      "Training loss [0.2827394  0.13266392 0.18780512 0.13253772]\n",
      "Training loss [0.29428637 0.13979776 0.20060474 0.1382643 ]\n",
      "Training loss [0.26911327 0.11474641 0.18935695 0.11496279]\n",
      "Training loss [0.30158666 0.14386612 0.22532648 0.14317918]\n",
      "Training loss [0.27932316 0.13419497 0.19195059 0.13408506]\n",
      "Training loss [0.2765454  0.12275555 0.18780252 0.1237065 ]\n",
      "Training loss [0.2871459  0.13018483 0.2006236  0.12837797]\n",
      "Training loss [0.26182014 0.10449933 0.18932946 0.1048658 ]\n",
      "Training loss [0.29334652 0.13277334 0.22533217 0.13213609]\n",
      "Training loss [0.27183542 0.12322359 0.1919372  0.12296984]\n",
      "Training loss [0.2704425  0.11157069 0.1877996  0.11174702]\n",
      "Training loss [0.2798787  0.1193154  0.20064351 0.11390436]\n",
      "Training loss [0.25464678 0.09389018 0.18930306 0.09044559]\n",
      "Training loss [0.28592998 0.1178302  0.2253376  0.11030658]\n",
      "Training loss [0.48799747 0.5160587  0.5618337  0.4870485 ]\n",
      "Training loss [0.31766868 0.17377806 0.27150676 0.17484403]\n",
      "Training loss [0.33720484 0.18164247 0.2791239  0.18153682]\n",
      "Training loss [0.32105154 0.16966864 0.28009775 0.17014888]\n",
      "Training loss [0.32521257 0.1698954  0.26752454 0.17035162]\n",
      "Training loss [0.3317912  0.17810318 0.27571887 0.17647229]\n",
      "Training loss [0.30914932 0.16582395 0.26959604 0.16591127]\n",
      "Training loss [0.33050355 0.17389952 0.27913296 0.17339367]\n",
      "Training loss [0.31170607 0.16225398 0.27971935 0.16274434]\n",
      "Training loss [0.3166591  0.16098383 0.26733422 0.16130166]\n",
      "Training loss [0.32160836 0.16933104 0.27551723 0.16759121]\n",
      "Training loss [0.2995693  0.15679328 0.269632   0.15712734]\n",
      "Training loss [0.32081026 0.16386661 0.27915326 0.16355297]\n",
      "Training loss [0.29866374 0.1521391  0.2797069  0.1537638 ]\n",
      "Training loss [0.30561858 0.15030836 0.2673277  0.1508548 ]\n",
      "Training loss [0.30928016 0.15797517 0.2754705  0.15702978]\n",
      "Training loss [0.28828657 0.14558177 0.26962876 0.14684182]\n",
      "Training loss [0.31097716 0.15220422 0.27915674 0.1520476 ]\n",
      "Training loss [0.286277   0.14022584 0.27969933 0.1435189 ]\n",
      "Training loss [0.29535022 0.13915205 0.267326   0.13976324]\n",
      "Training loss [0.2988395  0.14470477 0.27542734 0.14484128]\n",
      "Training loss [0.27812555 0.13289194 0.26962328 0.13564165]\n",
      "Training loss [0.30283213 0.13938043 0.27915856 0.14020775]\n",
      "Training loss [0.27632052 0.12832743 0.2796937  0.13292213]\n",
      "Training loss [0.28680202 0.12798347 0.26732594 0.12892777]\n",
      "Training loss [0.42968106 0.5203032  0.50157875 0.5204915 ]\n",
      "Training loss [0.2904028  0.13820295 0.18677342 0.1394237 ]\n",
      "Training loss [0.30478656 0.14007396 0.21165852 0.14062746]\n",
      "Training loss [0.29253355 0.13215041 0.20389673 0.13282846]\n",
      "Training loss [0.29782832 0.13097179 0.21285048 0.13152811]\n",
      "Training loss [0.30195937 0.13441795 0.23478702 0.13647859]\n",
      "Training loss [0.27788275 0.12140517 0.18333419 0.12283338]\n",
      "Training loss [0.29759675 0.12614825 0.21173759 0.12958167]\n",
      "Training loss [0.28803548 0.11723778 0.20393673 0.12019385]\n",
      "Training loss [0.29364634 0.11722612 0.2128545  0.12023261]\n",
      "Training loss [0.29728538 0.12053419 0.23477477 0.12398972]\n",
      "Training loss [0.2730435  0.10628042 0.1833231  0.10815895]\n",
      "Training loss [0.2915557  0.11102583 0.21173382 0.11476079]\n",
      "Training loss [0.28121948 0.10210317 0.20392205 0.10487184]\n",
      "Training loss [0.28781644 0.10546558 0.21286115 0.10716583]\n",
      "Training loss [0.29032508 0.11010144 0.23475523 0.11173508]\n",
      "Training loss [0.2663906  0.09512194 0.18332091 0.09611018]\n",
      "Training loss [0.28371784 0.10373853 0.2117292  0.10555892]\n",
      "Training loss [0.27226382 0.0933339  0.20390701 0.09566656]\n",
      "Training loss [0.2809023  0.09790194 0.21286738 0.0990508 ]\n",
      "Training loss [0.2824911  0.10318621 0.23473668 0.10444909]\n",
      "Training loss [0.2596168  0.08772011 0.18332025 0.08863334]\n",
      "Training loss [0.2763254  0.09874009 0.21172456 0.1004756 ]\n",
      "Training loss [0.2636179  0.08720869 0.2038911  0.08945692]\n",
      "Training loss [0.27435616 0.09173958 0.21287203 0.09274566]\n",
      "Training loss [0.40355444 0.41438216 0.4911958  0.45126465]\n",
      "Training loss [0.2831646  0.16294014 0.19415757 0.16302414]\n",
      "Training loss [0.26294035 0.14799945 0.19212979 0.14826813]\n",
      "Training loss [0.23883203 0.13506718 0.16918036 0.13595119]\n",
      "Training loss [0.2556958  0.13819814 0.17328404 0.13919292]\n",
      "Training loss [0.26448923 0.13790992 0.17532273 0.13902116]\n",
      "Training loss [0.2680776  0.14648831 0.1941472  0.14997412]\n",
      "Training loss [0.25171626 0.13766912 0.19206375 0.1388288 ]\n",
      "Training loss [0.22859639 0.12285247 0.16916788 0.1258965 ]\n",
      "Training loss [0.2455611  0.12633535 0.17326933 0.12873954]\n",
      "Training loss [0.2507824  0.12536936 0.17529163 0.12822984]\n",
      "Training loss [0.25633362 0.13391584 0.19410484 0.1396393 ]\n",
      "Training loss [0.24048579 0.12639606 0.19202831 0.12720534]\n",
      "Training loss [0.21701503 0.11020867 0.16915536 0.11307256]\n",
      "Training loss [0.23499668 0.11398526 0.1732555  0.11705016]\n",
      "Training loss [0.23803195 0.11283281 0.17526202 0.11591653]\n",
      "Training loss [0.24511349 0.12217624 0.19406292 0.12823868]\n",
      "Training loss [0.23092861 0.1162778  0.19199406 0.11631221]\n",
      "Training loss [0.20774746 0.09882286 0.16914171 0.10102335]\n",
      "Training loss [0.22593504 0.10194227 0.1732429  0.10664146]\n",
      "Training loss [0.2288259  0.09991303 0.17523414 0.10470086]\n",
      "Training loss [0.23633519 0.10901183 0.1940228  0.11803231]\n",
      "Training loss [0.22297943 0.10515047 0.19195934 0.10760705]\n",
      "Training loss [0.2006356  0.08683988 0.16912672 0.09179404]\n",
      "Training loss [0.21873719 0.09039146 0.17323092 0.09808883]\n",
      "Training loss [0.41424263 0.46095666 0.46982604 0.4441865 ]\n",
      "Training loss [0.25099126 0.14713392 0.17990398 0.14371648]\n",
      "Training loss [0.23003095 0.11808904 0.15479618 0.11706606]\n",
      "Training loss [0.2410229  0.13081068 0.17727262 0.13022086]\n",
      "Training loss [0.2223793  0.11198036 0.15844291 0.11246769]\n",
      "Training loss [0.25288078 0.11725719 0.1690979  0.11787222]\n",
      "Training loss [0.23188804 0.12460553 0.17978156 0.12455092]\n",
      "Training loss [0.21700704 0.10805131 0.1547837  0.10716935]\n",
      "Training loss [0.2285999  0.12082637 0.17727631 0.12103187]\n",
      "Training loss [0.20731296 0.10103947 0.1584285  0.10301094]\n",
      "Training loss [0.23656736 0.10743935 0.1690751  0.10897852]\n",
      "Training loss [0.21699962 0.11496028 0.17976186 0.11644313]\n",
      "Training loss [0.20140843 0.09903136 0.1547752  0.09921202]\n",
      "Training loss [0.21378528 0.11062957 0.17728062 0.11186796]\n",
      "Training loss [0.1907249  0.08993551 0.15841496 0.09352393]\n",
      "Training loss [0.21895625 0.09745592 0.1690568  0.09869105]\n",
      "Training loss [0.20383711 0.10542338 0.17974193 0.1088967 ]\n",
      "Training loss [0.18891966 0.08937417 0.15476795 0.09067047]\n",
      "Training loss [0.20283805 0.10102267 0.1772837  0.1025612 ]\n",
      "Training loss [0.17887805 0.08149394 0.15840267 0.08570679]\n",
      "Training loss [0.2063967  0.0895365  0.16904223 0.09041947]\n",
      "Training loss [0.19480756 0.09701694 0.17972258 0.10143141]\n",
      "Training loss [0.181037   0.08147773 0.15476021 0.08331446]\n",
      "Training loss [0.19601569 0.09312032 0.17728874 0.09491268]\n",
      "Training loss [0.17153308 0.07460721 0.15839079 0.0790021 ]\n",
      "Training loss [0.41838145 0.44035485 0.49403593 0.43237817]\n",
      "Training loss [0.25824705 0.15675658 0.16816461 0.15591225]\n",
      "Training loss [0.26242793 0.14165133 0.17696673 0.14202926]\n",
      "Training loss [0.23986301 0.13810037 0.17237347 0.13710743]\n",
      "Training loss [0.24402443 0.12997323 0.17408767 0.12920083]\n",
      "Training loss [0.2344074  0.12850606 0.16388078 0.12740381]\n",
      "Training loss [0.23504035 0.13250166 0.16768155 0.13307889]\n",
      "Training loss [0.24679792 0.1294561  0.17694016 0.12969057]\n",
      "Training loss [0.22637841 0.12961534 0.1723511  0.12764835]\n",
      "Training loss [0.22893286 0.1221489  0.17407563 0.12102547]\n",
      "Training loss [0.21679181 0.12067321 0.16386038 0.11960297]\n",
      "Training loss [0.22220117 0.12340648 0.16766182 0.12400398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.23350385 0.1210656  0.17692116 0.12101795]\n",
      "Training loss [0.21382086 0.12202009 0.17233232 0.11950815]\n",
      "Training loss [0.21494438 0.11549276 0.17406525 0.11435272]\n",
      "Training loss [0.20106763 0.11414764 0.16384241 0.11311425]\n",
      "Training loss [0.21205324 0.11604492 0.16764666 0.11666236]\n",
      "Training loss [0.22268775 0.1144262  0.17690715 0.11451594]\n",
      "Training loss [0.20389423 0.11538026 0.172317   0.113232  ]\n",
      "Training loss [0.20510325 0.11007094 0.1740548  0.10878939]\n",
      "Training loss [0.19026741 0.10864146 0.16382784 0.10702699]\n",
      "Training loss [0.20460172 0.11006179 0.16763401 0.11013786]\n",
      "Training loss [0.21399802 0.10894324 0.17689535 0.10895479]\n",
      "Training loss [0.19631377 0.1094737  0.17230329 0.10759792]\n",
      "Training loss [0.19821198 0.10514271 0.17404342 0.10385436]\n",
      "Training loss [0.44826883 0.4754961  0.51922774 0.46828344]\n",
      "Training loss [0.26462922 0.16720086 0.19939014 0.1635739 ]\n",
      "Training loss [0.25609002 0.15210432 0.20546877 0.15106195]\n",
      "Training loss [0.25773615 0.15942574 0.21713828 0.15894864]\n",
      "Training loss [0.24633244 0.14491181 0.21328309 0.14583695]\n",
      "Training loss [0.24459691 0.14249966 0.205091   0.14239483]\n",
      "Training loss [0.23599535 0.13551281 0.19957618 0.13277058]\n",
      "Training loss [0.24131885 0.1427029  0.2054803  0.1415658 ]\n",
      "Training loss [0.24605003 0.15074182 0.21710417 0.15109847]\n",
      "Training loss [0.23525769 0.13851419 0.21324942 0.1395118 ]\n",
      "Training loss [0.23221669 0.13556197 0.2050435  0.13519016]\n",
      "Training loss [0.22224489 0.12829679 0.19954485 0.12527551]\n",
      "Training loss [0.22890204 0.13598593 0.20546047 0.13504623]\n",
      "Training loss [0.23531067 0.14297558 0.21707594 0.14381479]\n",
      "Training loss [0.22560957 0.13171044 0.2132238  0.13319473]\n",
      "Training loss [0.22225928 0.12838781 0.20500433 0.12861304]\n",
      "Training loss [0.21115991 0.12127403 0.19951536 0.11960942]\n",
      "Training loss [0.21965495 0.12887844 0.2054446  0.12977788]\n",
      "Training loss [0.22715151 0.13606475 0.21705262 0.1377255 ]\n",
      "Training loss [0.21893418 0.12433033 0.2131994  0.1280538 ]\n",
      "Training loss [0.21544561 0.12226969 0.2049695  0.12315436]\n",
      "Training loss [0.20406598 0.1145306  0.19948708 0.11540776]\n",
      "Training loss [0.21351314 0.12233977 0.20543113 0.124944  ]\n",
      "Training loss [0.22130525 0.12930454 0.21703276 0.13290232]\n",
      "Training loss [0.21424022 0.11844241 0.21317758 0.12336879]\n",
      "Training loss [0.43115383 0.45240164 0.5081881  0.45508906]\n",
      "Training loss [0.25490755 0.16375114 0.19021377 0.1637968 ]\n",
      "Training loss [0.23103313 0.14063188 0.1852942  0.13866445]\n",
      "Training loss [0.22973065 0.13209376 0.19046599 0.13306755]\n",
      "Training loss [0.22742817 0.11798243 0.17857732 0.1181277 ]\n",
      "Training loss [0.22271597 0.1254056  0.18695684 0.12604235]\n",
      "Training loss [0.2225962  0.12257902 0.19032194 0.12494251]\n",
      "Training loss [0.21396315 0.12619285 0.18526243 0.1272504 ]\n",
      "Training loss [0.2156465  0.12426575 0.19043075 0.12543128]\n",
      "Training loss [0.2142896  0.11066218 0.17853233 0.11184056]\n",
      "Training loss [0.21120611 0.11844628 0.18690735 0.11928946]\n",
      "Training loss [0.21052107 0.11560474 0.1902917  0.11808172]\n",
      "Training loss [0.20220199 0.11855298 0.18523061 0.12016317]\n",
      "Training loss [0.20337927 0.11722235 0.19040187 0.11799383]\n",
      "Training loss [0.20296976 0.1045349  0.17849575 0.10538116]\n",
      "Training loss [0.20151678 0.11193495 0.18685937 0.11185461]\n",
      "Training loss [0.20097682 0.10943199 0.19026801 0.11076953]\n",
      "Training loss [0.19284806 0.11171263 0.18520316 0.11262573]\n",
      "Training loss [0.19480331 0.11103587 0.19037896 0.11053473]\n",
      "Training loss [0.1949503  0.09924895 0.17846385 0.09929289]\n",
      "Training loss [0.19445531 0.10629711 0.18681832 0.10489833]\n",
      "Training loss [0.19451866 0.10407948 0.190249   0.10430197]\n",
      "Training loss [0.1862512  0.10615326 0.18517885 0.10617759]\n",
      "Training loss [0.18933499 0.10596527 0.19036087 0.10410503]\n",
      "Training loss [0.18951054 0.09474549 0.17843431 0.09404749]\n",
      "Training loss [0.44641203 0.4768585  0.52889466 0.4687339 ]\n",
      "Training loss [0.2539871  0.17033115 0.19558737 0.1687438 ]\n",
      "Training loss [0.2264546  0.13129005 0.18332982 0.13096465]\n",
      "Training loss [0.22127624 0.12597744 0.18633765 0.1258638 ]\n",
      "Training loss [0.21436007 0.12383667 0.18505529 0.12316772]\n",
      "Training loss [0.20454001 0.12003508 0.17862019 0.11936048]\n",
      "Training loss [0.22209695 0.12022008 0.19552906 0.11872517]\n",
      "Training loss [0.20760992 0.11378828 0.18330704 0.11343804]\n",
      "Training loss [0.20491378 0.11607808 0.18631506 0.11573108]\n",
      "Training loss [0.19853187 0.11754269 0.18497321 0.11549481]\n",
      "Training loss [0.19176525 0.11489478 0.17858991 0.11374686]\n",
      "Training loss [0.2108466  0.11537051 0.19550186 0.11352256]\n",
      "Training loss [0.1962784  0.10937598 0.18328556 0.10865294]\n",
      "Training loss [0.19433013 0.11137576 0.18630305 0.1103975 ]\n",
      "Training loss [0.1881548  0.11316905 0.18491659 0.11054879]\n",
      "Training loss [0.18327534 0.11081135 0.17856249 0.10881865]\n",
      "Training loss [0.20282646 0.11105546 0.1954901  0.10911793]\n",
      "Training loss [0.18850875 0.10549607 0.1832622  0.10453165]\n",
      "Training loss [0.18735485 0.10701212 0.18629926 0.10578765]\n",
      "Training loss [0.18164735 0.10906003 0.18487069 0.10632557]\n",
      "Training loss [0.17730415 0.10708659 0.17853676 0.104367  ]\n",
      "Training loss [0.19714805 0.10739089 0.19548234 0.10539921]\n",
      "Training loss [0.18266353 0.10209583 0.18323915 0.10112801]\n",
      "Training loss [0.18229643 0.10329016 0.1862949  0.10202008]\n",
      "Training loss [0.17673638 0.10554988 0.18483105 0.10303281]\n",
      "Training loss [0.45064214 0.44412088 0.5010351  0.4855231 ]\n",
      "Training loss [0.25505292 0.17043124 0.18185672 0.17154601]\n",
      "Training loss [0.21642321 0.13279998 0.17479457 0.13331331]\n",
      "Training loss [0.2197949  0.12834418 0.18234831 0.12705253]\n",
      "Training loss [0.20589155 0.12051433 0.17704356 0.12122621]\n",
      "Training loss [0.20493087 0.11937596 0.18168724 0.11962509]\n",
      "Training loss [0.21133123 0.11930303 0.18176433 0.118411  ]\n",
      "Training loss [0.19557387 0.11107846 0.17475846 0.11182319]\n",
      "Training loss [0.19905782 0.11567615 0.18229878 0.11504235]\n",
      "Training loss [0.18985343 0.11180772 0.17696056 0.11280423]\n",
      "Training loss [0.1898911  0.11308847 0.18164322 0.11375252]\n",
      "Training loss [0.1967546  0.11373881 0.18169683 0.11354322]\n",
      "Training loss [0.1848259  0.10706557 0.17472683 0.10810763]\n",
      "Training loss [0.18711932 0.11174108 0.18226081 0.11146861]\n",
      "Training loss [0.1809277  0.10840873 0.1769098  0.10951481]\n",
      "Training loss [0.18051024 0.10962917 0.18160366 0.11035325]\n",
      "Training loss [0.18788038 0.11037139 0.1816458  0.11044705]\n",
      "Training loss [0.17803386 0.10405453 0.1746967  0.10509889]\n",
      "Training loss [0.18019633 0.10869054 0.18223253 0.10847407]\n",
      "Training loss [0.17501023 0.10569049 0.17687353 0.10670704]\n",
      "Training loss [0.17444462 0.10664737 0.1815739  0.10714263]\n",
      "Training loss [0.18192716 0.10805442 0.18160607 0.10791478]\n",
      "Training loss [0.17346385 0.10145188 0.1746684  0.1024944 ]\n",
      "Training loss [0.1757789  0.10633241 0.182208   0.10598037]\n",
      "Training loss [0.17080384 0.10331211 0.17684586 0.10441069]\n",
      "Training loss [0.4304877  0.4649737  0.49286118 0.4661761 ]\n",
      "Training loss [0.23934999 0.17524081 0.19260618 0.17744064]\n",
      "Training loss [0.2270981  0.1464408  0.19099489 0.14719936]\n",
      "Training loss [0.20044267 0.1278789  0.18421254 0.12809062]\n",
      "Training loss [0.20948341 0.12446271 0.18556534 0.12330221]\n",
      "Training loss [0.20581876 0.11918002 0.18398696 0.118742  ]\n",
      "Training loss [0.19795743 0.11927707 0.19256218 0.11936065]\n",
      "Training loss [0.20159245 0.11686675 0.19092819 0.11641065]\n",
      "Training loss [0.18187526 0.1115385  0.18417066 0.11156169]\n",
      "Training loss [0.19305372 0.11417866 0.18551211 0.11353852]\n",
      "Training loss [0.19051555 0.1110513  0.18392031 0.11061931]\n",
      "Training loss [0.18582246 0.11407067 0.1925322  0.11377144]\n",
      "Training loss [0.18971029 0.11265329 0.19087847 0.11155129]\n",
      "Training loss [0.1727647  0.10837643 0.18413794 0.10757796]\n",
      "Training loss [0.18354052 0.11126635 0.18547873 0.11069798]\n",
      "Training loss [0.18207534 0.10849059 0.18387479 0.10767756]\n",
      "Training loss [0.17883863 0.11192657 0.19251198 0.11119924]\n",
      "Training loss [0.18232569 0.11041668 0.19083919 0.10923699]\n",
      "Training loss [0.16744047 0.10644437 0.18410975 0.10511983]\n",
      "Training loss [0.17773038 0.10922243 0.18545306 0.10849972]\n",
      "Training loss [0.17666483 0.1065954  0.18384156 0.10548228]\n",
      "Training loss [0.17420016 0.11017632 0.19249664 0.10928464]\n",
      "Training loss [0.1775143  0.1085216  0.19080544 0.10761659]\n",
      "Training loss [0.16411832 0.10469835 0.18408257 0.10324978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.1737164  0.10751919 0.18543035 0.10672575]\n",
      "Training loss [0.44559088 0.47446835 0.52186346 0.47001058]\n",
      "Training loss [0.25234646 0.18801528 0.19795166 0.18673849]\n",
      "Training loss [0.22932196 0.15814061 0.19782652 0.15906832]\n",
      "Training loss [0.20789583 0.14150304 0.19369045 0.14175332]\n",
      "Training loss [0.21253529 0.13834822 0.199845   0.13856998]\n",
      "Training loss [0.19885126 0.13301533 0.19266334 0.1323967 ]\n",
      "Training loss [0.21238782 0.12811597 0.19770786 0.12928568]\n",
      "Training loss [0.20652997 0.12385915 0.19770207 0.1240244 ]\n",
      "Training loss [0.1911211  0.12223332 0.19364262 0.12176795]\n",
      "Training loss [0.197534   0.12511498 0.19976844 0.12455346]\n",
      "Training loss [0.18656439 0.12263738 0.19256102 0.12237663]\n",
      "Training loss [0.2005501  0.12119579 0.19763365 0.12158191]\n",
      "Training loss [0.19720611 0.11760509 0.19762821 0.11806323]\n",
      "Training loss [0.18345343 0.11851504 0.1936127  0.11778093]\n",
      "Training loss [0.18908206 0.12162731 0.19972087 0.12107144]\n",
      "Training loss [0.17992511 0.11955364 0.19248855 0.1196259 ]\n",
      "Training loss [0.19410053 0.11856012 0.19758433 0.11881745]\n",
      "Training loss [0.19115014 0.11512504 0.19757342 0.11579315]\n",
      "Training loss [0.17865658 0.11677672 0.19358714 0.11602395]\n",
      "Training loss [0.18363369 0.11980605 0.19968262 0.11935923]\n",
      "Training loss [0.17569381 0.11764137 0.1924285  0.11812322]\n",
      "Training loss [0.19011194 0.11693013 0.19754398 0.11729794]\n",
      "Training loss [0.18672106 0.11354737 0.19752684 0.11419759]\n",
      "Training loss [0.17518374 0.11560743 0.1935636  0.11468033]\n",
      "Training loss [0.17998919 0.11844306 0.19964877 0.11811122]\n",
      "Training loss [0.43173596 0.46297497 0.49753308 0.45918417]\n",
      "Training loss [0.23425883 0.1859352  0.19135286 0.18389967]\n",
      "Training loss [0.204667   0.1497964  0.18714294 0.1496069 ]\n",
      "Training loss [0.20045412 0.13602117 0.18987472 0.13615245]\n",
      "Training loss [0.20460275 0.1362533  0.19390489 0.13543592]\n",
      "Training loss [0.19071555 0.12473189 0.18738888 0.12508675]\n",
      "Training loss [0.19577672 0.12533826 0.19122949 0.12499379]\n",
      "Training loss [0.18326303 0.11517057 0.18704405 0.11480072]\n",
      "Training loss [0.18319197 0.11348495 0.18973705 0.11346796]\n",
      "Training loss [0.18994316 0.12162381 0.19385032 0.11991623]\n",
      "Training loss [0.17805488 0.11388228 0.18730584 0.1141119 ]\n",
      "Training loss [0.18483222 0.11578138 0.19118813 0.11596587]\n",
      "Training loss [0.17514989 0.10865878 0.18700485 0.10882321]\n",
      "Training loss [0.17491631 0.10860374 0.18966785 0.10855177]\n",
      "Training loss [0.18235475 0.11766725 0.19380991 0.11654303]\n",
      "Training loss [0.17075175 0.11119562 0.18725437 0.11152947]\n",
      "Training loss [0.17831299 0.11286789 0.19115877 0.11338729]\n",
      "Training loss [0.17043921 0.10632966 0.18697974 0.10694847]\n",
      "Training loss [0.16964418 0.10684124 0.1896249  0.10676853]\n",
      "Training loss [0.17737967 0.1158866  0.1937801  0.1152182 ]\n",
      "Training loss [0.1657199  0.11010821 0.18721303 0.11025859]\n",
      "Training loss [0.17383897 0.11164123 0.19113337 0.11202218]\n",
      "Training loss [0.16687587 0.10525292 0.18695995 0.10589489]\n",
      "Training loss [0.16580813 0.10595014 0.18959446 0.10583762]\n",
      "Training loss [0.1737567  0.11484883 0.1937559  0.11441873]\n",
      "Training loss [0.44217667 0.46272296 0.49423784 0.45027992]\n",
      "Training loss [0.2352698  0.18585242 0.1931957  0.18830591]\n",
      "Training loss [0.21799374 0.16534826 0.19300662 0.16410255]\n",
      "Training loss [0.22350924 0.15545818 0.20290619 0.1538507 ]\n",
      "Training loss [0.2058534  0.14305465 0.19333741 0.14165056]\n",
      "Training loss [0.1946679  0.13530315 0.19020858 0.13335691]\n",
      "Training loss [0.19333273 0.13183361 0.19295457 0.13109025]\n",
      "Training loss [0.19425404 0.13064206 0.19291595 0.13026471]\n",
      "Training loss [0.20515758 0.13112381 0.2028361  0.13036567]\n",
      "Training loss [0.19066346 0.12767895 0.19323191 0.12671311]\n",
      "Training loss [0.18253113 0.12155825 0.19006126 0.12149313]\n",
      "Training loss [0.18122756 0.12307504 0.19288197 0.12276342]\n",
      "Training loss [0.18451613 0.12350713 0.19287004 0.12318838]\n",
      "Training loss [0.1964387  0.12542176 0.20280835 0.1251339 ]\n",
      "Training loss [0.18250203 0.1236245  0.19317366 0.12288795]\n",
      "Training loss [0.17584383 0.11761436 0.18997934 0.11837945]\n",
      "Training loss [0.17481059 0.12026627 0.19283766 0.12022118]\n",
      "Training loss [0.17885885 0.12134293 0.19283539 0.1210364 ]\n",
      "Training loss [0.19089776 0.12328156 0.20278296 0.12296863]\n",
      "Training loss [0.17754443 0.12178866 0.1931248  0.12113227]\n",
      "Training loss [0.17162347 0.11606155 0.18992417 0.11691938]\n",
      "Training loss [0.17088535 0.11900038 0.19280292 0.11887084]\n",
      "Training loss [0.175058   0.12026713 0.19280578 0.12005687]\n",
      "Training loss [0.18708953 0.12208447 0.20275426 0.12174679]\n",
      "Training loss [0.17421085 0.12064514 0.19307882 0.1201117 ]\n",
      "Training loss [0.43247253 0.45290697 0.47480184 0.45237327]\n",
      "Training loss [0.21885279 0.1769045  0.18550369 0.17595953]\n",
      "Training loss [0.20389354 0.16314289 0.1914731  0.16241829]\n",
      "Training loss [0.20082268 0.1512281  0.19265664 0.15171012]\n",
      "Training loss [0.19366963 0.14189273 0.18500546 0.14172572]\n",
      "Training loss [0.18645084 0.13439077 0.18533069 0.13538077]\n",
      "Training loss [0.18227725 0.13074884 0.18508708 0.12989767]\n",
      "Training loss [0.18015397 0.13079734 0.19140701 0.13043751]\n",
      "Training loss [0.18390277 0.12986024 0.19251429 0.1302365 ]\n",
      "Training loss [0.17993021 0.12681974 0.18492022 0.12650353]\n",
      "Training loss [0.173327   0.12221672 0.18522939 0.1224415 ]\n",
      "Training loss [0.17233196 0.12170261 0.18492502 0.12109771]\n",
      "Training loss [0.17229241 0.12407178 0.19139417 0.1240202 ]\n",
      "Training loss [0.17648116 0.1248188  0.19244243 0.12481979]\n",
      "Training loss [0.17340589 0.12285118 0.18489838 0.1224962 ]\n",
      "Training loss [0.16664809 0.11922583 0.1851733  0.11916441]\n",
      "Training loss [0.1671106  0.11891548 0.18483278 0.11826186]\n",
      "Training loss [0.16798602 0.1218034  0.19138539 0.12174028]\n",
      "Training loss [0.17206097 0.12292777 0.19239098 0.12263827]\n",
      "Training loss [0.16938856 0.12131255 0.18489301 0.12076885]\n",
      "Training loss [0.16243333 0.11802582 0.18512976 0.11789814]\n",
      "Training loss [0.16378978 0.11753473 0.18476927 0.11688388]\n",
      "Training loss [0.16515109 0.12068388 0.19137773 0.12061409]\n",
      "Training loss [0.16909507 0.12197936 0.19234852 0.12148628]\n",
      "Training loss [0.16652814 0.12046765 0.18489414 0.11979364]\n",
      "Training loss [0.4276331  0.44129974 0.46681792 0.4438368 ]\n",
      "Training loss [0.22049068 0.18925554 0.19577289 0.18761975]\n",
      "Training loss [0.20218745 0.16834581 0.1936817  0.16835459]\n",
      "Training loss [0.20165512 0.16033396 0.19521734 0.16155307]\n",
      "Training loss [0.20255479 0.15528157 0.19867788 0.15533662]\n",
      "Training loss [0.1945014  0.1480647  0.19314273 0.14849246]\n",
      "Training loss [0.18497357 0.14461055 0.1952397  0.14426488]\n",
      "Training loss [0.1810813  0.13872205 0.19343129 0.13892964]\n",
      "Training loss [0.18493547 0.13905096 0.19508436 0.13878107]\n",
      "Training loss [0.18968627 0.13811007 0.198557   0.13870879]\n",
      "Training loss [0.18328828 0.1353667  0.19304588 0.13543914]\n",
      "Training loss [0.1764661  0.13516994 0.19517988 0.1344355 ]\n",
      "Training loss [0.17362241 0.13191342 0.1933611  0.13148968]\n",
      "Training loss [0.17789945 0.1340858  0.19503774 0.13365395]\n",
      "Training loss [0.18283004 0.13367617 0.19850802 0.1341787 ]\n",
      "Training loss [0.17766514 0.1321955  0.19301125 0.13208607]\n",
      "Training loss [0.17164135 0.13230786 0.19516009 0.13146612]\n",
      "Training loss [0.16931374 0.1301014  0.19332193 0.12923181]\n",
      "Training loss [0.17357163 0.13200682 0.19499548 0.13183963]\n",
      "Training loss [0.17813177 0.13196318 0.1984767  0.13239066]\n",
      "Training loss [0.17410271 0.13087828 0.19299355 0.13086453]\n",
      "Training loss [0.16833289 0.13088533 0.1951479  0.13017601]\n",
      "Training loss [0.16647705 0.12919796 0.1932913  0.12825352]\n",
      "Training loss [0.17061326 0.13085976 0.19495405 0.13089882]\n",
      "Training loss [0.17483383 0.13111922 0.19845037 0.13138975]\n",
      "Training loss [0.42368525 0.4451264  0.45649403 0.44728798]\n",
      "Training loss [0.21495846 0.18176475 0.18664059 0.18159893]\n",
      "Training loss [0.20325604 0.16951647 0.19101307 0.1687762 ]\n",
      "Training loss [0.20187214 0.16531152 0.19457057 0.1650348 ]\n",
      "Training loss [0.18165192 0.14838481 0.18414794 0.14825317]\n",
      "Training loss [0.1864318  0.14895754 0.18994817 0.14850375]\n",
      "Training loss [0.180939   0.14241391 0.1863875  0.14201398]\n",
      "Training loss [0.17918888 0.13952121 0.19082636 0.13973059]\n",
      "Training loss [0.18574715 0.14450735 0.19440031 0.14375873]\n",
      "Training loss [0.17042491 0.13324058 0.18408753 0.13322133]\n",
      "Training loss [0.17517881 0.13557646 0.18989488 0.135759  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.1710386  0.13232677 0.18634714 0.13300717]\n",
      "Training loss [0.17016497 0.13206655 0.19078663 0.13214612]\n",
      "Training loss [0.1781398  0.13876121 0.19434309 0.13820244]\n",
      "Training loss [0.16441819 0.12911151 0.18405987 0.12891428]\n",
      "Training loss [0.16927013 0.13183844 0.18987592 0.1321024 ]\n",
      "Training loss [0.16580883 0.12928422 0.18632725 0.13009425]\n",
      "Training loss [0.16524018 0.12959686 0.19076645 0.12956548]\n",
      "Training loss [0.17357257 0.13681349 0.19430383 0.13615155]\n",
      "Training loss [0.16086113 0.12743878 0.18404023 0.12722704]\n",
      "Training loss [0.16520882 0.1303429  0.18986267 0.13042572]\n",
      "Training loss [0.16263546 0.12807244 0.1863123  0.12876284]\n",
      "Training loss [0.1620639  0.12845048 0.19074798 0.12839898]\n",
      "Training loss [0.17055218 0.13586481 0.19427311 0.1350652 ]\n",
      "Training loss [0.1584672  0.12653197 0.18402377 0.12632802]\n",
      "Training loss [0.4019551  0.41938496 0.43692684 0.42074326]\n",
      "Training loss [0.21491942 0.17723343 0.18458661 0.17773443]\n",
      "Training loss [0.1868389  0.16104375 0.18132976 0.16121279]\n",
      "Training loss [0.18329106 0.15233222 0.1760138  0.15238397]\n",
      "Training loss [0.18067986 0.15069899 0.18567032 0.15083408]\n",
      "Training loss [0.1794602  0.14692307 0.18354946 0.14711592]\n",
      "Training loss [0.1788919  0.14412922 0.1841683  0.14419675]\n",
      "Training loss [0.16734524 0.13825914 0.18116954 0.13865775]\n",
      "Training loss [0.16916892 0.13510343 0.17593825 0.13595554]\n",
      "Training loss [0.16767588 0.13802105 0.18555246 0.13759771]\n",
      "Training loss [0.16906396 0.13533019 0.18347226 0.13534863]\n",
      "Training loss [0.16876334 0.13541742 0.18413058 0.1353073 ]\n",
      "Training loss [0.159563   0.13103773 0.18114072 0.13111068]\n",
      "Training loss [0.16212177 0.12932356 0.17590603 0.1304708 ]\n",
      "Training loss [0.16208097 0.13352394 0.18550676 0.13326053]\n",
      "Training loss [0.16421323 0.13154137 0.18343134 0.13182625]\n",
      "Training loss [0.16353944 0.13234225 0.18411708 0.13224472]\n",
      "Training loss [0.15549657 0.12834929 0.18111667 0.12862322]\n",
      "Training loss [0.15783322 0.12728232 0.175881   0.12813526]\n",
      "Training loss [0.15901428 0.13164091 0.18547913 0.131396  ]\n",
      "Training loss [0.16117676 0.13013096 0.18339825 0.13030604]\n",
      "Training loss [0.16009109 0.13086434 0.18410252 0.13086185]\n",
      "Training loss [0.15293308 0.12709025 0.18108962 0.12748322]\n",
      "Training loss [0.15518326 0.12637073 0.17585781 0.12658367]\n",
      "Training loss [0.15700725 0.13067189 0.18545774 0.13039222]\n",
      "Training loss [0.40550375 0.4196431  0.42998165 0.4163763 ]\n",
      "Training loss [0.2156433  0.18353519 0.19292843 0.18341398]\n",
      "Training loss [0.20020907 0.17162603 0.18964444 0.17168806]\n",
      "Training loss [0.1817464  0.15649918 0.18772781 0.155894  ]\n",
      "Training loss [0.17995271 0.15031417 0.18580616 0.14953396]\n",
      "Training loss [0.18571615 0.15302306 0.18865076 0.15301272]\n",
      "Training loss [0.1823741  0.14961383 0.19268838 0.15052953]\n",
      "Training loss [0.17924644 0.14831725 0.1895089  0.14849442]\n",
      "Training loss [0.16567445 0.13958368 0.18764983 0.1398513 ]\n",
      "Training loss [0.16798294 0.13737875 0.18575501 0.13731776]\n",
      "Training loss [0.17515224 0.14206612 0.18861325 0.14289299]\n",
      "Training loss [0.17281586 0.1419435  0.1926327  0.14273699]\n",
      "Training loss [0.1711545  0.14232437 0.18948781 0.14182465]\n",
      "Training loss [0.15861738 0.13472623 0.18762563 0.13529141]\n",
      "Training loss [0.16265419 0.13347611 0.18572834 0.13368458]\n",
      "Training loss [0.1699984  0.13867196 0.18859345 0.13951564]\n",
      "Training loss [0.16739705 0.13943471 0.19257565 0.13964847]\n",
      "Training loss [0.16669661 0.14012039 0.18947488 0.13938187]\n",
      "Training loss [0.15493733 0.133009   0.18761188 0.13332966]\n",
      "Training loss [0.15945572 0.13175306 0.18570682 0.13223591]\n",
      "Training loss [0.16673966 0.13736637 0.18857566 0.13807572]\n",
      "Training loss [0.16434164 0.13810645 0.192524   0.13817483]\n",
      "Training loss [0.16382498 0.1390265  0.18946192 0.13842618]\n",
      "Training loss [0.1526629  0.13202035 0.18759987 0.13236895]\n",
      "Training loss [0.15702972 0.13084951 0.18568783 0.13134046]\n",
      "Training loss [0.3980362  0.4058373  0.43040806 0.40588537]\n",
      "Training loss [0.20913011 0.1779562  0.18620774 0.17749149]\n",
      "Training loss [0.19354321 0.1643087  0.18275799 0.16443606]\n",
      "Training loss [0.18631907 0.15705562 0.18636581 0.15733081]\n",
      "Training loss [0.18465951 0.15339494 0.18473206 0.15372942]\n",
      "Training loss [0.17910282 0.1481555  0.18261603 0.14867242]\n",
      "Training loss [0.17791334 0.14731462 0.1859444  0.14811984]\n",
      "Training loss [0.17280215 0.1429004  0.18260191 0.1433891 ]\n",
      "Training loss [0.17157722 0.14174469 0.18635719 0.14228263]\n",
      "Training loss [0.17058222 0.14124869 0.18471786 0.14180557]\n",
      "Training loss [0.16905169 0.13909346 0.18257974 0.13988037]\n",
      "Training loss [0.16890803 0.13972913 0.18591118 0.14046435]\n",
      "Training loss [0.16476513 0.13664304 0.18258154 0.13724543]\n",
      "Training loss [0.16489011 0.137366   0.18634908 0.13788804]\n",
      "Training loss [0.16413336 0.13726324 0.18471679 0.1378348 ]\n",
      "Training loss [0.16421376 0.13601103 0.18255167 0.13652855]\n",
      "Training loss [0.16449887 0.13704193 0.18587424 0.13747025]\n",
      "Training loss [0.1600318  0.13443278 0.18256715 0.13498798]\n",
      "Training loss [0.16098829 0.13562825 0.18633518 0.13597107]\n",
      "Training loss [0.16035403 0.13554734 0.18471289 0.13600928]\n",
      "Training loss [0.16139996 0.1344135  0.18252084 0.13491684]\n",
      "Training loss [0.16157296 0.13589406 0.1858363  0.13597283]\n",
      "Training loss [0.1577103  0.13328981 0.1825499  0.13376424]\n",
      "Training loss [0.15867725 0.13496828 0.18632111 0.13498265]\n",
      "Training loss [0.15789057 0.13482162 0.18470833 0.13511777]\n",
      "Training loss [0.39522982 0.4016225  0.41044235 0.4037108 ]\n",
      "Training loss [0.20878926 0.18066044 0.18835129 0.18042842]\n",
      "Training loss [0.20144281 0.16970052 0.19135898 0.17033316]\n",
      "Training loss [0.19049789 0.16143936 0.18627286 0.1611492 ]\n",
      "Training loss [0.17212869 0.15161684 0.18544081 0.15159258]\n",
      "Training loss [0.17707232 0.15163055 0.18438259 0.15152606]\n",
      "Training loss [0.1797799  0.15274295 0.18811621 0.15221469]\n",
      "Training loss [0.1797491  0.14980572 0.19129345 0.14963293]\n",
      "Training loss [0.17396832 0.14678904 0.18622664 0.14646487]\n",
      "Training loss [0.16168202 0.14104569 0.18541673 0.14075814]\n",
      "Training loss [0.1667154  0.14285767 0.18436038 0.14282122]\n",
      "Training loss [0.17121859 0.14519739 0.18809393 0.14527556]\n",
      "Training loss [0.17192075 0.14435995 0.19128703 0.14379798]\n",
      "Training loss [0.16711624 0.141967   0.18620974 0.1418196 ]\n",
      "Training loss [0.15612715 0.13709673 0.18538341 0.13710849]\n",
      "Training loss [0.16175444 0.13969305 0.1843368  0.13980757]\n",
      "Training loss [0.16656464 0.142086   0.18806532 0.14248922]\n",
      "Training loss [0.16788432 0.14229056 0.19128235 0.14155504]\n",
      "Training loss [0.16325541 0.14000417 0.18620056 0.1398347 ]\n",
      "Training loss [0.15301335 0.13537522 0.18535239 0.13539305]\n",
      "Training loss [0.15919761 0.13831471 0.18431433 0.13847134]\n",
      "Training loss [0.16387203 0.14104596 0.18803346 0.14130859]\n",
      "Training loss [0.1652904  0.141312   0.19127794 0.14067663]\n",
      "Training loss [0.16091925 0.13897586 0.18619533 0.13893801]\n",
      "Training loss [0.15094425 0.13442592 0.18532522 0.13463755]\n",
      "Training loss [0.40291163 0.4002918  0.41280305 0.38493073]\n",
      "Training loss [0.27580214 0.17939587 0.19132097 0.17881712]\n",
      "Training loss [0.27183    0.17687365 0.19659537 0.17631377]\n",
      "Training loss [0.26179528 0.18803184 0.18117107 0.18824983]\n",
      "Training loss [0.27701935 0.19018647 0.20405275 0.19052681]\n",
      "Training loss [0.26860553 0.18363273 0.18652745 0.18296269]\n",
      "Training loss [0.2703765  0.1649285  0.18461667 0.16625978]\n",
      "Training loss [0.26648995 0.16435581 0.19557267 0.16437984]\n",
      "Training loss [0.2567     0.17134781 0.18076551 0.17249063]\n",
      "Training loss [0.27380982 0.174739   0.20414495 0.17597044]\n",
      "Training loss [0.2628939  0.16828787 0.18666133 0.16777056]\n",
      "Training loss [0.26350132 0.14821637 0.18458422 0.15124476]\n",
      "Training loss [0.25969788 0.14902796 0.19548461 0.14963624]\n",
      "Training loss [0.24948809 0.15064794 0.18074313 0.15416183]\n",
      "Training loss [0.26926368 0.15740368 0.20412591 0.16014194]\n",
      "Training loss [0.25519776 0.14941603 0.18667874 0.1512116 ]\n",
      "Training loss [0.25493503 0.128009   0.18457782 0.13612449]\n",
      "Training loss [0.25244325 0.13178432 0.19547176 0.13542801]\n",
      "Training loss [0.24209772 0.12216606 0.18074803 0.13631937]\n",
      "Training loss [0.26498818 0.13070598 0.20410731 0.14058255]\n",
      "Training loss [0.2482535  0.11480211 0.18668455 0.12608603]\n",
      "Training loss [0.24769847 0.09939598 0.18457204 0.11213668]\n",
      "Training loss [0.2468171  0.10794611 0.19546877 0.11491563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.23616433 0.08929554 0.18075602 0.10148513]\n",
      "Training loss [0.26141813 0.10190915 0.20409045 0.1065042 ]\n",
      "Training loss [0.40840638 0.41822216 0.45981142 0.40571168]\n",
      "Training loss [0.29310918 0.18209048 0.2131322  0.18249744]\n",
      "Training loss [0.31188375 0.17851163 0.21786278 0.17791563]\n",
      "Training loss [0.29514286 0.16592422 0.19369473 0.16585684]\n",
      "Training loss [0.28850693 0.16901314 0.20130683 0.17080513]\n",
      "Training loss [0.30081907 0.17078406 0.21923018 0.1727411 ]\n",
      "Training loss [0.28614938 0.16733119 0.21291143 0.16965939]\n",
      "Training loss [0.3048933  0.1640906  0.21782848 0.16306347]\n",
      "Training loss [0.28592324 0.15145323 0.19368272 0.15095961]\n",
      "Training loss [0.27973646 0.1537427  0.20129088 0.15553468]\n",
      "Training loss [0.29214284 0.15308315 0.21920863 0.15501645]\n",
      "Training loss [0.27823138 0.15192613 0.21288015 0.1548301 ]\n",
      "Training loss [0.29442424 0.14735134 0.21782666 0.1462135 ]\n",
      "Training loss [0.27413967 0.13465348 0.19367102 0.13324153]\n",
      "Training loss [0.26901808 0.13689163 0.20127629 0.13983269]\n",
      "Training loss [0.28291315 0.1354961  0.2191841  0.13739224]\n",
      "Training loss [0.27000666 0.13396505 0.21284872 0.13779216]\n",
      "Training loss [0.28543463 0.131429   0.21782508 0.13238715]\n",
      "Training loss [0.26502043 0.1192342  0.19365999 0.11821613]\n",
      "Training loss [0.26029924 0.12241092 0.2012616  0.12539825]\n",
      "Training loss [0.27525032 0.12028591 0.21916011 0.12239369]\n",
      "Training loss [0.26296866 0.12020299 0.21281673 0.12279163]\n",
      "Training loss [0.27863294 0.1176724  0.21782352 0.11946797]\n",
      "Training loss [0.25859416 0.1044201  0.19364914 0.10465077]\n",
      "Training loss [0.2536409  0.10835085 0.2012461  0.11154359]\n",
      "Training loss [0.39816287 0.41292772 0.4726547  0.4450851 ]\n",
      "Training loss [0.24300458 0.14024448 0.16299975 0.14180946]\n",
      "Training loss [0.2524686  0.14275645 0.17375243 0.14245811]\n",
      "Training loss [0.2592907  0.15505819 0.18190943 0.1565    ]\n",
      "Training loss [0.24944319 0.15599261 0.16931745 0.15586576]\n",
      "Training loss [0.24367163 0.14743844 0.16468701 0.14884047]\n",
      "Training loss [0.23746632 0.12424724 0.16019881 0.12358655]\n",
      "Training loss [0.2443527  0.12601411 0.17361288 0.12512267]\n",
      "Training loss [0.25317156 0.13955212 0.18188617 0.13998091]\n",
      "Training loss [0.24278313 0.13645837 0.16931486 0.1372369 ]\n",
      "Training loss [0.23684186 0.12706086 0.16466509 0.12751785]\n",
      "Training loss [0.22901693 0.11074337 0.16017507 0.10844089]\n",
      "Training loss [0.23436636 0.10926821 0.17360763 0.10626067]\n",
      "Training loss [0.24609253 0.12189269 0.18188529 0.12070446]\n",
      "Training loss [0.23375517 0.12154613 0.16931845 0.11969131]\n",
      "Training loss [0.228276   0.11132009 0.16464426 0.11003622]\n",
      "Training loss [0.21930325 0.09880421 0.16015604 0.09487523]\n",
      "Training loss [0.22384487 0.0955082  0.17360368 0.09186716]\n",
      "Training loss [0.23982963 0.10840109 0.18188578 0.10773218]\n",
      "Training loss [0.2254098  0.10879719 0.1693223  0.10636261]\n",
      "Training loss [0.22011426 0.09836115 0.16462456 0.09690819]\n",
      "Training loss [0.21142018 0.08776214 0.1601364  0.08369491]\n",
      "Training loss [0.2151793  0.08529197 0.17359984 0.08172189]\n",
      "Training loss [0.23464751 0.0984185  0.18188763 0.09682089]\n",
      "Training loss [0.21856001 0.09841402 0.1693261  0.09590881]\n",
      "Training loss [0.44755512 0.5037334  0.51243985 0.4758744 ]\n",
      "Training loss [0.29884225 0.17176826 0.20497513 0.17332087]\n",
      "Training loss [0.28963748 0.16593277 0.21905342 0.16578852]\n",
      "Training loss [0.28971475 0.17039695 0.2280994  0.1720053 ]\n",
      "Training loss [0.27774844 0.15851592 0.21238874 0.15909664]\n",
      "Training loss [0.282438   0.15160139 0.22906545 0.15277103]\n",
      "Training loss [0.28482553 0.15226945 0.20498887 0.15307623]\n",
      "Training loss [0.27640265 0.14994422 0.2189919  0.15047362]\n",
      "Training loss [0.27649182 0.15448159 0.22808972 0.15625492]\n",
      "Training loss [0.26589358 0.14413786 0.21237068 0.14429675]\n",
      "Training loss [0.26595813 0.13642246 0.22904558 0.13725811]\n",
      "Training loss [0.26910856 0.13918173 0.20497487 0.13814712]\n",
      "Training loss [0.26261324 0.13592148 0.21895835 0.13565892]\n",
      "Training loss [0.26084623 0.140933   0.22807561 0.14224905]\n",
      "Training loss [0.2544398  0.13041754 0.21235472 0.13104245]\n",
      "Training loss [0.25250456 0.12114562 0.22902739 0.12483886]\n",
      "Training loss [0.25613585 0.1262936  0.20496194 0.12690894]\n",
      "Training loss [0.25260907 0.12389341 0.21892694 0.12513986]\n",
      "Training loss [0.2491253  0.12915918 0.22806308 0.13089162]\n",
      "Training loss [0.24556494 0.11874894 0.21233885 0.11994482]\n",
      "Training loss [0.24349011 0.11172712 0.22900872 0.11388793]\n",
      "Training loss [0.24664421 0.11627238 0.20494917 0.11649346]\n",
      "Training loss [0.2454535  0.1137289  0.21889631 0.11555873]\n",
      "Training loss [0.24069157 0.11968113 0.22805196 0.12156929]\n",
      "Training loss [0.23827681 0.10865315 0.21232364 0.11112428]\n",
      "Training loss [0.42883903 0.43786705 0.44547534 0.42716977]\n",
      "Training loss [0.25843272 0.16430716 0.1990695  0.16377011]\n",
      "Training loss [0.25010642 0.14953366 0.19360052 0.14895879]\n",
      "Training loss [0.26133478 0.1449694  0.19135746 0.1439798 ]\n",
      "Training loss [0.23637787 0.14368099 0.1815327  0.14371094]\n",
      "Training loss [0.23336996 0.13551381 0.1772548  0.13585988]\n",
      "Training loss [0.24327767 0.1469357  0.19937213 0.1460076 ]\n",
      "Training loss [0.23901233 0.14044628 0.19362675 0.13937293]\n",
      "Training loss [0.25118953 0.13525385 0.19134822 0.13409121]\n",
      "Training loss [0.22472605 0.13320957 0.18151152 0.13301662]\n",
      "Training loss [0.21885487 0.12564725 0.17721379 0.12518851]\n",
      "Training loss [0.23147772 0.13628358 0.19934438 0.13515803]\n",
      "Training loss [0.22672017 0.12932518 0.19360436 0.12879664]\n",
      "Training loss [0.23741326 0.12446571 0.19134068 0.12236993]\n",
      "Training loss [0.21264355 0.12309286 0.18149012 0.12362361]\n",
      "Training loss [0.20562589 0.11605296 0.17717478 0.11415945]\n",
      "Training loss [0.22133578 0.1273216  0.19931665 0.12557885]\n",
      "Training loss [0.21770439 0.12013882 0.19358319 0.12085798]\n",
      "Training loss [0.22705464 0.11583468 0.19133428 0.11360262]\n",
      "Training loss [0.20390649 0.1155702  0.1814689  0.11627503]\n",
      "Training loss [0.19669779 0.10701194 0.17713818 0.10593591]\n",
      "Training loss [0.21441793 0.11898979 0.19928837 0.11753768]\n",
      "Training loss [0.21151479 0.11280684 0.19356397 0.11444569]\n",
      "Training loss [0.21963088 0.10760722 0.19132778 0.10585149]\n",
      "Training loss [0.19728953 0.1084777  0.18144804 0.10945715]\n",
      "Training loss [0.4482552  0.48389918 0.51525164 0.45603237]\n",
      "Training loss [0.2855948  0.17561483 0.21210608 0.17234841]\n",
      "Training loss [0.2742414  0.1573161  0.21022092 0.15892401]\n",
      "Training loss [0.2613989  0.14875665 0.21269909 0.14969204]\n",
      "Training loss [0.2684128  0.1521926  0.21808691 0.15443566]\n",
      "Training loss [0.26010835 0.14864336 0.21379045 0.15095505]\n",
      "Training loss [0.26339507 0.15011919 0.21213365 0.15147193]\n",
      "Training loss [0.25823766 0.14755495 0.21022472 0.1488364 ]\n",
      "Training loss [0.24589865 0.14023715 0.21267115 0.14203729]\n",
      "Training loss [0.25075057 0.1423811  0.21805263 0.1459414 ]\n",
      "Training loss [0.24542186 0.13867308 0.21376501 0.14180413]\n",
      "Training loss [0.24431491 0.13798577 0.21211483 0.14018744]\n",
      "Training loss [0.24096625 0.1380154  0.21022694 0.13878973]\n",
      "Training loss [0.2310077  0.13187408 0.21264839 0.13440815]\n",
      "Training loss [0.23524812 0.13269356 0.21801984 0.13697553]\n",
      "Training loss [0.23364133 0.1301851  0.21374208 0.13340864]\n",
      "Training loss [0.23092562 0.12880102 0.21209684 0.13080317]\n",
      "Training loss [0.22870137 0.12970087 0.21022765 0.13023768]\n",
      "Training loss [0.22067048 0.12451708 0.212629   0.12710637]\n",
      "Training loss [0.22485    0.12531696 0.2179896  0.12865669]\n",
      "Training loss [0.22606291 0.12329566 0.21372303 0.12559554]\n",
      "Training loss [0.22253081 0.12252493 0.21207856 0.1234169 ]\n",
      "Training loss [0.22040449 0.12257372 0.21022663 0.12094581]\n",
      "Training loss [0.21308708 0.11807683 0.21261084 0.11888309]\n",
      "Training loss [0.21758617 0.11944243 0.21796137 0.12045026]\n",
      "Training loss [0.45034721 0.45478326 0.50475806 0.45319536]\n",
      "Training loss [0.25540495 0.16698739 0.19524202 0.16916229]\n",
      "Training loss [0.241938   0.14472353 0.200623   0.14432338]\n",
      "Training loss [0.24747822 0.14466642 0.20398606 0.14445287]\n",
      "Training loss [0.24734011 0.14649943 0.2071055  0.14665349]\n",
      "Training loss [0.24375322 0.14729065 0.20311598 0.1491845 ]\n",
      "Training loss [0.23015565 0.14520511 0.1952309  0.14516252]\n",
      "Training loss [0.22483274 0.13673657 0.20060655 0.13658983]\n",
      "Training loss [0.23255609 0.13736692 0.2039622  0.13605869]\n",
      "Training loss [0.23491135 0.13965964 0.2070497  0.13951212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.22960918 0.14050168 0.20309678 0.14345416]\n",
      "Training loss [0.21659714 0.13760653 0.19520743 0.13642389]\n",
      "Training loss [0.21153142 0.12908591 0.20058788 0.12832569]\n",
      "Training loss [0.220875   0.12963964 0.20394102 0.12650394]\n",
      "Training loss [0.22486013 0.13315865 0.20700699 0.1309301 ]\n",
      "Training loss [0.21845835 0.13432942 0.20307943 0.13648672]\n",
      "Training loss [0.20700225 0.12988946 0.19518751 0.12762517]\n",
      "Training loss [0.20233837 0.12208422 0.20057091 0.12061261]\n",
      "Training loss [0.2137183  0.12199767 0.20392257 0.11899915]\n",
      "Training loss [0.2170332  0.12633306 0.20697168 0.12474284]\n",
      "Training loss [0.21087217 0.12838212 0.20306309 0.13011895]\n",
      "Training loss [0.20022349 0.12267849 0.19517031 0.12117575]\n",
      "Training loss [0.19544174 0.11551568 0.20055538 0.11465242]\n",
      "Training loss [0.20846245 0.11452461 0.20390624 0.11329643]\n",
      "Training loss [0.21107909 0.12049215 0.2069408  0.12011854]\n",
      "Training loss [0.4535912  0.48093036 0.5354239  0.45552868]\n",
      "Training loss [0.24816155 0.16535173 0.19837072 0.16278544]\n",
      "Training loss [0.25449443 0.14897747 0.19844222 0.14656895]\n",
      "Training loss [0.23881587 0.13262402 0.19979994 0.13216636]\n",
      "Training loss [0.23107134 0.13341108 0.2032448  0.13280693]\n",
      "Training loss [0.23305249 0.13591015 0.19983666 0.13550653]\n",
      "Training loss [0.21603912 0.12880535 0.19825248 0.12902012]\n",
      "Training loss [0.23900211 0.13566521 0.19842607 0.13396865]\n",
      "Training loss [0.22474763 0.12488439 0.19974837 0.1245065 ]\n",
      "Training loss [0.21734667 0.12568447 0.2032204  0.12536022]\n",
      "Training loss [0.21822323 0.12923563 0.19977066 0.12861508]\n",
      "Training loss [0.20379087 0.1213387  0.19823068 0.12184381]\n",
      "Training loss [0.22621553 0.12834924 0.19841711 0.12649639]\n",
      "Training loss [0.2134279  0.11827762 0.19970426 0.11841083]\n",
      "Training loss [0.20686811 0.11853495 0.20319982 0.1184122 ]\n",
      "Training loss [0.20667642 0.12277749 0.19971925 0.12271208]\n",
      "Training loss [0.19470713 0.11446066 0.19821206 0.11545091]\n",
      "Training loss [0.2176685  0.12198327 0.19840923 0.12012807]\n",
      "Training loss [0.20593446 0.11255382 0.19966412 0.11308036]\n",
      "Training loss [0.19957703 0.11327008 0.20318283 0.11291482]\n",
      "Training loss [0.19904056 0.1171627  0.19967341 0.11784433]\n",
      "Training loss [0.18828498 0.1091024  0.1981956  0.11063322]\n",
      "Training loss [0.21175651 0.11707534 0.1984023  0.11508299]\n",
      "Training loss [0.2005516  0.1082293  0.19962879 0.10866625]\n",
      "Training loss [0.19441582 0.10944642 0.2031671  0.10858752]\n",
      "Training loss [0.45954213 0.49488902 0.5241833  0.48529482]\n",
      "Training loss [0.2645248  0.17447604 0.19591519 0.1733008 ]\n",
      "Training loss [0.25363567 0.14349398 0.20239682 0.14437816]\n",
      "Training loss [0.23151442 0.13218702 0.19179606 0.13317254]\n",
      "Training loss [0.24215238 0.13240649 0.20598473 0.13279448]\n",
      "Training loss [0.22651497 0.12617238 0.19680682 0.12813534]\n",
      "Training loss [0.22289199 0.12714204 0.19575268 0.12757333]\n",
      "Training loss [0.22861567 0.12508075 0.20233479 0.12603654]\n",
      "Training loss [0.21232584 0.1246006  0.1917921  0.12402357]\n",
      "Training loss [0.22509645 0.12683198 0.20594189 0.12639447]\n",
      "Training loss [0.21178924 0.12107888 0.19676614 0.12298255]\n",
      "Training loss [0.2078159  0.12244534 0.19573481 0.12296051]\n",
      "Training loss [0.21351081 0.12006982 0.20230009 0.12103872]\n",
      "Training loss [0.20012583 0.11961607 0.19179112 0.11926655]\n",
      "Training loss [0.21449164 0.12258084 0.20590761 0.1220915 ]\n",
      "Training loss [0.20179152 0.11634843 0.19673628 0.11801296]\n",
      "Training loss [0.19785668 0.11780584 0.19571826 0.11852036]\n",
      "Training loss [0.20413852 0.11546044 0.20227517 0.11638571]\n",
      "Training loss [0.19258678 0.11465789 0.1917919  0.11497108]\n",
      "Training loss [0.20797467 0.11863524 0.20587891 0.11812846]\n",
      "Training loss [0.19500387 0.11205251 0.19671392 0.11334145]\n",
      "Training loss [0.19146672 0.11353415 0.19569784 0.11435732]\n",
      "Training loss [0.19792746 0.11128361 0.20225488 0.1123105 ]\n",
      "Training loss [0.18785457 0.11048815 0.19179133 0.11145701]\n",
      "Training loss [0.20330471 0.11489087 0.20585383 0.1145853 ]\n",
      "Training loss [0.46376514 0.49131447 0.51721984 0.4982361 ]\n",
      "Training loss [0.2440266  0.17201132 0.19477035 0.17345026]\n",
      "Training loss [0.2304602  0.1395193  0.1966498  0.13932995]\n",
      "Training loss [0.22356772 0.12556744 0.19742483 0.12679353]\n",
      "Training loss [0.23053731 0.12851009 0.20602939 0.12825474]\n",
      "Training loss [0.23195982 0.12293735 0.20805725 0.12431486]\n",
      "Training loss [0.20845827 0.1175238  0.1948837  0.11933402]\n",
      "Training loss [0.20795962 0.11583171 0.19657882 0.11583117]\n",
      "Training loss [0.20663741 0.11514123 0.19735348 0.11558868]\n",
      "Training loss [0.2170381  0.12098195 0.20596886 0.12125982]\n",
      "Training loss [0.21835801 0.11687368 0.20800996 0.11832707]\n",
      "Training loss [0.19825406 0.11261719 0.19483696 0.1146424 ]\n",
      "Training loss [0.1959984  0.11185435 0.19653249 0.11200376]\n",
      "Training loss [0.19618863 0.11164267 0.19730458 0.1117291 ]\n",
      "Training loss [0.20771542 0.11737151 0.20592639 0.11781655]\n",
      "Training loss [0.20830405 0.11353805 0.20797628 0.11476237]\n",
      "Training loss [0.19081289 0.10916862 0.19479428 0.11114886]\n",
      "Training loss [0.18756029 0.10826784 0.1964971  0.1084849 ]\n",
      "Training loss [0.18893394 0.10830018 0.19726236 0.10826848]\n",
      "Training loss [0.20100886 0.11372114 0.20589225 0.11413519]\n",
      "Training loss [0.20121877 0.11069988 0.2079527  0.11174364]\n",
      "Training loss [0.18555595 0.10584255 0.1947566  0.10784844]\n",
      "Training loss [0.18189052 0.10477696 0.19647023 0.10528367]\n",
      "Training loss [0.1837929  0.10521074 0.1972231  0.10522911]\n",
      "Training loss [0.19599704 0.11057578 0.20586279 0.11080158]\n",
      "Training loss [0.44858354 0.48421976 0.52893734 0.47385865]\n",
      "Training loss [0.23111099 0.16038617 0.17697984 0.15897126]\n",
      "Training loss [0.22343715 0.13298476 0.17804573 0.13276213]\n",
      "Training loss [0.21518067 0.117788   0.17766516 0.11680727]\n",
      "Training loss [0.2058743  0.11402532 0.17893606 0.11294605]\n",
      "Training loss [0.21086287 0.1089014  0.17960109 0.10796652]\n",
      "Training loss [0.18657018 0.10340627 0.17691584 0.1022727 ]\n",
      "Training loss [0.19838586 0.10625827 0.17791857 0.1060575 ]\n",
      "Training loss [0.19763727 0.10242248 0.17758775 0.10196356]\n",
      "Training loss [0.18909788 0.10264788 0.1788877  0.1031244 ]\n",
      "Training loss [0.19373785 0.101603   0.17950809 0.10048528]\n",
      "Training loss [0.17461376 0.09766983 0.17688105 0.09723579]\n",
      "Training loss [0.1872885  0.10202033 0.17784175 0.10228703]\n",
      "Training loss [0.18739277 0.09906732 0.17753124 0.0988185 ]\n",
      "Training loss [0.17979345 0.09953296 0.17886668 0.10019675]\n",
      "Training loss [0.18351233 0.09893541 0.17945614 0.09795303]\n",
      "Training loss [0.1676903  0.09516129 0.1768541  0.09479785]\n",
      "Training loss [0.18039918 0.09987818 0.17778626 0.10012537]\n",
      "Training loss [0.18073276 0.09694361 0.17749134 0.0969061 ]\n",
      "Training loss [0.1739944 0.097036  0.1788542 0.0979539]\n",
      "Training loss [0.17714451 0.096891   0.17942126 0.09600276]\n",
      "Training loss [0.16321234 0.0933307  0.17683324 0.09273344]\n",
      "Training loss [0.1759581  0.09804052 0.17773992 0.09824106]\n",
      "Training loss [0.17611226 0.09503209 0.17746109 0.09498888]\n",
      "Training loss [0.17010054 0.09478377 0.17884606 0.09593344]\n",
      "Training loss [0.44538444 0.4750981  0.49068272 0.4713561 ]\n",
      "Training loss [0.22613609 0.16524275 0.17464268 0.16601767]\n",
      "Training loss [0.22212777 0.14532481 0.18031798 0.14684057]\n",
      "Training loss [0.19753343 0.12544267 0.17610595 0.1272697 ]\n",
      "Training loss [0.20003438 0.12098445 0.17842646 0.12214452]\n",
      "Training loss [0.18923408 0.11677928 0.17506656 0.11707138]\n",
      "Training loss [0.18978643 0.10615501 0.17457442 0.10684593]\n",
      "Training loss [0.19618566 0.11272528 0.1802488  0.11352336]\n",
      "Training loss [0.18153678 0.10575433 0.17602937 0.10656499]\n",
      "Training loss [0.18664163 0.10904776 0.17830607 0.11003998]\n",
      "Training loss [0.1765568  0.10720617 0.17499465 0.10700426]\n",
      "Training loss [0.17970705 0.09947804 0.17451502 0.10011433]\n",
      "Training loss [0.18536152 0.10770185 0.18021597 0.10806677]\n",
      "Training loss [0.1742189  0.10167243 0.17597915 0.10229362]\n",
      "Training loss [0.17886361 0.10598448 0.178232   0.10685652]\n",
      "Training loss [0.1692947  0.10423675 0.17495501 0.10393875]\n",
      "Training loss [0.1737479  0.09745165 0.1744753  0.09786366]\n",
      "Training loss [0.17924102 0.10564775 0.18019335 0.10579821]\n",
      "Training loss [0.169567   0.09985615 0.17594087 0.10001142]\n",
      "Training loss [0.17357622 0.10434024 0.17817792 0.10506199]\n",
      "Training loss [0.16447896 0.10247014 0.17492652 0.10201097]\n",
      "Training loss [0.16956721 0.09625655 0.17444476 0.09650104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.17502056 0.10435236 0.18017471 0.10432146]\n",
      "Training loss [0.16615334 0.09869885 0.17590865 0.09851286]\n",
      "Training loss [0.16966727 0.10319222 0.17813519 0.10370182]\n",
      "Training loss [0.43744197 0.45587572 0.48485097 0.460305  ]\n",
      "Training loss [0.23816597 0.1793802  0.18728314 0.18069246]\n",
      "Training loss [0.19707204 0.1507298  0.17850986 0.15119255]\n",
      "Training loss [0.21294224 0.14401414 0.18811028 0.14415073]\n",
      "Training loss [0.20035085 0.13468598 0.190974   0.13563094]\n",
      "Training loss [0.19924906 0.1314154  0.18793976 0.13053124]\n",
      "Training loss [0.1976962  0.1258488  0.18715903 0.12567955]\n",
      "Training loss [0.1771023  0.12081099 0.17838855 0.12046303]\n",
      "Training loss [0.19581757 0.12306659 0.1880279  0.12235373]\n",
      "Training loss [0.1849715  0.12041039 0.1908965  0.11921145]\n",
      "Training loss [0.18662262 0.11960286 0.18785135 0.11929038]\n",
      "Training loss [0.18621713 0.11775111 0.18712023 0.11709268]\n",
      "Training loss [0.16975032 0.11499979 0.17834231 0.11453214]\n",
      "Training loss [0.18709795 0.11822097 0.18799132 0.11737026]\n",
      "Training loss [0.17737424 0.11653623 0.19085348 0.11548848]\n",
      "Training loss [0.17995268 0.11622927 0.18779531 0.11622135]\n",
      "Training loss [0.1797502  0.1151333  0.18709448 0.11469463]\n",
      "Training loss [0.16571432 0.11276735 0.17831425 0.11268348]\n",
      "Training loss [0.18145972 0.11626372 0.18796827 0.11540982]\n",
      "Training loss [0.17270836 0.11459304 0.19082324 0.11389239]\n",
      "Training loss [0.17565012 0.11481853 0.1877558  0.11461271]\n",
      "Training loss [0.1754832  0.11385719 0.18707182 0.11343338]\n",
      "Training loss [0.16304229 0.11148793 0.17829232 0.1117712 ]\n",
      "Training loss [0.17743462 0.11511195 0.18795052 0.11425362]\n",
      "Training loss [0.16944082 0.11338685 0.19079843 0.11286028]\n",
      "Training loss [0.43681479 0.46025315 0.46976793 0.46532995]\n",
      "Training loss [0.2216132  0.17848599 0.1818834  0.17709476]\n",
      "Training loss [0.20624879 0.1539819  0.1845631  0.1553974 ]\n",
      "Training loss [0.19560966 0.13997504 0.18265483 0.14006425]\n",
      "Training loss [0.18716377 0.1340028  0.18428443 0.13386542]\n",
      "Training loss [0.18721417 0.12812799 0.1847823  0.12799679]\n",
      "Training loss [0.1836977  0.12509742 0.18170486 0.12460209]\n",
      "Training loss [0.18560371 0.12135816 0.18442868 0.12245607]\n",
      "Training loss [0.17894933 0.11790265 0.1825679  0.11776443]\n",
      "Training loss [0.17377512 0.11809288 0.18415356 0.1186    ]\n",
      "Training loss [0.17505166 0.1168745  0.1846902  0.11621757]\n",
      "Training loss [0.17418468 0.11586738 0.1816399  0.1154491 ]\n",
      "Training loss [0.1775954  0.11423533 0.18436839 0.11513254]\n",
      "Training loss [0.17158423 0.11297244 0.18253355 0.11250529]\n",
      "Training loss [0.16718292 0.11369447 0.18409745 0.11417321]\n",
      "Training loss [0.16861394 0.11388533 0.1846528  0.1123985 ]\n",
      "Training loss [0.16884053 0.11292249 0.18160078 0.11217613]\n",
      "Training loss [0.17273241 0.11179428 0.1843273  0.11240306]\n",
      "Training loss [0.16692615 0.1108493  0.18251409 0.1105618 ]\n",
      "Training loss [0.16301271 0.11179328 0.18405719 0.11242542]\n",
      "Training loss [0.16436934 0.1121864  0.18463385 0.11065138]\n",
      "Training loss [0.16529825 0.11122912 0.1815731  0.1107669 ]\n",
      "Training loss [0.16932176 0.11059929 0.18429293 0.11118282]\n",
      "Training loss [0.16355467 0.10967037 0.18250032 0.10958864]\n",
      "Training loss [0.16003284 0.11089247 0.18402293 0.11143596]\n",
      "Training loss [0.42952645 0.44112456 0.44925514 0.4401544 ]\n",
      "Training loss [0.22657543 0.1852568  0.19118166 0.18679237]\n",
      "Training loss [0.20640391 0.16228059 0.19146985 0.16205087]\n",
      "Training loss [0.19098847 0.14817765 0.18825513 0.14767393]\n",
      "Training loss [0.19092253 0.13957945 0.18621896 0.13864033]\n",
      "Training loss [0.18237382 0.13442186 0.18820022 0.13326322]\n",
      "Training loss [0.188515   0.1367042  0.19100922 0.13574612]\n",
      "Training loss [0.18393026 0.12909451 0.19130953 0.13014166]\n",
      "Training loss [0.17488563 0.12627868 0.1880768  0.12654614]\n",
      "Training loss [0.17682105 0.1239415  0.18617386 0.12321863]\n",
      "Training loss [0.17016622 0.12337865 0.18803498 0.12229282]\n",
      "Training loss [0.17810942 0.12710875 0.19094193 0.12706207]\n",
      "Training loss [0.17578286 0.12233467 0.19126217 0.12307496]\n",
      "Training loss [0.167622   0.12169035 0.18800339 0.12174923]\n",
      "Training loss [0.16945955 0.12007978 0.18615547 0.1195052 ]\n",
      "Training loss [0.16386925 0.1203824  0.18793643 0.11941271]\n",
      "Training loss [0.17228568 0.12398174 0.19089898 0.12419131]\n",
      "Training loss [0.17076533 0.12025638 0.1912379  0.1205707 ]\n",
      "Training loss [0.16329479 0.11977044 0.18795493 0.11993127]\n",
      "Training loss [0.16488953 0.11848421 0.18613873 0.11817732]\n",
      "Training loss [0.16013297 0.11897688 0.18786442 0.11822135]\n",
      "Training loss [0.16867086 0.12254634 0.19086552 0.12275058]\n",
      "Training loss [0.16742036 0.11923422 0.19122371 0.11931311]\n",
      "Training loss [0.16022636 0.11868607 0.187916   0.11892766]\n",
      "Training loss [0.161635   0.11751676 0.18612292 0.11739882]\n",
      "Training loss [0.42977703 0.44097504 0.47391933 0.4503693 ]\n",
      "Training loss [0.22372612 0.18410753 0.18937911 0.18470451]\n",
      "Training loss [0.20105296 0.16510999 0.19014323 0.16633701]\n",
      "Training loss [0.19663763 0.15767586 0.19038855 0.1575616 ]\n",
      "Training loss [0.1963481  0.15131992 0.19676055 0.15221253]\n",
      "Training loss [0.19328034 0.1466486  0.19217405 0.14713992]\n",
      "Training loss [0.18489644 0.14145121 0.18894759 0.14109331]\n",
      "Training loss [0.17887157 0.13672994 0.18997054 0.13721003]\n",
      "Training loss [0.18022555 0.13639668 0.1902726  0.13636625]\n",
      "Training loss [0.1830079  0.13596305 0.19659942 0.13679022]\n",
      "Training loss [0.1813869  0.13407233 0.19211796 0.13435167]\n",
      "Training loss [0.17503467 0.1322107  0.18884203 0.13142632]\n",
      "Training loss [0.17064837 0.13000356 0.18991223 0.13049543]\n",
      "Training loss [0.17312334 0.13068458 0.19023067 0.13075915]\n",
      "Training loss [0.17654325 0.13207713 0.19654262 0.13268584]\n",
      "Training loss [0.17486192 0.13087961 0.19208558 0.13117862]\n",
      "Training loss [0.16977686 0.1294329  0.18879516 0.12819906]\n",
      "Training loss [0.16572484 0.12776607 0.18987878 0.12809359]\n",
      "Training loss [0.1687119  0.12871894 0.19020182 0.12890783]\n",
      "Training loss [0.17268135 0.13061753 0.19650814 0.13094011]\n",
      "Training loss [0.17054623 0.12957402 0.19205424 0.12988888]\n",
      "Training loss [0.16627052 0.12812018 0.18876432 0.12673293]\n",
      "Training loss [0.16242877 0.12662467 0.18985426 0.1268712 ]\n",
      "Training loss [0.16568142 0.12769046 0.19017601 0.12807181]\n",
      "Training loss [0.16996126 0.12978013 0.19648254 0.12992218]\n",
      "Training loss [0.4079897  0.43231323 0.45199758 0.42957097]\n",
      "Training loss [0.20246264 0.17136346 0.17902358 0.17166546]\n",
      "Training loss [0.19656487 0.16313566 0.18338795 0.16400808]\n",
      "Training loss [0.18063644 0.14728662 0.17872177 0.14735095]\n",
      "Training loss [0.18706778 0.14974229 0.18488343 0.14952049]\n",
      "Training loss [0.18226501 0.14323789 0.18588924 0.14348218]\n",
      "Training loss [0.169958   0.13362956 0.17878556 0.1335999 ]\n",
      "Training loss [0.17521578 0.13497701 0.1832018  0.13565817]\n",
      "Training loss [0.16361412 0.12857571 0.17863476 0.12883289]\n",
      "Training loss [0.17361349 0.13403414 0.18485966 0.1339699 ]\n",
      "Training loss [0.17191568 0.13105443 0.18577042 0.13108712]\n",
      "Training loss [0.16154817 0.12505455 0.17873599 0.12466978]\n",
      "Training loss [0.16717577 0.128488   0.18315993 0.12865202]\n",
      "Training loss [0.15727483 0.12391462 0.1786155  0.12388657]\n",
      "Training loss [0.16748226 0.13013363 0.18485424 0.12982899]\n",
      "Training loss [0.16653565 0.12792623 0.18570518 0.12775841]\n",
      "Training loss [0.1570298  0.12229378 0.1787079  0.12200625]\n",
      "Training loss [0.16302857 0.12669659 0.18314165 0.12648767]\n",
      "Training loss [0.15346897 0.12231946 0.17860284 0.12216912]\n",
      "Training loss [0.1637098  0.12859793 0.1848458  0.12836646]\n",
      "Training loss [0.16306241 0.12653364 0.18565378 0.1264014 ]\n",
      "Training loss [0.15411133 0.12092605 0.17868987 0.12080543]\n",
      "Training loss [0.16036665 0.12579116 0.18313515 0.1255148 ]\n",
      "Training loss [0.15115361 0.1215402  0.17859305 0.12128367]\n",
      "Training loss [0.16120446 0.1277344  0.18483266 0.12763704]\n",
      "Training loss [0.41083884 0.42794752 0.44832736 0.42388242]\n",
      "Training loss [0.2192139  0.18729243 0.19625239 0.18725316]\n",
      "Training loss [0.20966211 0.1751287  0.19749683 0.1753097 ]\n",
      "Training loss [0.19956674 0.16549382 0.19475247 0.16552043]\n",
      "Training loss [0.19006194 0.15948255 0.19277295 0.15984559]\n",
      "Training loss [0.19787568 0.161129   0.20124336 0.16112182]\n",
      "Training loss [0.18692872 0.15304394 0.19582343 0.15268731]\n",
      "Training loss [0.18820779 0.15087762 0.19727144 0.15023741]\n",
      "Training loss [0.18442622 0.14719994 0.19463417 0.14733586]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.17706501 0.14551145 0.19266465 0.14585528]\n",
      "Training loss [0.18729138 0.14916018 0.20118442 0.14934573]\n",
      "Training loss [0.17765121 0.14457262 0.19579822 0.14468995]\n",
      "Training loss [0.18049681 0.14457588 0.1972022  0.14351359]\n",
      "Training loss [0.17742793 0.14221556 0.1945953  0.14216933]\n",
      "Training loss [0.17091696 0.14128272 0.19261542 0.14185922]\n",
      "Training loss [0.1817099  0.14550665 0.20116305 0.14536515]\n",
      "Training loss [0.17321739 0.14161952 0.19579464 0.14192049]\n",
      "Training loss [0.1763596  0.1422023  0.19714949 0.14110565]\n",
      "Training loss [0.17367025 0.1403932  0.19457094 0.14000347]\n",
      "Training loss [0.16687638 0.13964269 0.19257814 0.14021005]\n",
      "Training loss [0.17817344 0.14400506 0.20114943 0.14355278]\n",
      "Training loss [0.17046334 0.14030868 0.1957899  0.14056125]\n",
      "Training loss [0.17391169 0.14096719 0.19709967 0.14007235]\n",
      "Training loss [0.17117858 0.13952377 0.19455388 0.1390234 ]\n",
      "Training loss [0.16456515 0.13875002 0.19254658 0.13932118]\n",
      "Training loss [0.40445262 0.41327235 0.42627364 0.41478655]\n",
      "Training loss [0.21538126 0.1876725  0.1947994  0.1877464 ]\n",
      "Training loss [0.18970867 0.16617413 0.18800509 0.16681781]\n",
      "Training loss [0.18809225 0.16198802 0.18952538 0.16218531]\n",
      "Training loss [0.18757728 0.1576437  0.1939271  0.15747055]\n",
      "Training loss [0.17978485 0.15134914 0.18810517 0.15187979]\n",
      "Training loss [0.18371381 0.15457317 0.19458169 0.15476775]\n",
      "Training loss [0.16961077 0.1440107  0.18794596 0.14423235]\n",
      "Training loss [0.17307267 0.14513043 0.18945341 0.14549427]\n",
      "Training loss [0.17432675 0.14530104 0.19385985 0.14502229]\n",
      "Training loss [0.16901994 0.14150202 0.18807234 0.1416871 ]\n",
      "Training loss [0.17401724 0.14605111 0.19455427 0.14634915]\n",
      "Training loss [0.16224271 0.13774593 0.18792199 0.13791898]\n",
      "Training loss [0.16664162 0.1400496  0.18942125 0.14055301]\n",
      "Training loss [0.1677804  0.1415048  0.19382387 0.14142448]\n",
      "Training loss [0.16365275 0.1384475  0.18805265 0.13841218]\n",
      "Training loss [0.16925906 0.14304258 0.19454111 0.14334181]\n",
      "Training loss [0.1580962  0.13536486 0.18789992 0.13559364]\n",
      "Training loss [0.16267923 0.13802272 0.18939523 0.13859123]\n",
      "Training loss [0.16409662 0.13993907 0.19379377 0.13986897]\n",
      "Training loss [0.16035736 0.13707227 0.18802845 0.13702619]\n",
      "Training loss [0.16635744 0.14173356 0.19453013 0.14188811]\n",
      "Training loss [0.15531613 0.13427658 0.1878795  0.13451955]\n",
      "Training loss [0.16025212 0.13697809 0.18936849 0.13756421]\n",
      "Training loss [0.16186728 0.13906038 0.19376725 0.13897634]\n",
      "Training loss [0.40255144 0.41460007 0.42772943 0.40913615]\n",
      "Training loss [0.20988002 0.18282008 0.19187158 0.18300301]\n",
      "Training loss [0.19325449 0.16951074 0.19064747 0.16980009]\n",
      "Training loss [0.18904147 0.16013362 0.19002992 0.15966223]\n",
      "Training loss [0.1838946  0.15639794 0.19058661 0.1563156 ]\n",
      "Training loss [0.18072765 0.15296383 0.18928747 0.15323898]\n",
      "Training loss [0.17757398 0.15022351 0.19153236 0.14999962]\n",
      "Training loss [0.17286235 0.14722323 0.19052142 0.14708583]\n",
      "Training loss [0.17283395 0.1438578  0.1900075  0.1436044 ]\n",
      "Training loss [0.17218745 0.14410365 0.19053291 0.14417654]\n",
      "Training loss [0.17019364 0.14263889 0.18920445 0.14279577]\n",
      "Training loss [0.16814744 0.14260983 0.19150458 0.14253786]\n",
      "Training loss [0.16539317 0.1409997  0.19048831 0.14087823]\n",
      "Training loss [0.16578063 0.139129   0.19000489 0.13917941]\n",
      "Training loss [0.16665162 0.1401712  0.19051181 0.1401153 ]\n",
      "Training loss [0.16502184 0.1395446  0.18914941 0.13945217]\n",
      "Training loss [0.16340202 0.13995698 0.19149746 0.14001095]\n",
      "Training loss [0.16113204 0.13882141 0.19045812 0.13864718]\n",
      "Training loss [0.16206598 0.13702413 0.19000147 0.13726966]\n",
      "Training loss [0.16334137 0.13857295 0.19049385 0.13834307]\n",
      "Training loss [0.16178443 0.13819724 0.18910372 0.13792357]\n",
      "Training loss [0.16049072 0.13871334 0.19149171 0.1387868 ]\n",
      "Training loss [0.15882997 0.13772097 0.19043224 0.13763866]\n",
      "Training loss [0.15984501 0.1360983  0.1899971  0.13629438]\n",
      "Training loss [0.16098353 0.13763173 0.1904746  0.13747391]\n",
      "Training loss [0.3898844  0.40154153 0.41117853 0.40023628]\n",
      "Training loss [0.20180973 0.17517449 0.18354288 0.17550233]\n",
      "Training loss [0.19691302 0.16768703 0.18666388 0.16781595]\n",
      "Training loss [0.18418364 0.15607719 0.18497345 0.15643731]\n",
      "Training loss [0.17529437 0.15229432 0.18520539 0.15235582]\n",
      "Training loss [0.17448887 0.15029657 0.18313181 0.15130174]\n",
      "Training loss [0.17012413 0.14673777 0.18336472 0.14683974]\n",
      "Training loss [0.1761226  0.14740898 0.18657106 0.14727393]\n",
      "Training loss [0.16872564 0.14200221 0.18492854 0.14223185]\n",
      "Training loss [0.16340533 0.14160562 0.18517238 0.14159083]\n",
      "Training loss [0.1655376  0.14151314 0.1831288  0.14244406]\n",
      "Training loss [0.16187495 0.1399794  0.18335722 0.14011791]\n",
      "Training loss [0.16846499 0.14157    0.1865501  0.14139593]\n",
      "Training loss [0.16289009 0.13753547 0.18490568 0.13770816]\n",
      "Training loss [0.15877762 0.13792996 0.185155   0.13801861]\n",
      "Training loss [0.16119385 0.13828401 0.18312395 0.13907711]\n",
      "Training loss [0.15805438 0.13761848 0.18334772 0.13753289]\n",
      "Training loss [0.16493984 0.1391871  0.1865347  0.13907087]\n",
      "Training loss [0.15940708 0.13563475 0.18488476 0.13576344]\n",
      "Training loss [0.15575054 0.13620803 0.18514182 0.13646509]\n",
      "Training loss [0.15844093 0.13691862 0.18312082 0.13751972]\n",
      "Training loss [0.1557344  0.13649471 0.18333678 0.13639706]\n",
      "Training loss [0.16269913 0.13793069 0.18652156 0.13791719]\n",
      "Training loss [0.15709384 0.1346781  0.18486662 0.13472047]\n",
      "Training loss [0.15364435 0.1352201  0.18513195 0.13577259]\n",
      "Training loss [0.44556284 0.47424537 0.46808907 0.4822371 ]\n",
      "Training loss [0.36000052 0.19162092 0.2904647  0.19106963]\n",
      "Training loss [0.36918163 0.19422281 0.2508117  0.19419524]\n",
      "Training loss [0.35105562 0.19303471 0.23756462 0.19019777]\n",
      "Training loss [0.3543896  0.19411981 0.25574303 0.19414026]\n",
      "Training loss [0.33708933 0.18378896 0.24053095 0.18444435]\n",
      "Training loss [0.35388637 0.18506196 0.2856871  0.18509921]\n",
      "Training loss [0.36634612 0.18752457 0.25110936 0.1866835 ]\n",
      "Training loss [0.34598434 0.18427277 0.23701169 0.1814329 ]\n",
      "Training loss [0.35066202 0.18592161 0.2555241  0.18538149]\n",
      "Training loss [0.33281595 0.17568356 0.24060926 0.1757457 ]\n",
      "Training loss [0.34799218 0.17772716 0.28540033 0.17734721]\n",
      "Training loss [0.36166725 0.17788167 0.25122672 0.17656052]\n",
      "Training loss [0.33884668 0.17229548 0.23697895 0.16914698]\n",
      "Training loss [0.34563512 0.17404294 0.25548372 0.17354278]\n",
      "Training loss [0.32733822 0.16452    0.24062105 0.16417666]\n",
      "Training loss [0.34028625 0.16789186 0.28533205 0.16755521]\n",
      "Training loss [0.3559441  0.16452867 0.2512351  0.16302669]\n",
      "Training loss [0.33055955 0.15813825 0.23696841 0.15517616]\n",
      "Training loss [0.34013003 0.15903023 0.25545892 0.15920717]\n",
      "Training loss [0.32179016 0.15090588 0.2406155  0.15062846]\n",
      "Training loss [0.33225203 0.15641375 0.285298   0.15684843]\n",
      "Training loss [0.35023767 0.14696388 0.25123    0.146305  ]\n",
      "Training loss [0.32279515 0.14318688 0.23696107 0.140493  ]\n",
      "Training loss [0.33503333 0.13949835 0.25543672 0.14187661]\n",
      "Training loss [0.46842664 0.4365895  0.55148864 0.48414207]\n",
      "Training loss [0.32841718 0.17966896 0.2355431  0.17922449]\n",
      "Training loss [0.32019565 0.17052667 0.22390641 0.16964391]\n",
      "Training loss [0.3207899  0.16718487 0.2372536  0.1666605 ]\n",
      "Training loss [0.32333666 0.16554418 0.23595588 0.16408384]\n",
      "Training loss [0.3132184  0.15280819 0.24197328 0.15104282]\n",
      "Training loss [0.31984237 0.16862443 0.23077744 0.16730523]\n",
      "Training loss [0.3149506  0.15931058 0.2233586  0.15901928]\n",
      "Training loss [0.31426615 0.15793167 0.2369883  0.1575819 ]\n",
      "Training loss [0.31674796 0.15512286 0.23594156 0.1542294 ]\n",
      "Training loss [0.30607575 0.14389485 0.24204357 0.14238098]\n",
      "Training loss [0.31168038 0.15649709 0.23077528 0.15537265]\n",
      "Training loss [0.30737364 0.14625646 0.22340104 0.14639823]\n",
      "Training loss [0.3053267  0.14800763 0.23696902 0.14804195]\n",
      "Training loss [0.30865037 0.1445226  0.23591828 0.14371398]\n",
      "Training loss [0.2977768  0.13420974 0.24204224 0.1333471 ]\n",
      "Training loss [0.30290723 0.14541122 0.23078197 0.14474694]\n",
      "Training loss [0.29973793 0.13420807 0.22339618 0.135261  ]\n",
      "Training loss [0.29616794 0.13874917 0.2369675  0.13877504]\n",
      "Training loss [0.30110264 0.13427088 0.23589498 0.13373834]\n",
      "Training loss [0.2900848  0.12508345 0.24203625 0.12446688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.29570195 0.13448048 0.23078826 0.13362363]\n",
      "Training loss [0.2937036  0.12298393 0.22338629 0.12449276]\n",
      "Training loss [0.2883988  0.12941372 0.2369671  0.12873593]\n",
      "Training loss [0.29488498 0.12386254 0.23587257 0.1243337 ]\n",
      "Training loss [0.43692797 0.47186378 0.48918504 0.45278186]\n",
      "Training loss [0.29873055 0.17647359 0.21717218 0.17720526]\n",
      "Training loss [0.2916938  0.17385623 0.21609849 0.17355525]\n",
      "Training loss [0.30118436 0.16891664 0.21696827 0.1681322 ]\n",
      "Training loss [0.2920645  0.16262126 0.20985776 0.16273409]\n",
      "Training loss [0.28649715 0.15594684 0.21346875 0.15373826]\n",
      "Training loss [0.28546017 0.16356468 0.2161799  0.16272625]\n",
      "Training loss [0.28328523 0.15584879 0.21605684 0.1561102 ]\n",
      "Training loss [0.29169798 0.14927188 0.21709011 0.14818126]\n",
      "Training loss [0.28277025 0.14357841 0.20980456 0.14361617]\n",
      "Training loss [0.27456143 0.13667503 0.21343903 0.13597532]\n",
      "Training loss [0.2730102  0.14539748 0.216169   0.1459843 ]\n",
      "Training loss [0.27038944 0.13389911 0.21603973 0.13570783]\n",
      "Training loss [0.27915588 0.128284   0.21707419 0.12924261]\n",
      "Training loss [0.27112037 0.12732263 0.20978722 0.12734163]\n",
      "Training loss [0.25968897 0.12071419 0.21342185 0.12171978]\n",
      "Training loss [0.26027715 0.13063914 0.21615821 0.13192664]\n",
      "Training loss [0.25795355 0.11908748 0.21602228 0.1206368 ]\n",
      "Training loss [0.2681829  0.11528392 0.2170563  0.11649062]\n",
      "Training loss [0.2611078  0.11618842 0.20977327 0.11572602]\n",
      "Training loss [0.24794903 0.10919826 0.2134048  0.11192836]\n",
      "Training loss [0.24992552 0.11918499 0.21614778 0.12079287]\n",
      "Training loss [0.24857028 0.10867807 0.21600375 0.11010391]\n",
      "Training loss [0.2591862  0.10576579 0.21703625 0.1078472 ]\n",
      "Training loss [0.25314298 0.10736296 0.20976074 0.10679577]\n",
      "Training loss [0.42571637 0.43365404 0.46619424 0.43025917]\n",
      "Training loss [0.30874544 0.1743626  0.20229372 0.17062141]\n",
      "Training loss [0.2707232  0.15111348 0.17649722 0.15067029]\n",
      "Training loss [0.2756283  0.15533715 0.18058516 0.15395206]\n",
      "Training loss [0.24719167 0.14178658 0.16924869 0.14148258]\n",
      "Training loss [0.26015872 0.15319201 0.17318015 0.15241915]\n",
      "Training loss [0.2937955  0.15629135 0.20216113 0.15389748]\n",
      "Training loss [0.25811318 0.1401865  0.17650425 0.13922736]\n",
      "Training loss [0.26347908 0.14173833 0.180574   0.14079618]\n",
      "Training loss [0.23363666 0.1299943  0.16923958 0.1292911 ]\n",
      "Training loss [0.2454386  0.13746044 0.17317902 0.13623403]\n",
      "Training loss [0.2759112  0.14257419 0.20215487 0.13997146]\n",
      "Training loss [0.24179284 0.12768897 0.17648438 0.12638837]\n",
      "Training loss [0.24985403 0.12835276 0.18056045 0.12675695]\n",
      "Training loss [0.22024374 0.1181986  0.16923025 0.11579669]\n",
      "Training loss [0.23138097 0.12333535 0.17317781 0.11843634]\n",
      "Training loss [0.26231265 0.13088699 0.20214882 0.12540805]\n",
      "Training loss [0.23021352 0.11671451 0.17646268 0.1125451 ]\n",
      "Training loss [0.24014597 0.1187049  0.18054707 0.11326639]\n",
      "Training loss [0.20985825 0.1070461  0.16922016 0.10267758]\n",
      "Training loss [0.2207303  0.11176245 0.1731766  0.10514769]\n",
      "Training loss [0.25370103 0.11863579 0.20214108 0.11561126]\n",
      "Training loss [0.22177398 0.10530857 0.17644231 0.10219342]\n",
      "Training loss [0.23288944 0.10878381 0.18053314 0.10399961]\n",
      "Training loss [0.20209195 0.09634647 0.16921023 0.09382512]\n",
      "Training loss [0.43819553 0.44308507 0.50658196 0.44101754]\n",
      "Training loss [0.27568126 0.15891066 0.20276    0.15479587]\n",
      "Training loss [0.2583916  0.14551027 0.1938455  0.14563018]\n",
      "Training loss [0.25527367 0.139295   0.19401798 0.13821349]\n",
      "Training loss [0.24769315 0.13380264 0.19117165 0.1331906 ]\n",
      "Training loss [0.2509789  0.13663441 0.20169142 0.1364928 ]\n",
      "Training loss [0.25271872 0.13357556 0.2023006  0.1329399 ]\n",
      "Training loss [0.24300776 0.13454449 0.1937456  0.13397121]\n",
      "Training loss [0.24096681 0.12754415 0.19400266 0.12656121]\n",
      "Training loss [0.23179    0.12136364 0.19113773 0.12087934]\n",
      "Training loss [0.23656371 0.12242562 0.20167528 0.12113018]\n",
      "Training loss [0.23722917 0.12100385 0.20228428 0.12052588]\n",
      "Training loss [0.22590624 0.12130683 0.19371977 0.1207834 ]\n",
      "Training loss [0.2267188  0.11276402 0.19398606 0.11266746]\n",
      "Training loss [0.21817091 0.10666601 0.19110683 0.10716266]\n",
      "Training loss [0.22510543 0.10704112 0.20166269 0.10687555]\n",
      "Training loss [0.22556941 0.108824   0.20226905 0.10954552]\n",
      "Training loss [0.21490246 0.10952595 0.19369477 0.11063759]\n",
      "Training loss [0.21625845 0.10067547 0.1939724  0.10146394]\n",
      "Training loss [0.20905374 0.0973144  0.19107883 0.09822962]\n",
      "Training loss [0.21693549 0.09728953 0.20164832 0.09882468]\n",
      "Training loss [0.2169081  0.1012765  0.20225555 0.1016912 ]\n",
      "Training loss [0.20768651 0.10162243 0.19367084 0.10337119]\n",
      "Training loss [0.2085353  0.09293938 0.19396153 0.09341229]\n",
      "Training loss [0.20266554 0.09087701 0.19105124 0.09179699]\n",
      "Training loss [0.46623862 0.44469082 0.5314539  0.4524797 ]\n",
      "Training loss [0.25848323 0.13627903 0.16887137 0.138162  ]\n",
      "Training loss [0.2573182  0.12554473 0.16789421 0.12611172]\n",
      "Training loss [0.23021083 0.12090994 0.16627231 0.12254658]\n",
      "Training loss [0.25068003 0.12348736 0.17954317 0.12424727]\n",
      "Training loss [0.21827045 0.1102481  0.15650423 0.10979934]\n",
      "Training loss [0.23709613 0.11136326 0.16885403 0.112271  ]\n",
      "Training loss [0.24482998 0.11245778 0.16788083 0.11259048]\n",
      "Training loss [0.2198447  0.11002465 0.16624404 0.11140107]\n",
      "Training loss [0.23895322 0.11200403 0.17953217 0.11303568]\n",
      "Training loss [0.20666647 0.10046868 0.15650927 0.09968473]\n",
      "Training loss [0.22485782 0.10214497 0.16884702 0.10233783]\n",
      "Training loss [0.23203894 0.10300729 0.16785184 0.10273238]\n",
      "Training loss [0.20803265 0.10246599 0.16621882 0.10301144]\n",
      "Training loss [0.22578913 0.10334218 0.1795238  0.10435306]\n",
      "Training loss [0.19421999 0.09338812 0.15651253 0.09255816]\n",
      "Training loss [0.2129164  0.09605922 0.16884133 0.09592403]\n",
      "Training loss [0.22063357 0.09617661 0.16782692 0.09617033]\n",
      "Training loss [0.19884881 0.09661791 0.16619724 0.0971916 ]\n",
      "Training loss [0.21632135 0.09571329 0.17951664 0.09803664]\n",
      "Training loss [0.18567804 0.08698936 0.15651384 0.08662985]\n",
      "Training loss [0.20422432 0.09037971 0.16883606 0.09034156]\n",
      "Training loss [0.21215597 0.08851846 0.16780388 0.0899761 ]\n",
      "Training loss [0.19264443 0.09002373 0.1661774  0.09162561]\n",
      "Training loss [0.21029699 0.08893688 0.17951044 0.09240086]\n",
      "Training loss [0.42802525 0.45001447 0.47189057 0.46203357]\n",
      "Training loss [0.24353571 0.14010552 0.17663825 0.13875866]\n",
      "Training loss [0.22841069 0.12024695 0.1854163  0.11879035]\n",
      "Training loss [0.23044421 0.11936955 0.1840204  0.12070475]\n",
      "Training loss [0.23098803 0.12058086 0.17947784 0.12092946]\n",
      "Training loss [0.22940359 0.11879125 0.18413466 0.11945583]\n",
      "Training loss [0.2142904  0.10911278 0.17661205 0.11021852]\n",
      "Training loss [0.21190953 0.10753574 0.18538167 0.10862873]\n",
      "Training loss [0.21585917 0.11032967 0.18399367 0.11294443]\n",
      "Training loss [0.21709758 0.11267097 0.17945436 0.11280368]\n",
      "Training loss [0.21576694 0.11158921 0.18410248 0.1122095 ]\n",
      "Training loss [0.19941986 0.10100187 0.17661616 0.10300256]\n",
      "Training loss [0.2001805  0.10027077 0.18535091 0.10161996]\n",
      "Training loss [0.2031863  0.10233131 0.18397157 0.10509969]\n",
      "Training loss [0.20463406 0.10591425 0.17943235 0.10618667]\n",
      "Training loss [0.20495369 0.10468268 0.18407115 0.10588773]\n",
      "Training loss [0.18678291 0.09396726 0.17661995 0.09711814]\n",
      "Training loss [0.19077966 0.09321167 0.18532398 0.09514593]\n",
      "Training loss [0.19335245 0.09550184 0.18395188 0.09795703]\n",
      "Training loss [0.19569066 0.10008089 0.1794127  0.10057548]\n",
      "Training loss [0.19733965 0.098636   0.18404017 0.10008641]\n",
      "Training loss [0.17793967 0.08807708 0.1766234  0.09157465]\n",
      "Training loss [0.18408218 0.08677854 0.18529938 0.08920687]\n",
      "Training loss [0.1865493  0.08927716 0.18393281 0.09179108]\n",
      "Training loss [0.18963374 0.09447715 0.17939025 0.09553967]\n",
      "Training loss [0.43568397 0.46708655 0.49540526 0.4665656 ]\n",
      "Training loss [0.251086   0.16044717 0.18547422 0.16145313]\n",
      "Training loss [0.22338942 0.12648785 0.17175284 0.12555167]\n",
      "Training loss [0.22367816 0.12143072 0.17865421 0.1210974 ]\n",
      "Training loss [0.22309193 0.11354785 0.17759952 0.11359183]\n",
      "Training loss [0.21578592 0.11484782 0.1798581  0.11561088]\n",
      "Training loss [0.21686722 0.12165242 0.18544829 0.12088229]\n",
      "Training loss [0.20335078 0.11407922 0.17173365 0.11300445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.21051869 0.11353271 0.17861593 0.11277843]\n",
      "Training loss [0.20657083 0.10661734 0.17756833 0.10628738]\n",
      "Training loss [0.20320624 0.10899495 0.17981994 0.1090176 ]\n",
      "Training loss [0.20404541 0.11453183 0.18542773 0.11369729]\n",
      "Training loss [0.19007179 0.1077827  0.17171776 0.10678305]\n",
      "Training loss [0.2001388  0.10690668 0.17858712 0.10657597]\n",
      "Training loss [0.19460195 0.10052505 0.17754076 0.10031703]\n",
      "Training loss [0.1930335  0.10356423 0.17978762 0.10341801]\n",
      "Training loss [0.19504867 0.10836156 0.18541092 0.10744502]\n",
      "Training loss [0.18136628 0.10250419 0.1717013  0.1017665 ]\n",
      "Training loss [0.19309863 0.10155292 0.17856267 0.10120335]\n",
      "Training loss [0.18735227 0.09591255 0.17751507 0.09540362]\n",
      "Training loss [0.18581949 0.09913647 0.17975911 0.09887351]\n",
      "Training loss [0.18865432 0.1033975  0.18539721 0.10226011]\n",
      "Training loss [0.17544974 0.09801254 0.17168559 0.09731901]\n",
      "Training loss [0.18778148 0.09727237 0.17854148 0.09704277]\n",
      "Training loss [0.1824238  0.09212159 0.17749128 0.0912757 ]\n",
      "Training loss [0.43419173 0.47358152 0.49840584 0.47140282]\n",
      "Training loss [0.2520534  0.16852748 0.19735442 0.17034867]\n",
      "Training loss [0.24447304 0.13903788 0.200239   0.14080784]\n",
      "Training loss [0.22871232 0.13013099 0.19508824 0.12953244]\n",
      "Training loss [0.22071204 0.12956542 0.1957093  0.12978646]\n",
      "Training loss [0.23216246 0.13087162 0.20326647 0.13145047]\n",
      "Training loss [0.2198541  0.12012709 0.19718629 0.12044202]\n",
      "Training loss [0.22483054 0.12204364 0.20019436 0.12234005]\n",
      "Training loss [0.21165337 0.12169459 0.19503953 0.12173603]\n",
      "Training loss [0.20587611 0.12472373 0.1956777  0.12470901]\n",
      "Training loss [0.21858224 0.12611714 0.2032191  0.1263904 ]\n",
      "Training loss [0.20679554 0.11609318 0.1971164  0.11633892]\n",
      "Training loss [0.2117914  0.1182069  0.2001594  0.11866102]\n",
      "Training loss [0.20101933 0.11799939 0.19500728 0.11768478]\n",
      "Training loss [0.19529042 0.12130082 0.19565205 0.12121722]\n",
      "Training loss [0.20935176 0.12220838 0.20317882 0.12175921]\n",
      "Training loss [0.19771394 0.11239086 0.1970605  0.11276529]\n",
      "Training loss [0.20251587 0.11404394 0.20012726 0.11457321]\n",
      "Training loss [0.19397059 0.11455049 0.19498667 0.11362436]\n",
      "Training loss [0.18840666 0.11780256 0.1956325  0.1177381 ]\n",
      "Training loss [0.2033507  0.11838676 0.20314604 0.11724366]\n",
      "Training loss [0.19124602 0.10886197 0.19701424 0.10943958]\n",
      "Training loss [0.1964723  0.11009623 0.20009829 0.11084841]\n",
      "Training loss [0.18884283 0.11124897 0.1949693  0.11005369]\n",
      "Training loss [0.18379664 0.11431589 0.1956169  0.11462319]\n",
      "Training loss [0.43908903 0.46316606 0.5160599  0.47278875]\n",
      "Training loss [0.2347469  0.15390477 0.17490155 0.15364411]\n",
      "Training loss [0.22249535 0.11976531 0.17287327 0.11919545]\n",
      "Training loss [0.21971834 0.11199357 0.17354344 0.11130288]\n",
      "Training loss [0.21686354 0.1116564  0.17792311 0.1105435 ]\n",
      "Training loss [0.20399365 0.10606753 0.17283717 0.10620795]\n",
      "Training loss [0.19308397 0.10462483 0.17477293 0.10432639]\n",
      "Training loss [0.19584675 0.094721   0.17282215 0.09515613]\n",
      "Training loss [0.19840172 0.10030147 0.17351943 0.09962875]\n",
      "Training loss [0.19811253 0.10346402 0.17786124 0.10280304]\n",
      "Training loss [0.18634173 0.09978233 0.17276725 0.09932159]\n",
      "Training loss [0.17937759 0.10014844 0.17473175 0.09953295]\n",
      "Training loss [0.18198869 0.09010203 0.1727958  0.09044263]\n",
      "Training loss [0.1856207  0.09630199 0.17350963 0.09504883]\n",
      "Training loss [0.1873751  0.09948053 0.17781797 0.0985692 ]\n",
      "Training loss [0.17586778 0.09597822 0.17271857 0.09508997]\n",
      "Training loss [0.17138907 0.097298   0.17469847 0.09611128]\n",
      "Training loss [0.17341249 0.08685899 0.1727783  0.08691841]\n",
      "Training loss [0.17814483 0.09274025 0.17350288 0.09147812]\n",
      "Training loss [0.18119048 0.09631065 0.17778367 0.09521467]\n",
      "Training loss [0.16920763 0.09308975 0.1726804  0.09203623]\n",
      "Training loss [0.1663229  0.09491484 0.17466879 0.09359986]\n",
      "Training loss [0.1676479  0.08438411 0.17276376 0.08434502]\n",
      "Training loss [0.17317985 0.08975126 0.1734962  0.08892117]\n",
      "Training loss [0.17722726 0.0936287  0.17775334 0.0928074 ]\n",
      "Training loss [0.4459007  0.47287112 0.50406617 0.46978748]\n",
      "Training loss [0.23685671 0.1675319  0.178541   0.16425633]\n",
      "Training loss [0.21954691 0.13658375 0.17723891 0.13634557]\n",
      "Training loss [0.2078483  0.12376545 0.1772945  0.12298062]\n",
      "Training loss [0.19351232 0.1140435  0.16901533 0.11474925]\n",
      "Training loss [0.19940105 0.11303438 0.17730147 0.1121551 ]\n",
      "Training loss [0.19538271 0.11033502 0.17843279 0.10951148]\n",
      "Training loss [0.1958798  0.10904005 0.17713597 0.10874367]\n",
      "Training loss [0.18910736 0.10747127 0.17722934 0.10695878]\n",
      "Training loss [0.17838515 0.10450182 0.16894126 0.10450466]\n",
      "Training loss [0.18513744 0.10450502 0.17720509 0.10380036]\n",
      "Training loss [0.18293    0.10406633 0.17838117 0.10378602]\n",
      "Training loss [0.18393606 0.10446807 0.17707941 0.10413879]\n",
      "Training loss [0.17792599 0.10360029 0.17718157 0.10309736]\n",
      "Training loss [0.16942099 0.10119756 0.16888829 0.10143133]\n",
      "Training loss [0.17599607 0.10134843 0.17713697 0.10090102]\n",
      "Training loss [0.17555931 0.1013549  0.1783396  0.10125385]\n",
      "Training loss [0.17629862 0.10231308 0.1770423  0.10191513]\n",
      "Training loss [0.17108005 0.10139576 0.1771425  0.10081005]\n",
      "Training loss [0.16338867 0.09886612 0.1688477  0.09929375]\n",
      "Training loss [0.1700267  0.09890521 0.17708336 0.09887522]\n",
      "Training loss [0.17072107 0.09911865 0.17830303 0.09926127]\n",
      "Training loss [0.17114949 0.10041803 0.1770123  0.10001856]\n",
      "Training loss [0.1665695  0.09954727 0.1771073  0.09890352]\n",
      "Training loss [0.15915066 0.09675892 0.16881518 0.09739171]\n",
      "Training loss [0.44379514 0.46969494 0.4980613  0.46226802]\n",
      "Training loss [0.2354284  0.18076827 0.19233471 0.18335113]\n",
      "Training loss [0.21564007 0.1512613  0.19234383 0.15168919]\n",
      "Training loss [0.21213634 0.13730133 0.1923205  0.13867107]\n",
      "Training loss [0.20163065 0.12804317 0.1941696  0.12886313]\n",
      "Training loss [0.19995648 0.12205753 0.19378401 0.12317088]\n",
      "Training loss [0.19383675 0.11985227 0.19229166 0.12059985]\n",
      "Training loss [0.19292443 0.11722008 0.19217846 0.11695347]\n",
      "Training loss [0.19476005 0.11684585 0.19222993 0.11680491]\n",
      "Training loss [0.1878652  0.11513864 0.19408704 0.11489481]\n",
      "Training loss [0.1858934  0.11108187 0.19374207 0.11204085]\n",
      "Training loss [0.18238205 0.11232895 0.1922585  0.11282982]\n",
      "Training loss [0.18317701 0.1114945  0.19208544 0.11132894]\n",
      "Training loss [0.18522032 0.11224486 0.19217387 0.11221895]\n",
      "Training loss [0.17991221 0.1116744  0.1940365  0.11137895]\n",
      "Training loss [0.17832682 0.10789962 0.19372083 0.10864023]\n",
      "Training loss [0.17584416 0.10987483 0.19224207 0.11026712]\n",
      "Training loss [0.17704417 0.10943943 0.19202666 0.10915942]\n",
      "Training loss [0.17990127 0.11034849 0.19212963 0.11040283]\n",
      "Training loss [0.17499706 0.10992931 0.19399843 0.10954525]\n",
      "Training loss [0.17374863 0.10627109 0.1937052  0.10694417]\n",
      "Training loss [0.17151533 0.10853063 0.19223335 0.10883895]\n",
      "Training loss [0.17284465 0.10815169 0.19198382 0.10785017]\n",
      "Training loss [0.1765467  0.10919066 0.19208997 0.10919669]\n",
      "Training loss [0.17153211 0.10874621 0.19396636 0.10823236]\n",
      "Training loss [0.42803305 0.45310727 0.48977053 0.45582178]\n",
      "Training loss [0.22222072 0.17106861 0.17932716 0.17234132]\n",
      "Training loss [0.19201349 0.14258775 0.17482036 0.14333054]\n",
      "Training loss [0.19181573 0.13293263 0.17799114 0.1328582 ]\n",
      "Training loss [0.19840375 0.12558551 0.1856234  0.12544501]\n",
      "Training loss [0.1848862  0.11879455 0.17838372 0.11989   ]\n",
      "Training loss [0.18260908 0.11370958 0.1792214  0.11333402]\n",
      "Training loss [0.16849102 0.10834142 0.17472668 0.10922766]\n",
      "Training loss [0.17516525 0.11052987 0.17789805 0.11065975]\n",
      "Training loss [0.18439913 0.10999411 0.18558322 0.10951201]\n",
      "Training loss [0.17194426 0.10700538 0.17830704 0.10793395]\n",
      "Training loss [0.17228682 0.10487091 0.17916723 0.10409835]\n",
      "Training loss [0.16033256 0.10240009 0.17467533 0.10275248]\n",
      "Training loss [0.16807887 0.10580623 0.17784797 0.10592268]\n",
      "Training loss [0.17735058 0.10601394 0.18555251 0.10544708]\n",
      "Training loss [0.16576333 0.10387285 0.17826952 0.10458111]\n",
      "Training loss [0.166671   0.1019823  0.17913795 0.10123539]\n",
      "Training loss [0.15567777 0.10031205 0.17464346 0.10058895]\n",
      "Training loss [0.16362754 0.10373987 0.1778109  0.10445209]\n",
      "Training loss [0.17286049 0.10418142 0.18552765 0.10388818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.16203238 0.10237026 0.17824423 0.10307455]\n",
      "Training loss [0.16313398 0.10067309 0.17911988 0.09995332]\n",
      "Training loss [0.15256193 0.09931807 0.17461838 0.0994793 ]\n",
      "Training loss [0.1603663  0.10249604 0.17777975 0.10372896]\n",
      "Training loss [0.16975692 0.10311454 0.18550608 0.10303474]\n",
      "Training loss [0.4331268  0.45868838 0.4737341  0.465049  ]\n",
      "Training loss [0.23395264 0.17773989 0.18524364 0.17798272]\n",
      "Training loss [0.20340872 0.15588199 0.18570869 0.1562235 ]\n",
      "Training loss [0.20539282 0.1479035  0.18992107 0.14798462]\n",
      "Training loss [0.20328465 0.1397436  0.18957509 0.13987589]\n",
      "Training loss [0.19564396 0.1338245  0.18976033 0.13401023]\n",
      "Training loss [0.19496214 0.12351711 0.18500866 0.12358206]\n",
      "Training loss [0.18033388 0.12252405 0.18557939 0.12239903]\n",
      "Training loss [0.18875921 0.12500246 0.18979192 0.12500247]\n",
      "Training loss [0.18851379 0.12513396 0.18944003 0.12601584]\n",
      "Training loss [0.18320166 0.12236319 0.18970089 0.12221191]\n",
      "Training loss [0.18365815 0.1138806  0.1849528  0.11398355]\n",
      "Training loss [0.17134272 0.11518247 0.18551934 0.11590835]\n",
      "Training loss [0.18058237 0.11994142 0.18973492 0.12002759]\n",
      "Training loss [0.18114312 0.12169719 0.18937203 0.12271387]\n",
      "Training loss [0.17663728 0.1190345  0.18967114 0.11914685]\n",
      "Training loss [0.17719984 0.11122128 0.18491665 0.11146088]\n",
      "Training loss [0.16624683 0.11297151 0.18547711 0.11384159]\n",
      "Training loss [0.17552115 0.11830304 0.18969794 0.11827396]\n",
      "Training loss [0.17646365 0.1201932  0.18932405 0.12116673]\n",
      "Training loss [0.17242935 0.11750277 0.18964821 0.11772555]\n",
      "Training loss [0.17283325 0.11009394 0.18488461 0.11026858]\n",
      "Training loss [0.16310959 0.1119124  0.18544076 0.11276439]\n",
      "Training loss [0.17200735 0.11744285 0.18966599 0.11728022]\n",
      "Training loss [0.17318276 0.11926763 0.18928339 0.12020331]\n",
      "Training loss [0.4220698  0.4409324  0.46901783 0.45261642]\n",
      "Training loss [0.22005948 0.18266657 0.18799779 0.18184939]\n",
      "Training loss [0.20495914 0.16521655 0.19290213 0.16579217]\n",
      "Training loss [0.19845006 0.15171912 0.18859535 0.15129876]\n",
      "Training loss [0.186789   0.14110301 0.18855357 0.1410687 ]\n",
      "Training loss [0.19009309 0.14306372 0.19541532 0.14246611]\n",
      "Training loss [0.185639   0.13679285 0.18783522 0.13770418]\n",
      "Training loss [0.18313657 0.13281173 0.19274819 0.13309805]\n",
      "Training loss [0.18171448 0.13027179 0.18846504 0.13056052]\n",
      "Training loss [0.174766   0.1268861  0.18848398 0.1265876 ]\n",
      "Training loss [0.17831717 0.1312206  0.19529831 0.13024075]\n",
      "Training loss [0.17605372 0.12833454 0.18777356 0.1290939 ]\n",
      "Training loss [0.17405343 0.12623289 0.19268788 0.12626141]\n",
      "Training loss [0.1736429  0.12577322 0.18840046 0.12592487]\n",
      "Training loss [0.16837797 0.12345649 0.18843552 0.12303824]\n",
      "Training loss [0.17225821 0.12790282 0.19523731 0.12714484]\n",
      "Training loss [0.17032486 0.12586881 0.18773317 0.12647425]\n",
      "Training loss [0.16874152 0.12404034 0.19265166 0.12424943]\n",
      "Training loss [0.16859612 0.1240903  0.18835554 0.12442818]\n",
      "Training loss [0.16436383 0.12196101 0.18839252 0.12161665]\n",
      "Training loss [0.16831717 0.12634553 0.19518778 0.12584987]\n",
      "Training loss [0.16645181 0.12472564 0.1876998  0.12508774]\n",
      "Training loss [0.16535999 0.12297884 0.19262479 0.12326213]\n",
      "Training loss [0.16518217 0.12322249 0.18831713 0.12360134]\n",
      "Training loss [0.16175014 0.12114042 0.18835184 0.12082721]\n",
      "Training loss [0.4264045  0.44325218 0.46100417 0.4389755 ]\n",
      "Training loss [0.22465812 0.18061516 0.1876187  0.18145461]\n",
      "Training loss [0.20784216 0.16567284 0.19257054 0.16581425]\n",
      "Training loss [0.19481404 0.15057151 0.18618138 0.1501748 ]\n",
      "Training loss [0.1928183  0.14426053 0.184625   0.14363101]\n",
      "Training loss [0.18830685 0.141615   0.1874684  0.14198303]\n",
      "Training loss [0.18997632 0.13779956 0.18734303 0.13809828]\n",
      "Training loss [0.18671043 0.13647349 0.19242549 0.13597077]\n",
      "Training loss [0.17872891 0.12887615 0.18605053 0.12908527]\n",
      "Training loss [0.17968549 0.12838456 0.18455227 0.1283569 ]\n",
      "Training loss [0.17770459 0.1296666  0.18740064 0.13055862]\n",
      "Training loss [0.18036693 0.12877846 0.18726397 0.12920445]\n",
      "Training loss [0.17830288 0.12989    0.19237712 0.1297497 ]\n",
      "Training loss [0.17100334 0.12380368 0.18601826 0.12433469]\n",
      "Training loss [0.17269596 0.12432121 0.18451825 0.12450898]\n",
      "Training loss [0.17188416 0.12620813 0.18736589 0.12726945]\n",
      "Training loss [0.17438468 0.1260885  0.18721938 0.1267319 ]\n",
      "Training loss [0.17356762 0.12781596 0.19234644 0.12780128]\n",
      "Training loss [0.16635668 0.12206227 0.18600583 0.12252731]\n",
      "Training loss [0.16831909 0.12278964 0.18449451 0.12314801]\n",
      "Training loss [0.16780683 0.12482056 0.18733619 0.12584941]\n",
      "Training loss [0.17028658 0.12483682 0.18718272 0.12558016]\n",
      "Training loss [0.17005402 0.12675573 0.19231984 0.12678222]\n",
      "Training loss [0.16321033 0.12117239 0.18599942 0.12151244]\n",
      "Training loss [0.16543068 0.1219059  0.1844747  0.122436  ]\n",
      "Training loss [0.4174575 0.4285531 0.4472293 0.426908 ]\n",
      "Training loss [0.20735884 0.17819032 0.18427746 0.17803651]\n",
      "Training loss [0.20288417 0.16780336 0.18830873 0.16807356]\n",
      "Training loss [0.19794762 0.15675348 0.18796152 0.15815252]\n",
      "Training loss [0.19783917 0.15316665 0.19082592 0.1545546 ]\n",
      "Training loss [0.18260641 0.14509891 0.18736455 0.14655021]\n",
      "Training loss [0.17552626 0.14079726 0.18386832 0.14170226]\n",
      "Training loss [0.17893928 0.14090857 0.18816395 0.14145648]\n",
      "Training loss [0.18014523 0.1363973  0.18782786 0.13745168]\n",
      "Training loss [0.18388033 0.13866174 0.19071916 0.13954225]\n",
      "Training loss [0.17131436 0.1338408  0.1872915  0.13504344]\n",
      "Training loss [0.16649488 0.13231501 0.1838144  0.13301675]\n",
      "Training loss [0.17034507 0.13446835 0.18811552 0.13494247]\n",
      "Training loss [0.17254145 0.13107994 0.187791   0.13189122]\n",
      "Training loss [0.1771749  0.13483232 0.19066897 0.1352683 ]\n",
      "Training loss [0.16506237 0.13063675 0.18725416 0.1315668 ]\n",
      "Training loss [0.16170244 0.12993051 0.18377754 0.1303601 ]\n",
      "Training loss [0.16518849 0.13234583 0.18807574 0.13272664]\n",
      "Training loss [0.16802865 0.12922005 0.18777204 0.12971213]\n",
      "Training loss [0.17319858 0.13322888 0.19062975 0.1334203 ]\n",
      "Training loss [0.16109847 0.12929986 0.18722185 0.12996942]\n",
      "Training loss [0.15873143 0.12891346 0.18374404 0.12917244]\n",
      "Training loss [0.16191098 0.13126682 0.18804242 0.13162434]\n",
      "Training loss [0.1649875  0.1282674  0.18775713 0.12851453]\n",
      "Training loss [0.17039436 0.13230364 0.19059324 0.13238937]\n",
      "Training loss [0.4056343  0.41639036 0.441175   0.41941643]\n",
      "Training loss [0.20051882 0.172643   0.18231164 0.17244464]\n",
      "Training loss [0.19245574 0.1629869  0.18595988 0.1632185 ]\n",
      "Training loss [0.1906414  0.15682241 0.18398911 0.1572082 ]\n",
      "Training loss [0.18917511 0.15499114 0.1897974  0.15532354]\n",
      "Training loss [0.1821118  0.15108801 0.188647   0.15091455]\n",
      "Training loss [0.16786496 0.13857356 0.18189788 0.13853818]\n",
      "Training loss [0.17031658 0.14049686 0.18570933 0.14042951]\n",
      "Training loss [0.17638241 0.14014237 0.18384433 0.140273  ]\n",
      "Training loss [0.17710298 0.1413683  0.18970451 0.14160025]\n",
      "Training loss [0.1714572  0.13960859 0.18858898 0.13974658]\n",
      "Training loss [0.15842324 0.13039449 0.18185976 0.13030644]\n",
      "Training loss [0.16257109 0.1340492  0.18564928 0.13436303]\n",
      "Training loss [0.17010337 0.13500285 0.18379857 0.13548678]\n",
      "Training loss [0.17102052 0.1372284  0.18967739 0.1374703 ]\n",
      "Training loss [0.1657823  0.13629848 0.18857096 0.13636038]\n",
      "Training loss [0.15374881 0.12758821 0.1818471  0.12760025]\n",
      "Training loss [0.15841842 0.1317809  0.18561885 0.13211706]\n",
      "Training loss [0.1665886  0.13321826 0.18376443 0.13351806]\n",
      "Training loss [0.16732615 0.13561834 0.18966421 0.13585445]\n",
      "Training loss [0.16273704 0.13515057 0.18855932 0.13493705]\n",
      "Training loss [0.15079378 0.12627536 0.18183553 0.12643518]\n",
      "Training loss [0.15580188 0.13066643 0.18559425 0.1309678 ]\n",
      "Training loss [0.1643257  0.13225584 0.18373653 0.13245389]\n",
      "Training loss [0.16495122 0.1347653  0.189652   0.13498813]\n",
      "Training loss [0.40548325 0.41541284 0.4343893  0.41620994]\n",
      "Training loss [0.21572277 0.18373266 0.19363359 0.18343887]\n",
      "Training loss [0.19755784 0.16914937 0.19128475 0.16951782]\n",
      "Training loss [0.19204473 0.16265684 0.1890503  0.16301788]\n",
      "Training loss [0.18750331 0.15522474 0.19080153 0.15527023]\n",
      "Training loss [0.18829885 0.15277126 0.1930471  0.1530559 ]\n",
      "Training loss [0.18249905 0.15076071 0.19317454 0.15117967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.17695317 0.14616467 0.19112632 0.14655055]\n",
      "Training loss [0.17719662 0.14525586 0.18901221 0.14542547]\n",
      "Training loss [0.17421675 0.14237821 0.19075677 0.1428316 ]\n",
      "Training loss [0.17726535 0.14218827 0.19299772 0.14245126]\n",
      "Training loss [0.17388716 0.14294675 0.19312403 0.14327788]\n",
      "Training loss [0.16964169 0.14002207 0.19106096 0.14018694]\n",
      "Training loss [0.17086448 0.14006709 0.18898979 0.1403645 ]\n",
      "Training loss [0.16821422 0.13881232 0.19072954 0.13912216]\n",
      "Training loss [0.17177911 0.1388154  0.19297874 0.13908522]\n",
      "Training loss [0.16935211 0.1402967  0.19308391 0.14035901]\n",
      "Training loss [0.16592343 0.13787682 0.19101176 0.13796471]\n",
      "Training loss [0.16708508 0.13803075 0.18897359 0.13819164]\n",
      "Training loss [0.16465186 0.1373405  0.19070922 0.13754943]\n",
      "Training loss [0.16828401 0.13734409 0.1929613  0.13769153]\n",
      "Training loss [0.16667144 0.13898319 0.19305101 0.13911939]\n",
      "Training loss [0.16361478 0.13679963 0.19096896 0.13697112]\n",
      "Training loss [0.16462845 0.13694778 0.18895869 0.13705121]\n",
      "Training loss [0.16228844 0.13648993 0.19069207 0.136677  ]\n",
      "Training loss [0.39935663 0.41007388 0.42642292 0.40830463]\n",
      "Training loss [0.20246598 0.17782855 0.18650472 0.17785579]\n",
      "Training loss [0.19978385 0.17077613 0.19385046 0.17194279]\n",
      "Training loss [0.18652853 0.16088507 0.18821119 0.16095062]\n",
      "Training loss [0.18617943 0.15646122 0.18848664 0.1568042 ]\n",
      "Training loss [0.17652509 0.1502788  0.18725306 0.15000917]\n",
      "Training loss [0.17079653 0.1461236  0.18628648 0.1466606 ]\n",
      "Training loss [0.17946729 0.14903513 0.19378163 0.14961883]\n",
      "Training loss [0.17221728 0.1456696  0.1881632  0.14565599]\n",
      "Training loss [0.17319384 0.14530633 0.18844832 0.1453998 ]\n",
      "Training loss [0.16646875 0.1413098  0.18719777 0.14103109]\n",
      "Training loss [0.16199999 0.13908595 0.18624562 0.13948801]\n",
      "Training loss [0.17165814 0.14321078 0.19380972 0.1432696 ]\n",
      "Training loss [0.16533966 0.14089414 0.18820259 0.14075895]\n",
      "Training loss [0.1675758  0.14174944 0.18843375 0.14131942]\n",
      "Training loss [0.16190141 0.1382928  0.18717033 0.13784313]\n",
      "Training loss [0.15743305 0.13621828 0.1862224  0.13674971]\n",
      "Training loss [0.16758674 0.14064842 0.19382486 0.14087605]\n",
      "Training loss [0.16186762 0.13883264 0.18821934 0.13874453]\n",
      "Training loss [0.16454053 0.14045337 0.1884234  0.13962659]\n",
      "Training loss [0.15905952 0.13722661 0.18714815 0.13650008]\n",
      "Training loss [0.15388687 0.13506925 0.18620703 0.13547558]\n",
      "Training loss [0.16543102 0.1393494  0.1938205  0.13942856]\n",
      "Training loss [0.15972462 0.13776451 0.18822391 0.13768664]\n",
      "Training loss [0.16233778 0.13956815 0.18841176 0.13876994]\n",
      "Training loss [0.39677465 0.4055402  0.41890728 0.40573156]\n",
      "Training loss [0.20664844 0.17944255 0.1895655  0.18007338]\n",
      "Training loss [0.18701842 0.16426344 0.18250799 0.16371022]\n",
      "Training loss [0.1892648  0.16073573 0.18704322 0.16007674]\n",
      "Training loss [0.18383588 0.15423545 0.18444161 0.15360355]\n",
      "Training loss [0.18102133 0.15293625 0.18704796 0.15336318]\n",
      "Training loss [0.1751971  0.14956358 0.1893266  0.14922929]\n",
      "Training loss [0.16685355 0.14293984 0.18244162 0.14249176]\n",
      "Training loss [0.17427357 0.14553171 0.18696527 0.14541574]\n",
      "Training loss [0.17184052 0.14279746 0.18443298 0.14269304]\n",
      "Training loss [0.17047355 0.14305803 0.1870726  0.1439662 ]\n",
      "Training loss [0.16726245 0.14230022 0.189304   0.14222711]\n",
      "Training loss [0.15865582 0.13689896 0.18243618 0.13682178]\n",
      "Training loss [0.16753069 0.14088774 0.18692976 0.14089969]\n",
      "Training loss [0.16388139 0.13918526 0.18442297 0.13936068]\n",
      "Training loss [0.16526403 0.13945088 0.18708202 0.14056872]\n",
      "Training loss [0.1625385  0.13950261 0.1892865  0.1396468 ]\n",
      "Training loss [0.15497981 0.13503987 0.18242557 0.1347414 ]\n",
      "Training loss [0.16375771 0.13888755 0.18690117 0.13887121]\n",
      "Training loss [0.16059306 0.13772327 0.18441203 0.1379958 ]\n",
      "Training loss [0.1622791  0.1383839  0.18708229 0.13909929]\n",
      "Training loss [0.16008465 0.13843468 0.18926921 0.13850465]\n",
      "Training loss [0.15288262 0.13390365 0.18241523 0.13379383]\n",
      "Training loss [0.16177219 0.13788055 0.18687874 0.13764992]\n",
      "Training loss [0.1584607  0.13682818 0.18439946 0.13720658]\n",
      "Training loss [0.40534592 0.442636   0.45788914 0.44038022]\n",
      "Training loss [0.3089667  0.18204638 0.20359868 0.18136564]\n",
      "Training loss [0.30495673 0.17073882 0.18629938 0.17245375]\n",
      "Training loss [0.27771464 0.15237054 0.17732558 0.1530686 ]\n",
      "Training loss [0.30881077 0.18793356 0.21593621 0.18822047]\n",
      "Training loss [0.29517213 0.15249851 0.2080467  0.15298522]\n",
      "Training loss [0.300954   0.16901761 0.20246252 0.1694641 ]\n",
      "Training loss [0.29858238 0.15700234 0.18824671 0.15789673]\n",
      "Training loss [0.27157533 0.1394831  0.17695989 0.13889726]\n",
      "Training loss [0.30119976 0.17115244 0.21519965 0.16992895]\n",
      "Training loss [0.28795218 0.13839896 0.20813496 0.1379329 ]\n",
      "Training loss [0.2919891  0.15332259 0.2025835  0.15457639]\n",
      "Training loss [0.28975302 0.13829987 0.18863493 0.13833375]\n",
      "Training loss [0.26307902 0.12392126 0.17699035 0.12306232]\n",
      "Training loss [0.2911008  0.15316953 0.21516234 0.151068  ]\n",
      "Training loss [0.27921706 0.12275399 0.20815398 0.12285675]\n",
      "Training loss [0.28123817 0.13721272 0.20259634 0.139316  ]\n",
      "Training loss [0.27984026 0.11994718 0.18870845 0.12135411]\n",
      "Training loss [0.2542115  0.10983347 0.17699555 0.11008357]\n",
      "Training loss [0.2806857  0.13742003 0.21513882 0.13502052]\n",
      "Training loss [0.27145833 0.1089118  0.20812222 0.11057299]\n",
      "Training loss [0.27226722 0.12316434 0.20258686 0.12568484]\n",
      "Training loss [0.2714104  0.10443799 0.18871857 0.10828943]\n",
      "Training loss [0.24679556 0.09757644 0.17699283 0.09865656]\n",
      "Training loss [0.27179217 0.12309125 0.21511677 0.12103269]\n",
      "Training loss [0.38822854 0.38374615 0.43456876 0.38984296]\n",
      "Training loss [0.2859812  0.1429919  0.1619055  0.14075258]\n",
      "Training loss [0.2702505  0.13385722 0.15661174 0.13266262]\n",
      "Training loss [0.27537757 0.13450268 0.1788617  0.13347337]\n",
      "Training loss [0.255041   0.11747482 0.13848476 0.11615302]\n",
      "Training loss [0.26866463 0.12020889 0.15970829 0.1194801 ]\n",
      "Training loss [0.27542073 0.1279192  0.1607176  0.12649362]\n",
      "Training loss [0.26448697 0.12262502 0.15671943 0.12127922]\n",
      "Training loss [0.26737145 0.11909032 0.17891955 0.11885107]\n",
      "Training loss [0.24786456 0.10594706 0.1384695  0.10466643]\n",
      "Training loss [0.2612153  0.10804457 0.15969251 0.10821372]\n",
      "Training loss [0.26711795 0.11560951 0.16071206 0.11508802]\n",
      "Training loss [0.2560664  0.11239776 0.1566991  0.11159845]\n",
      "Training loss [0.25755835 0.10620624 0.17891094 0.10641252]\n",
      "Training loss [0.23932728 0.09602211 0.13845307 0.09425937]\n",
      "Training loss [0.2523883  0.09808653 0.1596824  0.09890012]\n",
      "Training loss [0.25806904 0.10430782 0.16070461 0.10570168]\n",
      "Training loss [0.2480399  0.1012148  0.15667804 0.1031757 ]\n",
      "Training loss [0.24894515 0.09311642 0.17890218 0.09690294]\n",
      "Training loss [0.23224235 0.08545006 0.13843678 0.08533397]\n",
      "Training loss [0.24510095 0.08794941 0.15967073 0.09050283]\n",
      "Training loss [0.25057566 0.09478557 0.16069886 0.09705589]\n",
      "Training loss [0.24202007 0.09224599 0.15665679 0.09309847]\n",
      "Training loss [0.24272361 0.08401023 0.17889348 0.08644971]\n",
      "Training loss [0.22691661 0.07702456 0.13842043 0.07427333]\n",
      "Training loss [0.42328525 0.42719746 0.4836383  0.40097243]\n",
      "Training loss [0.25699848 0.16254848 0.17188033 0.1607224 ]\n",
      "Training loss [0.24995098 0.1528772  0.16806601 0.15331748]\n",
      "Training loss [0.24638975 0.14485383 0.16206917 0.1440584 ]\n",
      "Training loss [0.245017   0.14820275 0.17404613 0.14749397]\n",
      "Training loss [0.24194899 0.1430758  0.16168115 0.14265907]\n",
      "Training loss [0.24552551 0.14491993 0.17069754 0.14374912]\n",
      "Training loss [0.23946181 0.14010328 0.16813734 0.13961166]\n",
      "Training loss [0.2303662  0.12951575 0.16206525 0.12756546]\n",
      "Training loss [0.23317662 0.13448957 0.17402992 0.13374095]\n",
      "Training loss [0.22708295 0.12975442 0.16166313 0.12857695]\n",
      "Training loss [0.23053393 0.13162741 0.17068714 0.12955703]\n",
      "Training loss [0.22730926 0.1278005  0.16812603 0.12611842]\n",
      "Training loss [0.21443118 0.11700763 0.16205946 0.11415124]\n",
      "Training loss [0.22283748 0.1205369  0.17401212 0.12009891]\n",
      "Training loss [0.21415614 0.11580814 0.16164538 0.11549456]\n",
      "Training loss [0.21999018 0.11923113 0.17067665 0.11678074]\n",
      "Training loss [0.21848294 0.11580122 0.16811533 0.11369261]\n",
      "Training loss [0.20428444 0.10581699 0.16205469 0.10308076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.21553081 0.10738377 0.17399445 0.10617073]\n",
      "Training loss [0.20477518 0.10224891 0.16162826 0.10186332]\n",
      "Training loss [0.21315068 0.1073631  0.1706669  0.10451634]\n",
      "Training loss [0.2117379  0.10421991 0.16810468 0.10151609]\n",
      "Training loss [0.19723499 0.09490218 0.16204987 0.09191323]\n",
      "Training loss [0.21010017 0.09720645 0.1739775  0.09564785]\n",
      "Training loss [0.41828105 0.4582813  0.47512254 0.47688678]\n",
      "Training loss [0.3003624  0.1615419  0.22396772 0.16383824]\n",
      "Training loss [0.30097875 0.15501335 0.2219655  0.15559033]\n",
      "Training loss [0.29734474 0.15367118 0.21968602 0.15368259]\n",
      "Training loss [0.29011986 0.15675107 0.21912965 0.15641738]\n",
      "Training loss [0.28806692 0.14251485 0.20738727 0.14390795]\n",
      "Training loss [0.28724945 0.14553571 0.22440818 0.14587383]\n",
      "Training loss [0.29218763 0.14472285 0.2220175  0.14614059]\n",
      "Training loss [0.2855783  0.14415273 0.2196564  0.14506882]\n",
      "Training loss [0.2788585  0.14775752 0.21910125 0.14697544]\n",
      "Training loss [0.27808988 0.13112175 0.2073677  0.13299616]\n",
      "Training loss [0.2756017  0.13515148 0.22441272 0.13624597]\n",
      "Training loss [0.2824306  0.13501833 0.22200345 0.13707562]\n",
      "Training loss [0.27296188 0.13622119 0.21962816 0.13828097]\n",
      "Training loss [0.26748532 0.13913012 0.219072   0.13911402]\n",
      "Training loss [0.26642713 0.12174462 0.20734671 0.12393033]\n",
      "Training loss [0.26419342 0.12614208 0.22441578 0.12743244]\n",
      "Training loss [0.272444   0.12672998 0.22198999 0.1285193 ]\n",
      "Training loss [0.26322064 0.12923217 0.21960224 0.13185948]\n",
      "Training loss [0.2590265  0.13082826 0.21904558 0.1323049 ]\n",
      "Training loss [0.25625628 0.11404255 0.20732844 0.11614799]\n",
      "Training loss [0.2548451  0.1176495  0.22442088 0.11897036]\n",
      "Training loss [0.26349127 0.11898559 0.2219775  0.12079063]\n",
      "Training loss [0.25604978 0.12248473 0.21957871 0.12584172]\n",
      "Training loss [0.25269127 0.1229908  0.21901953 0.12589824]\n",
      "Training loss [0.45834643 0.4726413  0.5245448  0.51031053]\n",
      "Training loss [0.2942677  0.14210437 0.19845995 0.1447601 ]\n",
      "Training loss [0.27521458 0.13479604 0.20105341 0.13557132]\n",
      "Training loss [0.27064618 0.12555507 0.19874729 0.12400119]\n",
      "Training loss [0.27414054 0.12066783 0.20200302 0.12055416]\n",
      "Training loss [0.2626031  0.13461523 0.20000072 0.13401982]\n",
      "Training loss [0.27240175 0.1256575  0.19818501 0.1250611 ]\n",
      "Training loss [0.26023287 0.12694076 0.20102088 0.12626085]\n",
      "Training loss [0.25479743 0.11707063 0.19872798 0.11499174]\n",
      "Training loss [0.26089406 0.1121278  0.20196247 0.11163043]\n",
      "Training loss [0.24538568 0.12487111 0.19996461 0.12302336]\n",
      "Training loss [0.2563791  0.11605413 0.1981816  0.1147188 ]\n",
      "Training loss [0.24509038 0.11736093 0.20099194 0.11539783]\n",
      "Training loss [0.23768394 0.10785654 0.19870922 0.1046767 ]\n",
      "Training loss [0.24594378 0.10207627 0.20192672 0.10169052]\n",
      "Training loss [0.2291401  0.11378879 0.19993398 0.11179334]\n",
      "Training loss [0.242571   0.10676682 0.19817787 0.10611503]\n",
      "Training loss [0.23379493 0.1085931  0.20096505 0.10713734]\n",
      "Training loss [0.22513822 0.10075073 0.1986922  0.09712766]\n",
      "Training loss [0.23386078 0.09440356 0.20189515 0.09448221]\n",
      "Training loss [0.21750964 0.10563552 0.19990149 0.10377303]\n",
      "Training loss [0.23347393 0.09973954 0.19817355 0.09885292]\n",
      "Training loss [0.22576073 0.10173035 0.20093943 0.10026196]\n",
      "Training loss [0.21630916 0.09558231 0.19867478 0.0911988 ]\n",
      "Training loss [0.2252273  0.08869691 0.20186523 0.08792323]\n",
      "Training loss [0.43846792 0.4776882  0.47334883 0.46550328]\n",
      "Training loss [0.26581234 0.16244422 0.18822163 0.15952684]\n",
      "Training loss [0.2729151  0.14949921 0.19379842 0.14875826]\n",
      "Training loss [0.24306363 0.14074473 0.17804721 0.14071633]\n",
      "Training loss [0.23905265 0.13818265 0.17665903 0.13760482]\n",
      "Training loss [0.24530342 0.132765   0.19248894 0.1319325 ]\n",
      "Training loss [0.24679634 0.13841522 0.18840812 0.13777247]\n",
      "Training loss [0.26126033 0.1377183  0.19380155 0.13743697]\n",
      "Training loss [0.22910061 0.13337259 0.17805097 0.13301378]\n",
      "Training loss [0.22549418 0.1295962  0.17664337 0.12885311]\n",
      "Training loss [0.23107307 0.12438653 0.1924631  0.12406905]\n",
      "Training loss [0.2333141  0.13054329 0.18838471 0.12958936]\n",
      "Training loss [0.24753657 0.12925977 0.1938009  0.1276051 ]\n",
      "Training loss [0.21558675 0.12565523 0.17805853 0.12506616]\n",
      "Training loss [0.21239835 0.12100691 0.17663056 0.1197079 ]\n",
      "Training loss [0.22007021 0.11721323 0.19244036 0.11678088]\n",
      "Training loss [0.22264887 0.12270719 0.188362   0.12152745]\n",
      "Training loss [0.23695174 0.12182114 0.19379891 0.11867182]\n",
      "Training loss [0.20622292 0.11837533 0.17806786 0.11760655]\n",
      "Training loss [0.20295309 0.11351652 0.17661978 0.11189971]\n",
      "Training loss [0.21255483 0.11107519 0.19241777 0.11036292]\n",
      "Training loss [0.2147106  0.11645897 0.18834098 0.11532879]\n",
      "Training loss [0.22939518 0.11561231 0.19379622 0.11169586]\n",
      "Training loss [0.19931123 0.11210323 0.17807838 0.1111151 ]\n",
      "Training loss [0.19564588 0.10727387 0.17661038 0.10583796]\n",
      "Training loss [0.45471728 0.47803748 0.48924467 0.4650828 ]\n",
      "Training loss [0.25825015 0.14314458 0.1749807  0.14517431]\n",
      "Training loss [0.25270578 0.12765996 0.1882367  0.12769002]\n",
      "Training loss [0.23166227 0.12201911 0.18004256 0.12307727]\n",
      "Training loss [0.2228438  0.11489421 0.17883188 0.11640018]\n",
      "Training loss [0.23518594 0.12375118 0.19615945 0.1235069 ]\n",
      "Training loss [0.22528228 0.11502329 0.17491862 0.11682979]\n",
      "Training loss [0.23679593 0.11594531 0.18819883 0.11453802]\n",
      "Training loss [0.21804476 0.11504425 0.18004407 0.11510687]\n",
      "Training loss [0.209938   0.10757354 0.178818   0.10757712]\n",
      "Training loss [0.22429553 0.1159007  0.19610614 0.11456473]\n",
      "Training loss [0.21324712 0.10719985 0.17489187 0.10873607]\n",
      "Training loss [0.22435203 0.10875612 0.1881729  0.10504601]\n",
      "Training loss [0.2066149  0.10855762 0.18004435 0.10678656]\n",
      "Training loss [0.19795865 0.10075828 0.17880419 0.09902705]\n",
      "Training loss [0.21340746 0.10908885 0.19605339 0.10592216]\n",
      "Training loss [0.2024575  0.10016635 0.17487107 0.10083105]\n",
      "Training loss [0.21392861 0.10300924 0.18814912 0.09745853]\n",
      "Training loss [0.19712052 0.10255718 0.1800447  0.09978276]\n",
      "Training loss [0.18937273 0.09473552 0.17879349 0.09183576]\n",
      "Training loss [0.20449857 0.10337908 0.19600432 0.09865959]\n",
      "Training loss [0.19404325 0.0947196  0.17485501 0.09364775]\n",
      "Training loss [0.2061749  0.09718879 0.18812716 0.09219087]\n",
      "Training loss [0.19007033 0.09620761 0.18004246 0.09355523]\n",
      "Training loss [0.18298379 0.08931696 0.17878494 0.0866228 ]\n",
      "Training loss [0.46468997 0.47906947 0.5453073  0.4784456 ]\n",
      "Training loss [0.26945615 0.16160397 0.20025074 0.16502994]\n",
      "Training loss [0.26354727 0.14187992 0.21135974 0.14366232]\n",
      "Training loss [0.24444056 0.12882817 0.19812405 0.12817085]\n",
      "Training loss [0.23574877 0.12529312 0.20009264 0.12482744]\n",
      "Training loss [0.26067215 0.13095814 0.212876   0.13063379]\n",
      "Training loss [0.23688434 0.1241376  0.20038821 0.12413541]\n",
      "Training loss [0.2450589  0.12843448 0.2113352  0.12920153]\n",
      "Training loss [0.22640961 0.12210636 0.19813561 0.12004902]\n",
      "Training loss [0.21979006 0.11869048 0.20006222 0.11795253]\n",
      "Training loss [0.24615787 0.12556535 0.21280631 0.12495506]\n",
      "Training loss [0.22374871 0.11808923 0.20034713 0.1181042 ]\n",
      "Training loss [0.23188967 0.12310158 0.21131507 0.12340823]\n",
      "Training loss [0.21316984 0.1166338  0.1981411  0.11451486]\n",
      "Training loss [0.20785384 0.11223505 0.20003927 0.11239911]\n",
      "Training loss [0.23426    0.12057134 0.21274683 0.12048839]\n",
      "Training loss [0.21458563 0.11317457 0.20031312 0.11324424]\n",
      "Training loss [0.22196878 0.11798727 0.21129715 0.11876357]\n",
      "Training loss [0.20408404 0.11153885 0.19814655 0.10924073]\n",
      "Training loss [0.2001637  0.10755236 0.20002076 0.10821227]\n",
      "Training loss [0.22590283 0.11675125 0.21269427 0.11638607]\n",
      "Training loss [0.20830454 0.10962597 0.20028622 0.10891167]\n",
      "Training loss [0.21494727 0.1139507  0.21127893 0.11436346]\n",
      "Training loss [0.19779104 0.10645143 0.19815174 0.10516098]\n",
      "Training loss [0.19511041 0.10417241 0.20000437 0.10469839]\n",
      "Training loss [0.46596605 0.48874682 0.5312621  0.48181805]\n",
      "Training loss [0.26092532 0.16838045 0.19175023 0.16825631]\n",
      "Training loss [0.24258277 0.13935888 0.18684697 0.13933028]\n",
      "Training loss [0.23438624 0.13127731 0.19473487 0.1313366 ]\n",
      "Training loss [0.2230203  0.12627734 0.1936886  0.12745963]\n",
      "Training loss [0.2263608  0.12106033 0.19153282 0.12206131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.2223373  0.12447572 0.19172978 0.12404157]\n",
      "Training loss [0.22008975 0.12361489 0.18682915 0.12347721]\n",
      "Training loss [0.21528837 0.1221808  0.19468425 0.12219482]\n",
      "Training loss [0.20714226 0.12059322 0.19364816 0.12118744]\n",
      "Training loss [0.21013835 0.11574745 0.19150336 0.11687677]\n",
      "Training loss [0.20685327 0.11930698 0.1916995  0.11910452]\n",
      "Training loss [0.20571682 0.11909816 0.18681344 0.11914412]\n",
      "Training loss [0.20160747 0.11721574 0.19464397 0.11733331]\n",
      "Training loss [0.19672106 0.11669166 0.19362196 0.11717382]\n",
      "Training loss [0.20033708 0.11137602 0.19147885 0.11275186]\n",
      "Training loss [0.19760385 0.11518166 0.19167288 0.11529038]\n",
      "Training loss [0.19684419 0.11504942 0.18679996 0.1152114 ]\n",
      "Training loss [0.19345266 0.11273327 0.19461142 0.1132495 ]\n",
      "Training loss [0.19041342 0.11310934 0.19360186 0.11349384]\n",
      "Training loss [0.19394167 0.10750124 0.19145672 0.10918407]\n",
      "Training loss [0.1918641  0.11154354 0.19165021 0.11232787]\n",
      "Training loss [0.19091323 0.1117438  0.18678734 0.11165011]\n",
      "Training loss [0.18846712 0.10929574 0.19458258 0.10998578]\n",
      "Training loss [0.18620351 0.10995091 0.19358368 0.11023454]\n",
      "Training loss [0.4514907  0.46505094 0.5356096  0.46207452]\n",
      "Training loss [0.24142277 0.16454214 0.17734846 0.16592488]\n",
      "Training loss [0.20785421 0.13255942 0.17096092 0.1303496 ]\n",
      "Training loss [0.20332381 0.1219622  0.17151532 0.12179713]\n",
      "Training loss [0.20184499 0.12187335 0.17658117 0.12147993]\n",
      "Training loss [0.19219527 0.11405488 0.17471528 0.11397043]\n",
      "Training loss [0.20406336 0.1122479  0.17736787 0.11244346]\n",
      "Training loss [0.18510222 0.11258744 0.17090711 0.11256973]\n",
      "Training loss [0.18584305 0.1109118  0.1714324  0.11092083]\n",
      "Training loss [0.18616644 0.11483492 0.17651688 0.11495201]\n",
      "Training loss [0.17738016 0.10738081 0.17466027 0.10781007]\n",
      "Training loss [0.1918156  0.10738949 0.17735112 0.10652307]\n",
      "Training loss [0.173713   0.10879847 0.17087263 0.10912217]\n",
      "Training loss [0.1753951  0.10786229 0.17136967 0.10703674]\n",
      "Training loss [0.1766426  0.11133425 0.17646706 0.11130691]\n",
      "Training loss [0.16872028 0.10387718 0.1746197  0.10408252]\n",
      "Training loss [0.1837547  0.10412723 0.1773388  0.10271749]\n",
      "Training loss [0.1665946  0.10564566 0.17084497 0.10618751]\n",
      "Training loss [0.16940916 0.10512324 0.17132221 0.10390669]\n",
      "Training loss [0.1705489  0.10840416 0.17642702 0.10818274]\n",
      "Training loss [0.16336659 0.10103098 0.17458864 0.10087232]\n",
      "Training loss [0.17810616 0.10125849 0.17732772 0.09992733]\n",
      "Training loss [0.16201288 0.10282581 0.1708234  0.10357598]\n",
      "Training loss [0.16561772 0.10259445 0.17128289 0.10128881]\n",
      "Training loss [0.166381   0.10602722 0.17639235 0.10553467]\n",
      "Training loss [0.44640383 0.4723205  0.48958492 0.46929544]\n",
      "Training loss [0.23674646 0.16576096 0.17906779 0.16674514]\n",
      "Training loss [0.21731779 0.135535   0.178771   0.13645944]\n",
      "Training loss [0.19483042 0.12015478 0.17706105 0.11997332]\n",
      "Training loss [0.20174277 0.11958611 0.17936856 0.11983089]\n",
      "Training loss [0.20061973 0.12025888 0.18318164 0.12117215]\n",
      "Training loss [0.20090042 0.11575466 0.1789477  0.11607884]\n",
      "Training loss [0.19188422 0.11048552 0.17869183 0.11095212]\n",
      "Training loss [0.17799757 0.10604833 0.17696114 0.10552119]\n",
      "Training loss [0.18788965 0.10991983 0.1793537  0.10990892]\n",
      "Training loss [0.18720376 0.11229606 0.18316185 0.11211482]\n",
      "Training loss [0.18935522 0.11026583 0.17886248 0.11024906]\n",
      "Training loss [0.18050861 0.10571457 0.17865346 0.10620135]\n",
      "Training loss [0.16890374 0.10217969 0.1768976  0.10142589]\n",
      "Training loss [0.17979814 0.1060179  0.17934786 0.10626525]\n",
      "Training loss [0.17851621 0.10905805 0.18313876 0.10880209]\n",
      "Training loss [0.18181223 0.10735103 0.17880803 0.10769793]\n",
      "Training loss [0.17380607 0.10354656 0.17862439 0.1037471 ]\n",
      "Training loss [0.16330016 0.09976473 0.17685063 0.09897555]\n",
      "Training loss [0.17457795 0.10360992 0.17934223 0.10415253]\n",
      "Training loss [0.17294908 0.10674173 0.18311298 0.10654069]\n",
      "Training loss [0.17645285 0.10516962 0.17876905 0.10592388]\n",
      "Training loss [0.16929686 0.10194105 0.17859918 0.1019778 ]\n",
      "Training loss [0.1594732  0.09779625 0.17681281 0.09718596]\n",
      "Training loss [0.17075922 0.10178772 0.17933536 0.10244078]\n",
      "Training loss [0.44921964 0.47911566 0.50340843 0.45444056]\n",
      "Training loss [0.24199384 0.17601424 0.19066493 0.17746578]\n",
      "Training loss [0.2320756  0.1545743  0.19670941 0.15522337]\n",
      "Training loss [0.22656694 0.14198866 0.19914672 0.14277601]\n",
      "Training loss [0.21112096 0.13010421 0.19353509 0.1290043 ]\n",
      "Training loss [0.20655823 0.1267098  0.19449419 0.12696771]\n",
      "Training loss [0.19670737 0.12081306 0.1905705  0.1210847 ]\n",
      "Training loss [0.20460258 0.12348178 0.19665319 0.12364036]\n",
      "Training loss [0.20592773 0.12192427 0.19907871 0.12165596]\n",
      "Training loss [0.19446103 0.11689119 0.19350027 0.11631858]\n",
      "Training loss [0.19253366 0.11635834 0.1944158  0.11706921]\n",
      "Training loss [0.18305677 0.11425979 0.1905611  0.11523495]\n",
      "Training loss [0.19207093 0.11804195 0.19663212 0.11852164]\n",
      "Training loss [0.19721603 0.11762208 0.19904315 0.11712322]\n",
      "Training loss [0.18624412 0.11321783 0.19348215 0.1131339 ]\n",
      "Training loss [0.18563452 0.11331658 0.19435921 0.11393182]\n",
      "Training loss [0.17531124 0.11193968 0.19054973 0.11285825]\n",
      "Training loss [0.18528421 0.11561072 0.1966217  0.11627628]\n",
      "Training loss [0.19209498 0.11553788 0.19901726 0.11487515]\n",
      "Training loss [0.18124144 0.11149764 0.19346909 0.11139126]\n",
      "Training loss [0.18143697 0.11148416 0.19431168 0.1119038 ]\n",
      "Training loss [0.17052607 0.11054602 0.19053566 0.1111531 ]\n",
      "Training loss [0.18100081 0.11394948 0.19661349 0.11450435]\n",
      "Training loss [0.18834877 0.11433598 0.19899452 0.11335766]\n",
      "Training loss [0.17777328 0.11031815 0.19345772 0.11004651]\n",
      "Training loss [0.4471733  0.4843978  0.5035241  0.47019956]\n",
      "Training loss [0.23810107 0.18257284 0.19394    0.18339488]\n",
      "Training loss [0.23275295 0.16276349 0.20359348 0.16377838]\n",
      "Training loss [0.2073512  0.14130262 0.19663678 0.14021082]\n",
      "Training loss [0.2040818  0.13065791 0.1977711  0.13036478]\n",
      "Training loss [0.21661486 0.1359835  0.20534058 0.13400015]\n",
      "Training loss [0.19866455 0.12188745 0.19377492 0.12226837]\n",
      "Training loss [0.20743039 0.12739429 0.20348689 0.1269409 ]\n",
      "Training loss [0.19160035 0.11837646 0.19647577 0.11669361]\n",
      "Training loss [0.18942851 0.11432033 0.19770226 0.11436465]\n",
      "Training loss [0.20498018 0.12259306 0.20522226 0.12186981]\n",
      "Training loss [0.18761879 0.11373703 0.19369671 0.11374041]\n",
      "Training loss [0.19650863 0.12109401 0.20343968 0.12065517]\n",
      "Training loss [0.18378222 0.1131684  0.19638737 0.11219579]\n",
      "Training loss [0.18109997 0.11049758 0.19766778 0.11092905]\n",
      "Training loss [0.19826916 0.11926935 0.20516142 0.11872375]\n",
      "Training loss [0.18120024 0.11119995 0.1936543  0.11132722]\n",
      "Training loss [0.19017337 0.11910568 0.2034045  0.11864844]\n",
      "Training loss [0.17862175 0.11117765 0.19632572 0.11036941]\n",
      "Training loss [0.17572284 0.10910222 0.19764444 0.10946292]\n",
      "Training loss [0.19391549 0.11779375 0.2051215  0.11714963]\n",
      "Training loss [0.1769689  0.1098974  0.19362393 0.11009917]\n",
      "Training loss [0.18616165 0.11804433 0.20337522 0.11748178]\n",
      "Training loss [0.17503572 0.11008347 0.19627607 0.10916135]\n",
      "Training loss [0.17210796 0.10818373 0.19762455 0.10845807]\n",
      "Training loss [0.43633062 0.44834018 0.47839823 0.44679058]\n",
      "Training loss [0.21937631 0.17736357 0.18341202 0.17896193]\n",
      "Training loss [0.2099365  0.16080156 0.18937592 0.16076523]\n",
      "Training loss [0.19285017 0.1421099  0.1830547  0.14190537]\n",
      "Training loss [0.19953702 0.1399633  0.19116774 0.14091086]\n",
      "Training loss [0.19189888 0.1366082  0.19028574 0.1363754 ]\n",
      "Training loss [0.18162234 0.12815069 0.1832161  0.12857096]\n",
      "Training loss [0.1851921  0.12726234 0.18926373 0.12676772]\n",
      "Training loss [0.17791772 0.11955892 0.18294024 0.11914048]\n",
      "Training loss [0.18663678 0.12407091 0.19108109 0.12496797]\n",
      "Training loss [0.18101294 0.12392809 0.19018556 0.12346407]\n",
      "Training loss [0.17168243 0.1203073  0.18312538 0.12014179]\n",
      "Training loss [0.17599756 0.12100925 0.18922156 0.12057394]\n",
      "Training loss [0.17071609 0.11449672 0.18288538 0.11400766]\n",
      "Training loss [0.17950496 0.11978268 0.1910287  0.12064892]\n",
      "Training loss [0.17550565 0.12025182 0.19011939 0.12002354]\n",
      "Training loss [0.16633494 0.11813292 0.18307272 0.1175055 ]\n",
      "Training loss [0.17065684 0.11850397 0.18919286 0.11830135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.16593534 0.11253976 0.18284798 0.11212198]\n",
      "Training loss [0.17490532 0.11797864 0.19099025 0.11869832]\n",
      "Training loss [0.17189291 0.11863244 0.19006701 0.11851986]\n",
      "Training loss [0.16279995 0.11704509 0.18303339 0.11619727]\n",
      "Training loss [0.16723548 0.11704619 0.18916811 0.11701937]\n",
      "Training loss [0.16248095 0.11143272 0.18281801 0.11109508]\n",
      "Training loss [0.17164886 0.11698209 0.19096124 0.11756554]\n",
      "Training loss [0.41541156 0.43459082 0.4714942  0.44010493]\n",
      "Training loss [0.21564484 0.16962254 0.17580189 0.17013873]\n",
      "Training loss [0.19733566 0.15675148 0.17922595 0.15731072]\n",
      "Training loss [0.19443807 0.14558834 0.17887443 0.14575255]\n",
      "Training loss [0.176482   0.13154197 0.17076293 0.13050847]\n",
      "Training loss [0.17512092 0.12810996 0.17446084 0.1279242 ]\n",
      "Training loss [0.173163   0.12459841 0.17548308 0.12370093]\n",
      "Training loss [0.17199084 0.12661526 0.17909536 0.12702295]\n",
      "Training loss [0.17645139 0.12446521 0.17876731 0.12444645]\n",
      "Training loss [0.1631732  0.1166629  0.17070623 0.11542739]\n",
      "Training loss [0.16421187 0.11604883 0.17436486 0.11592607]\n",
      "Training loss [0.16299766 0.11518312 0.17538983 0.11433588]\n",
      "Training loss [0.16315575 0.11999562 0.17904487 0.12024921]\n",
      "Training loss [0.16827774 0.11886027 0.1787298  0.11939079]\n",
      "Training loss [0.15627928 0.11232097 0.17068687 0.11182354]\n",
      "Training loss [0.15777932 0.11264085 0.17431712 0.11265266]\n",
      "Training loss [0.15759432 0.1121419  0.17533489 0.11150462]\n",
      "Training loss [0.15779425 0.1178453  0.17900753 0.11786414]\n",
      "Training loss [0.16343012 0.11679158 0.17871049 0.11740331]\n",
      "Training loss [0.15217683 0.1105098  0.17067008 0.11036922]\n",
      "Training loss [0.15418804 0.11110467 0.17428334 0.11136964]\n",
      "Training loss [0.15423818 0.11066125 0.17529374 0.11029243]\n",
      "Training loss [0.15464303 0.11681899 0.17897412 0.11667469]\n",
      "Training loss [0.16033433 0.11575289 0.17869686 0.11629501]\n",
      "Training loss [0.14963052 0.10947012 0.1706534  0.10953709]\n",
      "Training loss [0.41656673 0.43635237 0.45543098 0.44327125]\n",
      "Training loss [0.21490383 0.17254905 0.17871812 0.17240022]\n",
      "Training loss [0.20153177 0.16393325 0.18200816 0.16354315]\n",
      "Training loss [0.19213694 0.15208207 0.18118958 0.15080324]\n",
      "Training loss [0.19117244 0.1432862  0.18006754 0.14325796]\n",
      "Training loss [0.18193024 0.13990845 0.18568972 0.1393731 ]\n",
      "Training loss [0.17422308 0.12933294 0.17827879 0.12918913]\n",
      "Training loss [0.17970736 0.13143285 0.18172306 0.13088113]\n",
      "Training loss [0.1760806  0.12926492 0.18101737 0.12868272]\n",
      "Training loss [0.17745878 0.12692899 0.18001358 0.12729324]\n",
      "Training loss [0.17089677 0.12646776 0.18557191 0.12624511]\n",
      "Training loss [0.16412282 0.11910051 0.17819636 0.1197243 ]\n",
      "Training loss [0.17088154 0.1233532  0.18166085 0.12255625]\n",
      "Training loss [0.16904327 0.12365912 0.1809725  0.12295301]\n",
      "Training loss [0.17015418 0.12245154 0.17997876 0.12258327]\n",
      "Training loss [0.16488567 0.12261632 0.1855146  0.12270939]\n",
      "Training loss [0.15919285 0.11608669 0.17815322 0.1166601 ]\n",
      "Training loss [0.16608582 0.12093674 0.18163776 0.11994725]\n",
      "Training loss [0.16454926 0.1216895  0.18095207 0.12087223]\n",
      "Training loss [0.16578752 0.12076408 0.17995137 0.12065612]\n",
      "Training loss [0.1607861  0.12102202 0.18547544 0.12109041]\n",
      "Training loss [0.15589046 0.1148039  0.17811912 0.1153542 ]\n",
      "Training loss [0.1630444  0.11982933 0.18162528 0.11870046]\n",
      "Training loss [0.16126175 0.12059063 0.18094358 0.1197595 ]\n",
      "Training loss [0.16292655 0.11977105 0.17992812 0.11960085]\n",
      "Training loss [0.4106028  0.42464426 0.44158554 0.42541   ]\n",
      "Training loss [0.21206415 0.17692712 0.1828826  0.17724241]\n",
      "Training loss [0.1994834  0.16452566 0.18601257 0.16485503]\n",
      "Training loss [0.18592674 0.14897183 0.17989087 0.14884283]\n",
      "Training loss [0.19410017 0.1515619  0.18716948 0.15139472]\n",
      "Training loss [0.19290435 0.14987555 0.18976352 0.15038192]\n",
      "Training loss [0.18036138 0.14090295 0.18243714 0.14061934]\n",
      "Training loss [0.17856301 0.13731834 0.18573137 0.1387093 ]\n",
      "Training loss [0.1686998  0.1310602  0.17970625 0.13028799]\n",
      "Training loss [0.18089113 0.1365568  0.18705639 0.13723429]\n",
      "Training loss [0.18232678 0.13793947 0.1896815  0.13819912]\n",
      "Training loss [0.1704177  0.13137966 0.18236732 0.13148317]\n",
      "Training loss [0.17092225 0.13005614 0.18568148 0.13100101]\n",
      "Training loss [0.16183114 0.12609364 0.17967552 0.12530649]\n",
      "Training loss [0.17473508 0.13255648 0.18701258 0.1329323 ]\n",
      "Training loss [0.17603147 0.13462014 0.18965632 0.13488051]\n",
      "Training loss [0.16458242 0.1282964  0.1823362  0.12856454]\n",
      "Training loss [0.1665279  0.12771517 0.18564825 0.12845917]\n",
      "Training loss [0.15760827 0.12419359 0.17966218 0.12348129]\n",
      "Training loss [0.17104848 0.13091987 0.18697587 0.13113452]\n",
      "Training loss [0.17190832 0.13324825 0.18964285 0.13361138]\n",
      "Training loss [0.1610693  0.12682652 0.18232042 0.12717316]\n",
      "Training loss [0.16353182 0.12663934 0.1856159  0.12728153]\n",
      "Training loss [0.15470187 0.1230989  0.17965026 0.12251729]\n",
      "Training loss [0.16838564 0.13000305 0.18694112 0.13009754]\n",
      "Training loss [0.4051313  0.42020825 0.4410245  0.42270476]\n",
      "Training loss [0.2123247  0.17956087 0.18830314 0.18052176]\n",
      "Training loss [0.19319162 0.16522637 0.18408404 0.16517079]\n",
      "Training loss [0.18470573 0.15678994 0.18488616 0.15679921]\n",
      "Training loss [0.1932264  0.15602696 0.19080222 0.15662621]\n",
      "Training loss [0.17799005 0.14617    0.18596502 0.14583029]\n",
      "Training loss [0.17809647 0.1452694  0.18793318 0.14485201]\n",
      "Training loss [0.17325146 0.14150468 0.18394321 0.14120266]\n",
      "Training loss [0.16945352 0.13831156 0.18481123 0.13846162]\n",
      "Training loss [0.18106487 0.14213145 0.19074136 0.14181054]\n",
      "Training loss [0.16780028 0.13589898 0.18589088 0.1356079 ]\n",
      "Training loss [0.16839455 0.136149   0.18785952 0.1360243 ]\n",
      "Training loss [0.1653803  0.13450882 0.18389045 0.1341109 ]\n",
      "Training loss [0.16266271 0.13267708 0.18479675 0.13313977]\n",
      "Training loss [0.17456259 0.13798085 0.19071344 0.13742097]\n",
      "Training loss [0.16209893 0.13216923 0.18585928 0.13195641]\n",
      "Training loss [0.16317119 0.13310781 0.18780865 0.13260853]\n",
      "Training loss [0.16091965 0.13233498 0.18385309 0.13168487]\n",
      "Training loss [0.15870285 0.13090296 0.18479025 0.13104406]\n",
      "Training loss [0.17028299 0.13639052 0.19068623 0.13578375]\n",
      "Training loss [0.15890431 0.13070248 0.18583186 0.13036935]\n",
      "Training loss [0.1599102  0.13167988 0.18776628 0.1311808 ]\n",
      "Training loss [0.15793681 0.13108806 0.1838224  0.13059041]\n",
      "Training loss [0.1561073  0.13000718 0.1847853  0.12989593]\n",
      "Training loss [0.1671651  0.13539428 0.19065878 0.1349784 ]\n",
      "Training loss [0.40865076 0.41982716 0.42820257 0.41677195]\n",
      "Training loss [0.207489   0.18327063 0.18991421 0.18324056]\n",
      "Training loss [0.19836567 0.16911182 0.18887994 0.16808476]\n",
      "Training loss [0.1909028  0.16205883 0.18976715 0.16165853]\n",
      "Training loss [0.18612936 0.15557119 0.18854707 0.15529397]\n",
      "Training loss [0.18341918 0.15147337 0.18843755 0.1510159 ]\n",
      "Training loss [0.17702727 0.15140362 0.18977489 0.15044327]\n",
      "Training loss [0.17809236 0.14528987 0.18875623 0.14477488]\n",
      "Training loss [0.17511106 0.14509502 0.18968044 0.14509179]\n",
      "Training loss [0.17264375 0.1425263  0.18852782 0.1426717 ]\n",
      "Training loss [0.17320192 0.14127454 0.18840453 0.14075218]\n",
      "Training loss [0.16883591 0.14313069 0.18975712 0.14225732]\n",
      "Training loss [0.17045465 0.13866177 0.18871458 0.13850915]\n",
      "Training loss [0.1687466  0.13995339 0.18963736 0.13993707]\n",
      "Training loss [0.16674261 0.13832076 0.18852825 0.1387335 ]\n",
      "Training loss [0.1680778  0.13738874 0.18839855 0.13736138]\n",
      "Training loss [0.1644014  0.14013177 0.18973818 0.13961017]\n",
      "Training loss [0.16630726 0.1363191  0.18868671 0.13631606]\n",
      "Training loss [0.16543293 0.13801807 0.18959814 0.13801725]\n",
      "Training loss [0.163453   0.1367116  0.18852517 0.13699293]\n",
      "Training loss [0.16497545 0.13575746 0.1883905  0.13563444]\n",
      "Training loss [0.1617508  0.13866183 0.18971553 0.13824898]\n",
      "Training loss [0.16368078 0.13521461 0.18866035 0.13527745]\n",
      "Training loss [0.1631525  0.13695279 0.18956284 0.13718235]\n",
      "Training loss [0.1613261  0.13597246 0.1885196  0.13601604]\n",
      "Training loss [0.39769244 0.40691277 0.41677868 0.4069504 ]\n",
      "Training loss [0.20778367 0.18259181 0.19053933 0.18225226]\n",
      "Training loss [0.20046693 0.17088684 0.1875464  0.17057547]\n",
      "Training loss [0.18744978 0.15640754 0.18554005 0.15655152]\n",
      "Training loss [0.18121594 0.15383041 0.1860544  0.15437555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.17318289 0.14985006 0.18532893 0.15012136]\n",
      "Training loss [0.18078741 0.15146849 0.190155   0.15184167]\n",
      "Training loss [0.18101376 0.15033567 0.1874107  0.15044913]\n",
      "Training loss [0.17021653 0.14137623 0.18549064 0.14145799]\n",
      "Training loss [0.16918328 0.14228517 0.18603653 0.1421562 ]\n",
      "Training loss [0.1642417  0.14093736 0.18532488 0.14112633]\n",
      "Training loss [0.17176709 0.14367582 0.19015127 0.14420748]\n",
      "Training loss [0.17425549 0.14474466 0.1873856  0.14482078]\n",
      "Training loss [0.16304068 0.13701299 0.18546994 0.13687304]\n",
      "Training loss [0.16312078 0.13871999 0.18605918 0.13835737]\n",
      "Training loss [0.15978588 0.13760224 0.1853309  0.13816322]\n",
      "Training loss [0.16737258 0.14105204 0.19014466 0.14119536]\n",
      "Training loss [0.17077072 0.14246902 0.18736267 0.14261994]\n",
      "Training loss [0.15908685 0.13517329 0.18544444 0.13516785]\n",
      "Training loss [0.15969574 0.13680619 0.18607125 0.13673912]\n",
      "Training loss [0.15735695 0.13625017 0.1853297  0.13671276]\n",
      "Training loss [0.16472644 0.13989596 0.19012687 0.1398688 ]\n",
      "Training loss [0.16918957 0.14126748 0.18733516 0.14152429]\n",
      "Training loss [0.15649155 0.13425058 0.18541665 0.13424246]\n",
      "Training loss [0.15754056 0.13585362 0.18606731 0.13591936]\n",
      "Training loss [0.3938104  0.4015726  0.4172979  0.40256864]\n",
      "Training loss [0.20795548 0.18122706 0.1914253  0.18086803]\n",
      "Training loss [0.19762617 0.16759786 0.1878081  0.16744149]\n",
      "Training loss [0.1894609  0.16343832 0.1919464  0.16323867]\n",
      "Training loss [0.18663585 0.16058797 0.19077769 0.15981698]\n",
      "Training loss [0.17586559 0.15211669 0.18842244 0.15228947]\n",
      "Training loss [0.17641039 0.15204763 0.19126356 0.15141697]\n",
      "Training loss [0.17770138 0.1482226  0.18777199 0.14807507]\n",
      "Training loss [0.17412442 0.14948127 0.1918949  0.14936386]\n",
      "Training loss [0.1740984  0.14887613 0.1907856  0.14901184]\n",
      "Training loss [0.16637129 0.14359018 0.18845278 0.1435542 ]\n",
      "Training loss [0.16715698 0.14468071 0.19122782 0.14442798]\n",
      "Training loss [0.17060438 0.14292084 0.18776825 0.14285357]\n",
      "Training loss [0.16779414 0.1451199  0.19187196 0.1454142 ]\n",
      "Training loss [0.16851436 0.14473693 0.19079807 0.14542331]\n",
      "Training loss [0.16200362 0.14066285 0.18846425 0.14045155]\n",
      "Training loss [0.1619739  0.14194056 0.19120333 0.14175735]\n",
      "Training loss [0.16724497 0.14054802 0.18776263 0.14064303]\n",
      "Training loss [0.16431071 0.14332896 0.19184726 0.14363176]\n",
      "Training loss [0.16519095 0.14306214 0.19080107 0.14351854]\n",
      "Training loss [0.15957403 0.13928157 0.18846035 0.13921493]\n",
      "Training loss [0.15903655 0.14079353 0.1911861  0.14034086]\n",
      "Training loss [0.16456938 0.13935392 0.18775369 0.13964179]\n",
      "Training loss [0.16232827 0.14238125 0.19182444 0.14254613]\n",
      "Training loss [0.16322005 0.14223978 0.19079912 0.14266971]\n",
      "Training loss [0.34913623 0.33570215 0.3664733  0.33313015]\n",
      "Training loss [0.22813459 0.15553743 0.14491785 0.15338199]\n",
      "Training loss [0.24242327 0.16511041 0.15047693 0.16346498]\n",
      "Training loss [0.24658921 0.17070898 0.13523108 0.16795175]\n",
      "Training loss [0.24689713 0.16010904 0.14693291 0.15623173]\n",
      "Training loss [0.20164306 0.14864182 0.1253932  0.14604652]\n",
      "Training loss [0.22038484 0.13774894 0.1382294  0.1320028 ]\n",
      "Training loss [0.23525116 0.14557755 0.14933887 0.14056458]\n",
      "Training loss [0.24093065 0.15286322 0.13446727 0.14664651]\n",
      "Training loss [0.24069169 0.13849421 0.14720425 0.13342437]\n",
      "Training loss [0.19193193 0.12660447 0.12522012 0.12400793]\n",
      "Training loss [0.21173778 0.11342233 0.13814571 0.10855372]\n",
      "Training loss [0.22514498 0.12077574 0.14933029 0.11713655]\n",
      "Training loss [0.23410676 0.13099317 0.13444082 0.12448329]\n",
      "Training loss [0.23390317 0.11849692 0.14719552 0.11376699]\n",
      "Training loss [0.18151543 0.10772181 0.12519304 0.10587691]\n",
      "Training loss [0.2027236  0.0953913  0.13814369 0.09071699]\n",
      "Training loss [0.21596014 0.10349177 0.14931867 0.10006932]\n",
      "Training loss [0.22767463 0.11230244 0.13443272 0.10540967]\n",
      "Training loss [0.22840093 0.10316012 0.14717701 0.09807305]\n",
      "Training loss [0.17365946 0.09144242 0.12517382 0.09103428]\n",
      "Training loss [0.19545482 0.0810313  0.13814571 0.07721501]\n",
      "Training loss [0.2092233  0.08834133 0.14930701 0.08576868]\n",
      "Training loss [0.22205421 0.09280476 0.13442567 0.08889335]\n",
      "Training loss [0.22384807 0.08792477 0.14715862 0.08367264]\n",
      "Training loss [0.44695017 0.4454465  0.46518523 0.4280601 ]\n",
      "Training loss [0.28744453 0.15625179 0.18363032 0.15513866]\n",
      "Training loss [0.3148218  0.15827933 0.20504098 0.15979314]\n",
      "Training loss [0.2839988  0.15246558 0.16637659 0.15237632]\n",
      "Training loss [0.29960293 0.1529895  0.18079028 0.15150633]\n",
      "Training loss [0.2974314  0.15695831 0.17819177 0.155512  ]\n",
      "Training loss [0.27817336 0.14190519 0.18041602 0.13983807]\n",
      "Training loss [0.3094902  0.14949502 0.20484272 0.14936517]\n",
      "Training loss [0.27623957 0.14173365 0.16635846 0.1393976 ]\n",
      "Training loss [0.29160866 0.14163898 0.18069701 0.1386287 ]\n",
      "Training loss [0.29038924 0.14480741 0.17822865 0.14168897]\n",
      "Training loss [0.26904964 0.12760192 0.18040863 0.12548134]\n",
      "Training loss [0.30201155 0.13851039 0.20483696 0.1388072 ]\n",
      "Training loss [0.2651034  0.12872812 0.16634995 0.12654012]\n",
      "Training loss [0.28008902 0.12829837 0.18069538 0.12596801]\n",
      "Training loss [0.28132454 0.12998512 0.1782178  0.1280528 ]\n",
      "Training loss [0.25816485 0.11078636 0.1803969  0.1123933 ]\n",
      "Training loss [0.29321522 0.1288242  0.20483205 0.131578  ]\n",
      "Training loss [0.2531944  0.11222679 0.16634056 0.1112698 ]\n",
      "Training loss [0.26863313 0.11087498 0.18069538 0.10864149]\n",
      "Training loss [0.27333015 0.11237972 0.17820588 0.11120797]\n",
      "Training loss [0.24911994 0.09736488 0.18038486 0.09559105]\n",
      "Training loss [0.28629345 0.11693456 0.20482798 0.11799935]\n",
      "Training loss [0.24393085 0.10114361 0.16633141 0.09791475]\n",
      "Training loss [0.2595016  0.09958219 0.1806953  0.09562645]\n",
      "Training loss [0.4299098  0.4251296  0.5029795  0.46229762]\n",
      "Training loss [0.26822805 0.12233932 0.14053783 0.12357242]\n",
      "Training loss [0.2827329  0.11854085 0.16272637 0.11815439]\n",
      "Training loss [0.2850814  0.12318404 0.16157731 0.1230695 ]\n",
      "Training loss [0.24995425 0.10898716 0.1331222  0.10991891]\n",
      "Training loss [0.28254277 0.11387318 0.16115037 0.11386546]\n",
      "Training loss [0.2511668  0.10315204 0.14051795 0.10431498]\n",
      "Training loss [0.27306002 0.10118014 0.16270483 0.10149492]\n",
      "Training loss [0.2774175  0.10975207 0.1615692  0.10977747]\n",
      "Training loss [0.24036053 0.09077056 0.13310635 0.09487589]\n",
      "Training loss [0.2715301  0.09976885 0.16113794 0.10146056]\n",
      "Training loss [0.23962253 0.08727414 0.14051506 0.08967509]\n",
      "Training loss [0.2634418  0.08794456 0.16267571 0.08880676]\n",
      "Training loss [0.2663678  0.09823453 0.16156074 0.09831915]\n",
      "Training loss [0.2274467  0.07999567 0.13309167 0.08354458]\n",
      "Training loss [0.25783035 0.09012539 0.16112618 0.09197685]\n",
      "Training loss [0.22657812 0.07852843 0.14051332 0.08036576]\n",
      "Training loss [0.25515282 0.08059148 0.16264836 0.0812234 ]\n",
      "Training loss [0.25611967 0.09049033 0.16155256 0.09004708]\n",
      "Training loss [0.21559888 0.07332227 0.13307709 0.07618523]\n",
      "Training loss [0.24574722 0.08267587 0.16111457 0.08355193]\n",
      "Training loss [0.21616155 0.07216205 0.14051247 0.07379686]\n",
      "Training loss [0.24905609 0.074591   0.16262212 0.0753475 ]\n",
      "Training loss [0.24843025 0.08396181 0.16154435 0.08209664]\n",
      "Training loss [0.20625992 0.06732504 0.13306281 0.07051949]\n",
      "Training loss [0.46709877 0.48697236 0.52706873 0.45570356]\n",
      "Training loss [0.2850783  0.1343103  0.17609629 0.13368618]\n",
      "Training loss [0.28515732 0.14231908 0.19358611 0.14254227]\n",
      "Training loss [0.28072536 0.13799419 0.18940943 0.13943821]\n",
      "Training loss [0.2863344  0.14374402 0.21258324 0.14223117]\n",
      "Training loss [0.29456478 0.1332382  0.20333692 0.13022254]\n",
      "Training loss [0.26633704 0.11813445 0.17353252 0.11682449]\n",
      "Training loss [0.2752747  0.13256596 0.19348305 0.13155775]\n",
      "Training loss [0.27275392 0.12882046 0.18936579 0.1300742 ]\n",
      "Training loss [0.27864924 0.13271713 0.21257469 0.13059358]\n",
      "Training loss [0.2864273  0.12116955 0.20333274 0.1181542 ]\n",
      "Training loss [0.25461504 0.10653111 0.17353097 0.10535067]\n",
      "Training loss [0.26257107 0.12033642 0.1934698  0.12042776]\n",
      "Training loss [0.26285595 0.11720745 0.18936433 0.11994954]\n",
      "Training loss [0.26945442 0.11849274 0.2125687  0.12015056]\n",
      "Training loss [0.27692086 0.10796414 0.20332953 0.10899154]\n",
      "Training loss [0.24174446 0.09270497 0.17353293 0.0957004 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.25028554 0.10647838 0.19345619 0.10857749]\n",
      "Training loss [0.2549989  0.10422759 0.18936191 0.10846239]\n",
      "Training loss [0.26136723 0.10706218 0.21256208 0.10757428]\n",
      "Training loss [0.2691726  0.09650977 0.20332596 0.09697297]\n",
      "Training loss [0.23158978 0.08276801 0.1735325  0.08430263]\n",
      "Training loss [0.24123389 0.09561519 0.19344309 0.09913833]\n",
      "Training loss [0.24938983 0.09441694 0.18935983 0.09911576]\n",
      "Training loss [0.2547964  0.09955574 0.21255583 0.09879285]\n",
      "Training loss [0.43304938 0.45514137 0.4893878  0.4379524 ]\n",
      "Training loss [0.25533867 0.11800344 0.16008034 0.11682815]\n",
      "Training loss [0.24822795 0.11131984 0.1620731  0.110939  ]\n",
      "Training loss [0.23964138 0.11018333 0.16574112 0.11004078]\n",
      "Training loss [0.2544732  0.1134578  0.18065475 0.11390169]\n",
      "Training loss [0.22533402 0.09897208 0.14279434 0.09928784]\n",
      "Training loss [0.22972736 0.09645219 0.15966243 0.09601979]\n",
      "Training loss [0.2335565  0.10141286 0.16203909 0.10002636]\n",
      "Training loss [0.22805205 0.1018747  0.16571811 0.10143487]\n",
      "Training loss [0.24338847 0.10601197 0.18063645 0.10562225]\n",
      "Training loss [0.21436061 0.09074922 0.14277038 0.09050328]\n",
      "Training loss [0.21566069 0.08815035 0.15965697 0.08725819]\n",
      "Training loss [0.21943232 0.09375767 0.16201907 0.09184338]\n",
      "Training loss [0.2156521  0.09472183 0.1656971  0.09390656]\n",
      "Training loss [0.23294921 0.09953672 0.18061653 0.09864384]\n",
      "Training loss [0.20268837 0.08342326 0.14275065 0.08323995]\n",
      "Training loss [0.2030702  0.08066319 0.15964684 0.07979149]\n",
      "Training loss [0.20754734 0.08724196 0.16199784 0.08516195]\n",
      "Training loss [0.20534557 0.08873537 0.16567677 0.08799748]\n",
      "Training loss [0.22510009 0.0935872  0.18059933 0.09252758]\n",
      "Training loss [0.19350475 0.07691008 0.14273235 0.07684359]\n",
      "Training loss [0.19410771 0.07418889 0.1596367  0.07349004]\n",
      "Training loss [0.19934225 0.08131821 0.16197908 0.07925745]\n",
      "Training loss [0.19814625 0.08338697 0.16565788 0.08290099]\n",
      "Training loss [0.21941015 0.08764636 0.18058479 0.08696295]\n",
      "Training loss [0.4470535  0.46624288 0.47737893 0.48121026]\n",
      "Training loss [0.2687288  0.15765633 0.20068869 0.15771778]\n",
      "Training loss [0.24766655 0.1399368  0.19464359 0.13975912]\n",
      "Training loss [0.24922442 0.13631555 0.1921922  0.1357925 ]\n",
      "Training loss [0.24710885 0.13527867 0.18833849 0.13460845]\n",
      "Training loss [0.25518605 0.13487116 0.20257112 0.1339547 ]\n",
      "Training loss [0.23865247 0.1296178  0.20079444 0.13003175]\n",
      "Training loss [0.2332122  0.12775394 0.19462657 0.12747872]\n",
      "Training loss [0.23663387 0.12750669 0.19219501 0.12664846]\n",
      "Training loss [0.23334086 0.12485933 0.18831743 0.12472915]\n",
      "Training loss [0.24014856 0.12619546 0.20254876 0.12528887]\n",
      "Training loss [0.22534585 0.12010851 0.20076539 0.12112504]\n",
      "Training loss [0.2217077  0.11716012 0.19461115 0.11784644]\n",
      "Training loss [0.22616485 0.11966087 0.19220033 0.11954311]\n",
      "Training loss [0.22051197 0.11589336 0.18829882 0.11681645]\n",
      "Training loss [0.22694968 0.11790284 0.20252824 0.11713441]\n",
      "Training loss [0.21643987 0.11190952 0.20074011 0.11357466]\n",
      "Training loss [0.2132087  0.10888863 0.19459711 0.10996596]\n",
      "Training loss [0.2178685  0.11356384 0.19220808 0.11407119]\n",
      "Training loss [0.21112454 0.10920531 0.18828191 0.11047243]\n",
      "Training loss [0.21730019 0.11142358 0.2025089  0.11060622]\n",
      "Training loss [0.21014236 0.10562027 0.2007209  0.1074708 ]\n",
      "Training loss [0.20649065 0.10296826 0.19458404 0.10395204]\n",
      "Training loss [0.21098751 0.10840039 0.19221792 0.1094337 ]\n",
      "Training loss [0.20419359 0.10405678 0.18826544 0.1051124 ]\n",
      "Training loss [0.43934765 0.47208324 0.48608175 0.44232142]\n",
      "Training loss [0.2568022  0.15420622 0.18244526 0.15587708]\n",
      "Training loss [0.24230166 0.12822218 0.18572384 0.12933528]\n",
      "Training loss [0.23709482 0.12639683 0.19243217 0.12621512]\n",
      "Training loss [0.21691018 0.11576982 0.17566514 0.11632857]\n",
      "Training loss [0.2343347  0.12263691 0.1880686  0.12181846]\n",
      "Training loss [0.2352244  0.1223142  0.18222609 0.12200642]\n",
      "Training loss [0.22738475 0.11715397 0.18572816 0.11641029]\n",
      "Training loss [0.22626811 0.11748356 0.19240367 0.11711707]\n",
      "Training loss [0.20531873 0.10897883 0.17561741 0.1096371 ]\n",
      "Training loss [0.22290826 0.11490148 0.18804055 0.11387345]\n",
      "Training loss [0.22428083 0.11523928 0.18220349 0.11441006]\n",
      "Training loss [0.21527883 0.10999163 0.18572155 0.10872032]\n",
      "Training loss [0.21462744 0.11000507 0.19238192 0.10947991]\n",
      "Training loss [0.19308567 0.10204791 0.17557782 0.10246016]\n",
      "Training loss [0.21204422 0.10797912 0.18801475 0.1068977 ]\n",
      "Training loss [0.21428329 0.10845472 0.18218121 0.10778562]\n",
      "Training loss [0.20506461 0.10353663 0.18571611 0.10228764]\n",
      "Training loss [0.20506947 0.10391591 0.19236548 0.10372569]\n",
      "Training loss [0.18373981 0.09664852 0.17553928 0.09640028]\n",
      "Training loss [0.2036609  0.1024773  0.18799089 0.10147135]\n",
      "Training loss [0.20691344 0.10303996 0.18215747 0.1022955 ]\n",
      "Training loss [0.19803171 0.09839305 0.18571165 0.09701592]\n",
      "Training loss [0.19853678 0.09927709 0.19235304 0.09920239]\n",
      "Training loss [0.1773963  0.09276342 0.17550246 0.09167621]\n",
      "Training loss [0.46390137 0.47691378 0.5266564  0.46820664]\n",
      "Training loss [0.27853853 0.17512181 0.21893777 0.17062429]\n",
      "Training loss [0.2536378  0.13585296 0.20216984 0.13539323]\n",
      "Training loss [0.25595507 0.13374865 0.21034758 0.13329053]\n",
      "Training loss [0.24205744 0.12917432 0.21037082 0.12880503]\n",
      "Training loss [0.24995402 0.12888598 0.21094105 0.12880799]\n",
      "Training loss [0.24487834 0.13301489 0.21889624 0.13309202]\n",
      "Training loss [0.23500414 0.12382084 0.20211157 0.12414055]\n",
      "Training loss [0.23921248 0.12696257 0.21030551 0.12539026]\n",
      "Training loss [0.2255458  0.12383027 0.21032897 0.12266009]\n",
      "Training loss [0.2352334  0.1244632  0.21089375 0.12281315]\n",
      "Training loss [0.23172256 0.12774423 0.2188555  0.1267052 ]\n",
      "Training loss [0.22278517 0.11898517 0.20206223 0.11840644]\n",
      "Training loss [0.22766337 0.12192614 0.21026811 0.11873637]\n",
      "Training loss [0.21430537 0.11862136 0.21029207 0.11630275]\n",
      "Training loss [0.22312243 0.1198084  0.21084785 0.11674737]\n",
      "Training loss [0.22334948 0.12226325 0.21882553 0.12034997]\n",
      "Training loss [0.21418491 0.11406697 0.20202014 0.11256099]\n",
      "Training loss [0.2194587  0.11688562 0.21023397 0.11253653]\n",
      "Training loss [0.20640457 0.11324411 0.21026535 0.11039038]\n",
      "Training loss [0.21411732 0.11565437 0.21080469 0.11180364]\n",
      "Training loss [0.21770024 0.11743797 0.21880092 0.11542976]\n",
      "Training loss [0.20796326 0.11012632 0.20198295 0.10809643]\n",
      "Training loss [0.21330339 0.11241269 0.21020514 0.10797009]\n",
      "Training loss [0.20066306 0.10855545 0.21024263 0.10551028]\n",
      "Training loss [0.46486259 0.4686186  0.51733476 0.45596522]\n",
      "Training loss [0.24948815 0.16645741 0.18480223 0.16826037]\n",
      "Training loss [0.23489073 0.13757506 0.1836979  0.13699314]\n",
      "Training loss [0.21823898 0.12760581 0.18629162 0.12557371]\n",
      "Training loss [0.2143062  0.11894325 0.18315753 0.11913456]\n",
      "Training loss [0.21595861 0.12415855 0.18755755 0.12547134]\n",
      "Training loss [0.21220571 0.12811631 0.18469615 0.12956628]\n",
      "Training loss [0.21215007 0.12038943 0.18367837 0.12043423]\n",
      "Training loss [0.20163614 0.11654338 0.18622309 0.11731559]\n",
      "Training loss [0.19719276 0.11186786 0.18312433 0.11249243]\n",
      "Training loss [0.20199439 0.11849616 0.18751769 0.11955455]\n",
      "Training loss [0.19891694 0.12187473 0.18468305 0.12445557]\n",
      "Training loss [0.19801255 0.11484611 0.18365912 0.11550359]\n",
      "Training loss [0.19127142 0.11109312 0.18616536 0.11257699]\n",
      "Training loss [0.18622348 0.10667535 0.18309505 0.10774813]\n",
      "Training loss [0.1925651  0.113488   0.18748005 0.11484791]\n",
      "Training loss [0.19041364 0.11630066 0.18467095 0.11910741]\n",
      "Training loss [0.18872519 0.10999349 0.1836432  0.11077762]\n",
      "Training loss [0.18488982 0.10634648 0.1861223  0.10759548]\n",
      "Training loss [0.18002191 0.10209737 0.18306753 0.10357298]\n",
      "Training loss [0.1866872  0.10916035 0.18744454 0.11058949]\n",
      "Training loss [0.18513238 0.11197913 0.18465832 0.11434171]\n",
      "Training loss [0.18248308 0.10603425 0.18362617 0.10660528]\n",
      "Training loss [0.18034613 0.10266794 0.18608837 0.10338555]\n",
      "Training loss [0.17595363 0.09861851 0.18304062 0.10017825]\n",
      "Training loss [0.45522878 0.48489338 0.5283544  0.4882985 ]\n",
      "Training loss [0.24790472 0.17258736 0.19438332 0.17193455]\n",
      "Training loss [0.23827824 0.14487362 0.19742413 0.1439052 ]\n",
      "Training loss [0.22271442 0.12914711 0.1984499  0.1288159 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.22100708 0.12661995 0.1958415  0.12668075]\n",
      "Training loss [0.22093946 0.12587163 0.20545733 0.12617783]\n",
      "Training loss [0.211038   0.12170627 0.19429725 0.12254354]\n",
      "Training loss [0.21988583 0.12190464 0.19736691 0.12186715]\n",
      "Training loss [0.20915529 0.11806649 0.1983958  0.11779068]\n",
      "Training loss [0.20780808 0.12066805 0.19580022 0.11990266]\n",
      "Training loss [0.20892288 0.12079008 0.20540303 0.12090984]\n",
      "Training loss [0.20014088 0.11853211 0.19425353 0.11885467]\n",
      "Training loss [0.20981869 0.11857784 0.19733255 0.11878398]\n",
      "Training loss [0.20030352 0.11551626 0.1983616  0.1149502 ]\n",
      "Training loss [0.19888724 0.11796162 0.19576718 0.11714753]\n",
      "Training loss [0.20076153 0.1181043  0.20537437 0.1180878 ]\n",
      "Training loss [0.19248751 0.11628021 0.19423148 0.11600401]\n",
      "Training loss [0.20252682 0.11596122 0.19730496 0.11642826]\n",
      "Training loss [0.19374263 0.11310965 0.19833228 0.11215949]\n",
      "Training loss [0.19277352 0.11515939 0.19573966 0.11434138]\n",
      "Training loss [0.19482657 0.11541022 0.2053546  0.11524214]\n",
      "Training loss [0.18671046 0.11383607 0.19421694 0.11323726]\n",
      "Training loss [0.19710931 0.11320461 0.19728115 0.11405832]\n",
      "Training loss [0.1888712  0.11039658 0.19830471 0.10929671]\n",
      "Training loss [0.18824005 0.11234978 0.19571283 0.11158974]\n",
      "Training loss [0.44687787 0.46350825 0.5044662  0.46754897]\n",
      "Training loss [0.25762817 0.18119144 0.20239261 0.18319003]\n",
      "Training loss [0.22727102 0.14304781 0.19280916 0.14274015]\n",
      "Training loss [0.21545923 0.130252   0.19842502 0.1297468 ]\n",
      "Training loss [0.21682209 0.12361877 0.19763736 0.12347353]\n",
      "Training loss [0.21483718 0.12169611 0.19710878 0.12188473]\n",
      "Training loss [0.22026213 0.12197903 0.20234708 0.12229857]\n",
      "Training loss [0.20283481 0.11202215 0.19276519 0.11185461]\n",
      "Training loss [0.19738126 0.11425846 0.19834062 0.11383799]\n",
      "Training loss [0.20040141 0.11343222 0.19760649 0.11319619]\n",
      "Training loss [0.20084158 0.1136185  0.19701657 0.11387207]\n",
      "Training loss [0.20839679 0.11605299 0.20230374 0.11610812]\n",
      "Training loss [0.19161417 0.10722328 0.19274324 0.10719797]\n",
      "Training loss [0.186782   0.11090862 0.19829178 0.11094204]\n",
      "Training loss [0.19064026 0.11023112 0.1975855  0.11057435]\n",
      "Training loss [0.19270943 0.11106218 0.19695365 0.11150312]\n",
      "Training loss [0.20090213 0.11342786 0.2022722  0.11332105]\n",
      "Training loss [0.18499672 0.10470298 0.19272915 0.10471292]\n",
      "Training loss [0.18019058 0.10846902 0.19825575 0.10892716]\n",
      "Training loss [0.18415236 0.10791542 0.1975706  0.10860726]\n",
      "Training loss [0.1875635  0.10890131 0.19690362 0.10951498]\n",
      "Training loss [0.19582793 0.11126307 0.20224631 0.11117995]\n",
      "Training loss [0.18043059 0.10279731 0.19271839 0.10260747]\n",
      "Training loss [0.17584431 0.10634898 0.19822657 0.10723552]\n",
      "Training loss [0.17956814 0.10579742 0.19755888 0.10673511]\n",
      "Training loss [0.4453831  0.48575497 0.49883044 0.4653294 ]\n",
      "Training loss [0.24958059 0.18585646 0.19413358 0.18621819]\n",
      "Training loss [0.21449724 0.14743127 0.18078391 0.14883116]\n",
      "Training loss [0.2177371  0.14181168 0.19174366 0.14156146]\n",
      "Training loss [0.20336317 0.1279315  0.18398076 0.12913425]\n",
      "Training loss [0.20717569 0.13001332 0.19459255 0.1311545 ]\n",
      "Training loss [0.20698904 0.12730433 0.19397566 0.1285008 ]\n",
      "Training loss [0.19321817 0.11694562 0.18070874 0.11694603]\n",
      "Training loss [0.20043814 0.12268331 0.19165815 0.12157135]\n",
      "Training loss [0.18778095 0.11626956 0.18395403 0.11615682]\n",
      "Training loss [0.19397415 0.12000094 0.19446164 0.12050505]\n",
      "Training loss [0.19442977 0.12030255 0.19389628 0.12071154]\n",
      "Training loss [0.18300512 0.11119444 0.18067475 0.11142004]\n",
      "Training loss [0.19121218 0.11827774 0.1916017  0.11803854]\n",
      "Training loss [0.17863095 0.11348823 0.18393561 0.11311511]\n",
      "Training loss [0.18675278 0.11680066 0.19437352 0.11759223]\n",
      "Training loss [0.1870416  0.11778081 0.19384491 0.11852919]\n",
      "Training loss [0.17631878 0.10902638 0.18065579 0.1094236 ]\n",
      "Training loss [0.18560565 0.11623943 0.19155775 0.11618711]\n",
      "Training loss [0.172937   0.11207108 0.18392038 0.11168912]\n",
      "Training loss [0.1821008  0.11503567 0.19430411 0.11609881]\n",
      "Training loss [0.18232496 0.11623091 0.19380656 0.11730425]\n",
      "Training loss [0.17165852 0.10774778 0.18064232 0.10820875]\n",
      "Training loss [0.18177348 0.11489526 0.19152106 0.1149841 ]\n",
      "Training loss [0.16902244 0.11086607 0.18390657 0.11063761]\n",
      "Training loss [0.43565705 0.45974606 0.47882515 0.45927352]\n",
      "Training loss [0.2378802  0.18393704 0.19531064 0.18451378]\n",
      "Training loss [0.21787135 0.16268942 0.19594887 0.16056573]\n",
      "Training loss [0.21565124 0.14917026 0.19902103 0.14821008]\n",
      "Training loss [0.18938786 0.13101676 0.18875816 0.13157097]\n",
      "Training loss [0.19583687 0.13076471 0.19108745 0.12999685]\n",
      "Training loss [0.19829828 0.12877381 0.19514194 0.12820089]\n",
      "Training loss [0.1947451  0.12874788 0.19576357 0.1273988 ]\n",
      "Training loss [0.19675554 0.12667036 0.19890702 0.126282  ]\n",
      "Training loss [0.17424783 0.11680802 0.18861371 0.11767691]\n",
      "Training loss [0.1826253  0.1191694  0.19098751 0.11923231]\n",
      "Training loss [0.1882019  0.12007827 0.19509022 0.12013753]\n",
      "Training loss [0.18588567 0.12256134 0.1956716  0.12158746]\n",
      "Training loss [0.18807438 0.12144056 0.19884513 0.12181817]\n",
      "Training loss [0.16693866 0.11295718 0.18852979 0.11387298]\n",
      "Training loss [0.17568526 0.11606954 0.19093525 0.11616806]\n",
      "Training loss [0.18221232 0.11741859 0.1950647  0.11721331]\n",
      "Training loss [0.18090051 0.120299   0.1956121  0.11951821]\n",
      "Training loss [0.18285255 0.11901155 0.1988024  0.11980011]\n",
      "Training loss [0.16238038 0.11143192 0.1884695  0.11227338]\n",
      "Training loss [0.17120755 0.11469898 0.19089803 0.11474966]\n",
      "Training loss [0.17786962 0.11607818 0.19504753 0.11582825]\n",
      "Training loss [0.17760915 0.11906919 0.19556746 0.11835186]\n",
      "Training loss [0.17905445 0.11755282 0.1987699  0.11847471]\n",
      "Training loss [0.1590721  0.11065282 0.1884206  0.11132891]\n",
      "Training loss [0.43004495 0.44421983 0.48041415 0.44959903]\n",
      "Training loss [0.22383009 0.17661257 0.18176693 0.17683625]\n",
      "Training loss [0.19853184 0.15414192 0.1777853  0.15403807]\n",
      "Training loss [0.20389563 0.1463266  0.18174157 0.1446724 ]\n",
      "Training loss [0.19839534 0.13696079 0.18257992 0.13673654]\n",
      "Training loss [0.17981789 0.12787822 0.17918625 0.12784217]\n",
      "Training loss [0.18344536 0.12626919 0.18158893 0.12625429]\n",
      "Training loss [0.17640634 0.12254857 0.17770818 0.1230054 ]\n",
      "Training loss [0.18646379 0.12482454 0.18167108 0.12306626]\n",
      "Training loss [0.18328667 0.12140214 0.18249607 0.12122037]\n",
      "Training loss [0.16797543 0.11600477 0.17911154 0.11583965]\n",
      "Training loss [0.17291674 0.11771131 0.1815245  0.11723365]\n",
      "Training loss [0.16905968 0.11645141 0.17768292 0.11633107]\n",
      "Training loss [0.17847419 0.1199524  0.18163677 0.11858768]\n",
      "Training loss [0.17571104 0.11731097 0.18245183 0.11710171]\n",
      "Training loss [0.162278   0.11229812 0.17907001 0.11253863]\n",
      "Training loss [0.16725868 0.1149952  0.1814799  0.11437506]\n",
      "Training loss [0.16452816 0.11443871 0.17766869 0.11416228]\n",
      "Training loss [0.1737     0.11835062 0.18161203 0.11695041]\n",
      "Training loss [0.17094763 0.1156695  0.18242033 0.11544493]\n",
      "Training loss [0.15884176 0.11072993 0.17903657 0.11106585]\n",
      "Training loss [0.16357742 0.11366754 0.18144359 0.11295867]\n",
      "Training loss [0.16127753 0.11338079 0.17766005 0.11308716]\n",
      "Training loss [0.17053953 0.11738312 0.18159117 0.1159764 ]\n",
      "Training loss [0.1677052  0.11469226 0.18239397 0.11453728]\n",
      "Training loss [0.42636973 0.4463647  0.46645328 0.44190937]\n",
      "Training loss [0.2219813  0.17464249 0.18211123 0.17589632]\n",
      "Training loss [0.20284852 0.15966335 0.18254036 0.15889245]\n",
      "Training loss [0.20288199 0.15011911 0.1864598  0.15195358]\n",
      "Training loss [0.18998727 0.1376796  0.17970009 0.13787036]\n",
      "Training loss [0.18420127 0.1305772  0.17727672 0.13104156]\n",
      "Training loss [0.18076223 0.12745127 0.18181896 0.12810613]\n",
      "Training loss [0.17957035 0.12866539 0.18236575 0.12834601]\n",
      "Training loss [0.1853948  0.12868637 0.18639562 0.13062459]\n",
      "Training loss [0.17542689 0.12200521 0.17963076 0.12141503]\n",
      "Training loss [0.17124674 0.11910336 0.17717075 0.11962879]\n",
      "Training loss [0.17043175 0.11829343 0.18175407 0.11874366]\n",
      "Training loss [0.17142999 0.12216503 0.18231145 0.12190498]\n",
      "Training loss [0.17756855 0.12361977 0.18635261 0.12480788]\n",
      "Training loss [0.16754535 0.11775147 0.17958847 0.11722875]\n",
      "Training loss [0.16440268 0.11592407 0.17711027 0.11657219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.16474307 0.11574002 0.18172571 0.11595677]\n",
      "Training loss [0.16700304 0.12038162 0.1822764  0.11970755]\n",
      "Training loss [0.17259076 0.12177786 0.18631652 0.12259976]\n",
      "Training loss [0.16272163 0.11599167 0.17956087 0.11567308]\n",
      "Training loss [0.1600605  0.1145143  0.17706749 0.11528201]\n",
      "Training loss [0.16089237 0.11456903 0.18170929 0.11463939]\n",
      "Training loss [0.16395816 0.11951834 0.18224692 0.11866982]\n",
      "Training loss [0.16918513 0.12075052 0.1862838  0.1214413 ]\n",
      "Training loss [0.159399   0.11491778 0.17953986 0.11481099]\n",
      "Training loss [0.41966775 0.43699437 0.46863678 0.44346562]\n",
      "Training loss [0.21344537 0.17367043 0.18124813 0.17501298]\n",
      "Training loss [0.19497302 0.16036041 0.18316688 0.16095454]\n",
      "Training loss [0.19262654 0.14758138 0.180667   0.14827767]\n",
      "Training loss [0.1808539  0.14083618 0.18577051 0.14171912]\n",
      "Training loss [0.1847415  0.13875224 0.18509431 0.13903137]\n",
      "Training loss [0.17779014 0.13118161 0.1807731  0.13136661]\n",
      "Training loss [0.17254229 0.13111126 0.18298715 0.1308226 ]\n",
      "Training loss [0.17536435 0.12629187 0.18054068 0.1263407 ]\n",
      "Training loss [0.16892481 0.12494659 0.18572779 0.12503868]\n",
      "Training loss [0.17247486 0.12593324 0.18495278 0.12604308]\n",
      "Training loss [0.16798756 0.12156816 0.18067202 0.12155174]\n",
      "Training loss [0.16423965 0.12398762 0.18294163 0.12406595]\n",
      "Training loss [0.16794443 0.12089352 0.18049797 0.12109055]\n",
      "Training loss [0.16333225 0.12081286 0.18569311 0.12088102]\n",
      "Training loss [0.16601071 0.12268992 0.18487462 0.1227115 ]\n",
      "Training loss [0.162784   0.11880979 0.1806202  0.11876011]\n",
      "Training loss [0.1600278  0.12168214 0.18291232 0.12182615]\n",
      "Training loss [0.16352402 0.11884777 0.18047136 0.11922839]\n",
      "Training loss [0.15991892 0.11915065 0.18566072 0.11931393]\n",
      "Training loss [0.1619364  0.12133411 0.18481489 0.1213825 ]\n",
      "Training loss [0.15975174 0.11752445 0.1805813  0.11747203]\n",
      "Training loss [0.15746818 0.12054355 0.18289012 0.12071814]\n",
      "Training loss [0.16070391 0.11774483 0.18044704 0.11820133]\n",
      "Training loss [0.15741716 0.11827804 0.1856307  0.11845501]\n",
      "Training loss [0.42459238 0.43116018 0.44908506 0.43085223]\n",
      "Training loss [0.2147195  0.18328193 0.18934563 0.1828408 ]\n",
      "Training loss [0.2047527  0.1699087  0.1917355  0.17046031]\n",
      "Training loss [0.1946693  0.15862513 0.1879184  0.15878491]\n",
      "Training loss [0.19008768 0.15184693 0.18990192 0.1520215 ]\n",
      "Training loss [0.20237668 0.15551494 0.19571912 0.15585268]\n",
      "Training loss [0.1830301  0.14156124 0.18902965 0.1423982 ]\n",
      "Training loss [0.18211839 0.14219591 0.1915797  0.14166139]\n",
      "Training loss [0.17882779 0.13769163 0.18778539 0.13790357]\n",
      "Training loss [0.17640354 0.13712046 0.18980473 0.13736013]\n",
      "Training loss [0.18953405 0.14310476 0.19566372 0.14339316]\n",
      "Training loss [0.17355078 0.13144697 0.18899491 0.13264962]\n",
      "Training loss [0.17315489 0.13532102 0.1915417  0.13469131]\n",
      "Training loss [0.17114452 0.13225165 0.18774784 0.1323255 ]\n",
      "Training loss [0.16942888 0.1333758  0.18976262 0.13325593]\n",
      "Training loss [0.18276206 0.13944615 0.19564295 0.13980807]\n",
      "Training loss [0.16803908 0.12885788 0.18897584 0.12969643]\n",
      "Training loss [0.16776365 0.13294998 0.1915143  0.13229598]\n",
      "Training loss [0.16661173 0.1302958  0.18772842 0.1303824 ]\n",
      "Training loss [0.16508222 0.1317882  0.1897339  0.13171783]\n",
      "Training loss [0.1787584  0.13778156 0.19562182 0.13828644]\n",
      "Training loss [0.16436139 0.1276226  0.18895617 0.1285098 ]\n",
      "Training loss [0.16411597 0.13181263 0.1914894  0.13115236]\n",
      "Training loss [0.16329588 0.12926078 0.18771228 0.12945499]\n",
      "Training loss [0.16233444 0.13082178 0.18970758 0.13082889]\n",
      "Training loss [0.40810764 0.42267555 0.44286713 0.41720456]\n",
      "Training loss [0.21134166 0.17769507 0.1857712  0.17837189]\n",
      "Training loss [0.19262612 0.16543123 0.1861476  0.1654848 ]\n",
      "Training loss [0.19125378 0.15808737 0.18727732 0.1579633 ]\n",
      "Training loss [0.18883748 0.15264261 0.190154   0.15290108]\n",
      "Training loss [0.17991507 0.14814204 0.18588576 0.14863488]\n",
      "Training loss [0.17993036 0.14485575 0.18541466 0.14450888]\n",
      "Training loss [0.17277065 0.13973232 0.18597169 0.14012587]\n",
      "Training loss [0.17443112 0.13979593 0.18718484 0.13901848]\n",
      "Training loss [0.17547256 0.13876915 0.1900602  0.13903534]\n",
      "Training loss [0.1690202  0.13701728 0.18583702 0.137607  ]\n",
      "Training loss [0.1716807  0.1367046  0.18536618 0.1364769 ]\n",
      "Training loss [0.16573447 0.1331934  0.18592536 0.13363716]\n",
      "Training loss [0.1671578  0.13489507 0.1871483  0.13419473]\n",
      "Training loss [0.16900067 0.13460323 0.19002405 0.13486642]\n",
      "Training loss [0.16325136 0.13356937 0.18581025 0.13417903]\n",
      "Training loss [0.1670297  0.1340871  0.185339   0.13391016]\n",
      "Training loss [0.16149633 0.13105287 0.1858958  0.13146539]\n",
      "Training loss [0.16314262 0.13306579 0.18712392 0.13245988]\n",
      "Training loss [0.16517463 0.13299301 0.1899979  0.13322672]\n",
      "Training loss [0.1598261  0.13216744 0.18578586 0.13267463]\n",
      "Training loss [0.16404763 0.13288957 0.18531346 0.13282892]\n",
      "Training loss [0.15850928 0.1300732  0.1858702  0.13035893]\n",
      "Training loss [0.16060033 0.13202208 0.18710518 0.1315961 ]\n",
      "Training loss [0.1625984  0.13225299 0.18997362 0.13235852]\n",
      "Training loss [0.40500736 0.4142986  0.43297035 0.41450644]\n",
      "Training loss [0.21614292 0.18924542 0.1978823  0.18975589]\n",
      "Training loss [0.20383641 0.1751499  0.19626695 0.17592533]\n",
      "Training loss [0.1929881  0.16672818 0.19506042 0.16660431]\n",
      "Training loss [0.18693131 0.15558353 0.19139178 0.15600812]\n",
      "Training loss [0.18821824 0.15721564 0.19717366 0.15674612]\n",
      "Training loss [0.1864292  0.15450309 0.19751948 0.1549311 ]\n",
      "Training loss [0.18204394 0.15101665 0.19613338 0.15083587]\n",
      "Training loss [0.17757826 0.14892802 0.1949858  0.14872554]\n",
      "Training loss [0.17428227 0.14325948 0.19135736 0.1436481 ]\n",
      "Training loss [0.17740428 0.14622876 0.19709383 0.14613609]\n",
      "Training loss [0.17803064 0.14629279 0.19749904 0.14690992]\n",
      "Training loss [0.17402676 0.14483583 0.19611257 0.1443899 ]\n",
      "Training loss [0.17110594 0.14354798 0.19497262 0.14359684]\n",
      "Training loss [0.16842084 0.13962957 0.19135952 0.14012334]\n",
      "Training loss [0.1726503  0.14277405 0.19701713 0.14293939]\n",
      "Training loss [0.17352578 0.14358263 0.19749707 0.14428532]\n",
      "Training loss [0.16988263 0.14246593 0.19609141 0.14205128]\n",
      "Training loss [0.16765061 0.14157704 0.19496006 0.14158429]\n",
      "Training loss [0.16527686 0.13804844 0.19136786 0.138571  ]\n",
      "Training loss [0.16918775 0.14132759 0.19694725 0.14165875]\n",
      "Training loss [0.17071661 0.14248554 0.19749689 0.14303358]\n",
      "Training loss [0.16744715 0.14126839 0.19606586 0.14093097]\n",
      "Training loss [0.16551168 0.14058076 0.1949414  0.1406275 ]\n",
      "Training loss [0.16318387 0.13725275 0.19137253 0.13770613]\n",
      "Training loss [0.40214464 0.41331974 0.42854726 0.4112276 ]\n",
      "Training loss [0.20765907 0.17971759 0.18701471 0.17951095]\n",
      "Training loss [0.19270757 0.16599596 0.18525168 0.16588794]\n",
      "Training loss [0.18990943 0.16336738 0.1920403  0.16287431]\n",
      "Training loss [0.18538076 0.15474558 0.18614048 0.15428537]\n",
      "Training loss [0.17840722 0.15298158 0.18821931 0.15281278]\n",
      "Training loss [0.17567104 0.14909655 0.18667741 0.14915222]\n",
      "Training loss [0.17268832 0.14539608 0.18517451 0.14494914]\n",
      "Training loss [0.17277354 0.14587969 0.19198713 0.14595133]\n",
      "Training loss [0.1727052  0.14229864 0.18604977 0.14209405]\n",
      "Training loss [0.16826332 0.14322561 0.18820615 0.14284027]\n",
      "Training loss [0.16728036 0.14171693 0.18661118 0.14101923]\n",
      "Training loss [0.16574576 0.1392444  0.18514743 0.13887274]\n",
      "Training loss [0.16543135 0.1405169  0.19195852 0.14064293]\n",
      "Training loss [0.16665483 0.13818786 0.1860133  0.13813493]\n",
      "Training loss [0.16368017 0.13993402 0.18820071 0.13949627]\n",
      "Training loss [0.16277066 0.1387285  0.18655922 0.13799733]\n",
      "Training loss [0.16150953 0.13698012 0.18512163 0.13673678]\n",
      "Training loss [0.16157119 0.13832712 0.19192965 0.13867632]\n",
      "Training loss [0.1631962  0.13670735 0.18598925 0.13646734]\n",
      "Training loss [0.16084436 0.13847221 0.18819264 0.13810658]\n",
      "Training loss [0.15995884 0.13744609 0.1865176  0.13658842]\n",
      "Training loss [0.15905412 0.13549969 0.18509334 0.13576533]\n",
      "Training loss [0.15913755 0.13737072 0.19189997 0.137811  ]\n",
      "Training loss [0.16088672 0.13585936 0.18596843 0.13558696]\n",
      "Training loss [0.39669394 0.40741813 0.4219522  0.4081831 ]\n",
      "Training loss [0.20796052 0.18453419 0.19367087 0.18466014]\n",
      "Training loss [0.1955417  0.17007698 0.1915497  0.17036279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss [0.18282723 0.1608232  0.18838596 0.16111127]\n",
      "Training loss [0.18395135 0.15821177 0.1906951  0.1585209 ]\n",
      "Training loss [0.17637393 0.15379871 0.1894511  0.15424137]\n",
      "Training loss [0.1771946  0.15414524 0.19335797 0.1545373 ]\n",
      "Training loss [0.17339766 0.14940651 0.19147071 0.14973623]\n",
      "Training loss [0.16844833 0.14655687 0.18838266 0.14683492]\n",
      "Training loss [0.17140666 0.14721967 0.19066644 0.1472932 ]\n",
      "Training loss [0.16691422 0.14491782 0.18943956 0.14502582]\n",
      "Training loss [0.16934252 0.14748839 0.1933586  0.14783901]\n",
      "Training loss [0.16528001 0.14380284 0.19145262 0.1442153 ]\n",
      "Training loss [0.16248676 0.14193669 0.18836907 0.14211932]\n",
      "Training loss [0.16607316 0.14355105 0.19064344 0.14370325]\n",
      "Training loss [0.16241471 0.14181697 0.18941785 0.14188747]\n",
      "Training loss [0.16584654 0.14488693 0.19335043 0.14549184]\n",
      "Training loss [0.16133483 0.1416915  0.19143334 0.14210966]\n",
      "Training loss [0.15929827 0.14010021 0.18834084 0.14032617]\n",
      "Training loss [0.16299874 0.14198482 0.19061956 0.14228843]\n",
      "Training loss [0.15961236 0.1405412  0.18939103 0.14045098]\n",
      "Training loss [0.16375805 0.14386576 0.19333497 0.14430629]\n",
      "Training loss [0.1586947  0.14063005 0.19141069 0.14105868]\n",
      "Training loss [0.15714368 0.13916658 0.18831323 0.13929285]\n",
      "Training loss [0.1610171  0.14123866 0.19059767 0.14148995]\n"
     ]
    }
   ],
   "source": [
    "iris_trials = [list(map(iris_l, n_ds)) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctt = np.stack(ctrl_trials, axis=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "itt = np.stack(iris_trials, axis=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 21)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fec28ef3100>,\n",
       " <matplotlib.lines.Line2D at 0x7fec28ef3220>,\n",
       " <matplotlib.lines.Line2D at 0x7fec28ef3940>,\n",
       " <matplotlib.lines.Line2D at 0x7fec28ef3d00>,\n",
       " <matplotlib.lines.Line2D at 0x7fec199bedf0>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ctt.transpose(), 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function numpy.repeat(a, repeats, axis=None)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7feade9dd070>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(np.repeat(n_ds,repeats=5,axis=0), itt.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpi=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'MNIST')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(dpi=dpi)\n",
    "plt.semilogx(n_ds, np.ones_like(n_ds)*upper_limit[1], color=\"black\", label=\"Conventional Limit\")\n",
    "plt.semilogx(n_ds, itt.transpose()[:,0], 'o', color=\"blue\", label=\"Bridge Network\")\n",
    "plt.semilogx(n_ds, ctt.transpose()[:,0], 'o', color=\"red\", label=\"Control Network\")\n",
    "plt.semilogx(n_ds, itt.transpose()[:,1:], 'o', color=\"blue\")\n",
    "plt.semilogx(n_ds, ctt.transpose()[:,1:], 'o', color=\"red\")\n",
    "plt.xlabel(\"Vector-Symbol Dimensionality\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"mnist_sweep.npz\", iris=itt, control=ctt, upper=upper_limit[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98698"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(itt[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009744742172063618"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(itt[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.036331214010715485, 0.9915000200271606]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
